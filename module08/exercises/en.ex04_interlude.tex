%******************************************************************************%
%                                                                              %
%                                 Interlude                                    %
%                         for Machine Learning module                          %
%                                                                              %
%******************************************************************************%

% =============================================== %
\section*{Interlude}
% =============================================== %
\subsection*{Improve}
% ----------------------------------------------- %

\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.25]{assets/Improve.png}
    %\caption{The Learning Cycle: Improve}
\end{figure}
\noindent{Now we want to improve the algorithm's 
performance, or in other words, reduce the loss of its predictions.}\\
\\
This brings us (again) to calculating the gradient, which will tell us by
how much and in which direction the theta parameters belonging to the model should be adjusted.

\newpage
% =============================================== %
\subsection*{The logistic gradient}
% ----------------------------------------------- %
If you remember, to calculate the gradient, we start with the loss function and we derive it 
with respect to each of the theta parameters.\\
\\
If you know multivariate calculus already, you can try it for yourself, otherwise we've got you covered:\\

$$
\begin{matrix}
\nabla(J)_0 &  = &\cfrac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)}) & \\
\nabla(J)_j & = &\cfrac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})x_{j}^{(i)} & \text{ for j = 1, ..., n}    
\end{matrix}
$$
Where:
\begin{itemize}
    \item $\nabla(J)$ is a vector of dimension $(n + 1)$, the gradient vector
    \item $\nabla(J)_j$ is the j$^\text{th}$ component of $\nabla(J)$, 
    the partial derivative of $J$ with respect to $\theta_j$
    \item $y$ is a vector of dimension $m$, the vector of expected values
    \item $y^{(i)}$ is a scalar, the i$^\text{th}$ component of vector $y$
    \item $x^{(i)}$ is the feature vector of the i$^\text{th}$ example
    \item $x^{(i)}_j$ is a scalar, the j$^\text{th}$ feature value of the i$^\text{th}$ example
    \item $h_{\theta}(x^{(i)})$ is a scalar, the model's estimation of $y^{(i)}$\\
\end{itemize}
This formula should be very familiar to you, as it's the same one you used to calculate the linear regression gradient!\\
\\
The only difference is that $h_{\theta}(x^{(i)})$ corresponds to \textbf{the logistic regression hypothesis instead of the linear regression hypothesis}.\\
\\
In other words:\\
$$
h_{\theta}(x^{(i)}) = \text{sigmoid}( \theta \cdot x'^{(i)}) = \cfrac{1} {1 + e^{-\theta \cdot x'^{(i)}}}
$$
\\
Instead of:
\\
$$
\cancel{h_{\theta}(x^{(i)}) = \theta \cdot x'^{(i)}}
$$
