\chapter{Exercise 06}
\extitle{Logistic Regression}
%\input{exercises/en.ex06_interlude.tex}
%\newpage
\turnindir{ex06}
\exnumber{06}
\exfiles{my\_logistic\_regression.py}
\exforbidden{sklearn}
\makeheaderfilesforbidden

% ================================= %
\section*{Objective}
% --------------------------------- %
The time to use everything you built so far has come! Demonstrate your knowledge by implementing a logistic regression classifier using the gradient descent algorithm.
You must have seen the power of \texttt{numpy} for vectorized operations. Well let's make something more concrete with that.

You may have to take a look at Scikit-Learn's implementation of logistic regression and noticed that the \textbf{sklearn.linear\_model.LogisticRegression} class offers a lot of options.

The goal of this exercise is to make a simplified but nonetheless useful and powerful version, with fewer options.

% ================================= %
\section*{Instructions}
% --------------------------------- %
In the \texttt{my\_logistic\_regression.py} file, write a \texttt{MyLogisticRegression} class as in the instructions below:

\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
class MyLogisticRegression():
	"""
	Description:
		My personnal logistic regression to classify things.
	"""
    def __init__(self, theta, alpha=0.001, max_iter=1000):
        self.alpha = alpha
        self.max_iter = max_iter
        self.theta = theta
        ... Your code here ...

	... other methods ...
\end{minted}

You will add at least the following methods:
\begin{itemize}
  \item \texttt{predict\_(self, x)}
  \item \texttt{loss\_elem\_(self, y, yhat)}
  \item \texttt{loss\_(self, y, yhat)}
  \item \texttt{fit\_(self, x, y)}
\end{itemize}

You have already written these functions, you will just need few adjustments so that they all work well within your \texttt{MyLogisticRegression} class.

% ================================= %
\subsection*{Examples}
% --------------------------------- %

\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
import numpy as np
from my_logistic_regression import MyLogisticRegression as MyLR
X = np.array([[1., 1., 2., 3.], [5., 8., 13., 21.], [3., 5., 9., 14.]])
Y = np.array([[1], [0], [1]])
thetas = np.array([[2], [0.5], [7.1], [-4.3], [2.09]])
mylr = MyLR(thetas)

# Example 0:
mylr.predict_(X)
# Output:
array([[0.99930437],
       [1.        ],
       [1.        ]])

# Example 1:
mylr.loss_(X,Y)
# Output:
11.513157421577002

# Example 2:
mylr.fit_(X, Y)
mylr.theta
# Output:
array([[ 2.11826435]
       [ 0.10154334]
       [ 6.43942899]
       [-5.10817488]
       [ 0.6212541 ]])

# Example 3:
mylr.predict_(X)
# Output:
array([[0.57606717]
       [0.68599807]
       [0.06562156]])

# Example 4:
mylr.loss_(X,Y)
# Output:
1.4779126923052268
\end{minted}