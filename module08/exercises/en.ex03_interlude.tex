%******************************************************************************%
%                                                                              %
%                                 Interlude                                    %
%                         for Machine Learning module                          %
%                                                                              %
%******************************************************************************%

% =============================== %
\section*{Interlude}
% =============================== %
\subsection*{Linear Algebra Strikes Again!}
% ******************************* %

Hopefully, you've become quite used to vectorization by now.
You may have already tried to vectorize the logistic loss function by yourself.
Let's look one last time at the former equation:

$$
J( \theta) = -\cfrac{1} {m} \lbrack \sum_{i = 1}^{m} y^{(i)}\log(\hat{y}^{(i)})) + (1 - y^{(i)})\log(1 - \hat{y}^{(i)})\rbrack
$$

% =============================== %
\subsection*{Vectorized Logistic Loss Function}
% ******************************* %
In the \textbf{vectorized version}, we remove the sum ($\sum$) because it is captured by the dot products:
$$
J( \theta) = -\cfrac{1} {m} \lbrack y \cdot \log(\hat{y}) + (\vec{1} - y) \cdot \log(\vec{1} - \hat{y})\rbrack
$$
\\
Where:
\begin{itemize}
       \item $\vec{1}$ is a vector full of $1$'s with the same length of $y$ ($m$).
             $$
             \vec{1} = \begin{bmatrix}
                 1 \\
                 \vdots \\
                 1
             \end{bmatrix}
             $$
\end{itemize}

% =============================== %
\subsection*{Note: Operations Between Vectors and Scalars}
% ******************************* %
We use the $\vec{1}$ notation to be rigorous, because \textbf{addition (or subtraction) between a vector and a scalar is not defined}.
In other words, mathematically, you cannot write this: $1 - y$.\\
\hint{The only operation permitted between a scalar 
and a vector is a multiplication, remember?}

% =============================== %
\subsubsection*{However...}
% ******************************* %
\texttt{NumPy} is a bit permissive on vectors and matrices operations...\\
\\
The following instructions will get you the same results:\\
\\
\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
# Proper mathematical notation
y = np.array([[4], [7.16], [3.2], [9.37], [0.56]])
ones = np.ones(y.shape)
ones - y
# Output
array([[-3.  ],
       [-6.16],
       [-2.2 ],
       [-8.37],
       [ 0.44]])

# Incorrect mathematical notation
y = np.array([[4], [7.16], [3.2], [9.37], [0.56]])
1 - y
# Output
array([[-3.  ],
       [-6.16],
       [-2.2 ],
       [-8.37],
       [ 0.44]])
\end{minted}
\newline
\textbf{Strange, isn't it?}\\
\\
It happens because of one of \texttt{NumPy}'s 
most permissive operations called \textbf{Broadcasting}.\\
\\
Broadcasting is a powerful feature whereby \texttt{NumPy} is able to figure out 
that you actually wanted to perform a subtraction on each element of the vector, 
so it does it for you automatically.\\
\\
It's very handy to write concise lines of code, but it can insert 
very sneaky bugs if you aren't $100$\% confident with what you're doing.\\
\\
Many of the bugs you will encounter while working on Machine Learning 
problems will come from \texttt{NumPy}'s excessive permissiveness.\\
\\
Such bugs generally don't throw any errors, but mess up with the content of your
 vectors and matrices and you'll have to spend an awful lot of time looking for why 
 your model doesn't learn as it should.\\
\\
This is why we \textbf{strongly} suggest that you pay attention to your 
vector (and matrix) shapes and \textbf{stick as much as 
possible to the actual mathematical operations}.\\
\\
\info{
For more information, you can watch \href{https://www.youtube.com/watch?v=V2QlTmh6P2Y&t=213s}{this video about Broadcasting}
}
