\chapter{Exercise 02}
\input{exercises/en.ex02_interlude.tex}
\newpage
\extitle{Logistic Loss Function}
\turnindir{ex02}
\exnumber{02}
\exfiles{log\_loss.py}
\exforbidden{None}
\makeheaderfilesforbidden

% ================================= %
\section*{Objective}
% --------------------------------- %
Understanding and manipulation of the loss function in the context of logistic regression.\\
\\
You must implement the following formula as a function:  

$$
J( \theta) = -\cfrac{1} {m} \lbrack \sum_{i = 1}^{m} y^{(i)}\log(\hat{y}^{(i)})) + (1 - y^{(i)})\log(1 - \hat{y}^{(i)})\rbrack
$$
Where:
\begin{itemize}
  \item $\hat{y}$ is a vector of dimension $m$, the vector of predicted values
  \item $\hat{y}^{(i)}$ is the $i^{th}$ component of the $\hat{y}$ vector
  \item $y$ is a vector of dimension $m$, the vector of expected values
  \item $y^{(i)}$ is the $i^{th}$ component of the $y$ vector
\end{itemize}

% ================================= %
\section*{Instructions}
% --------------------------------- %
In the \texttt{log\_loss.py} file, write the following function as per the instructions below:
\\
\par
\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
def log_loss_(y, y_hat, eps=1e-15):
    """
    Computes the logistic loss value.
    Args:
        y: has to be an numpy.ndarray, a vector of shape m * 1.
        y_hat: has to be an numpy.ndarray, a vector of shape m * 1.
        eps: has to be a float, epsilon (default=1e-15)
    Returns:
        The logistic loss value as a float.
        None on any error.
    Raises:
        This function should not raise any Exception.
    """
    ... Your code ...
\end{minted}

\hint{
  The logarithmic function isn't defined in $0$.
  This means that if $y^{(i)} = 0$ you will get an error when you try to compute $log(y^{(i)})$.
  The purpose of the \texttt{eps} argument is to avoid $log(0)$ errors.
  It is a very small residual value we add to \texttt{y}, also referred to as `epsilon`.
}

% ================================= %
\section*{Examples}
% --------------------------------- %
\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
# Example 1:
y1 = np.array([1]).reshape((-1, 1))
x1 = np.array([4]).reshape((-1, 1))
theta1 = np.array([[2], [0.5]])
y_hat1 = logistic_predict_(x1, theta1)
log_loss_(y1, y_hat1)
# Output:
0.01814992791780973

# Example 2:
y2 = np.array([[1], [0], [1], [0], [1]])
x2 = np.array([[4], [7.16], [3.2], [9.37], [0.56]])
theta2 = np.array([[2], [0.5]])
y_hat2 = logistic_predict_(x2, theta2)
log_loss_(y2, y_hat2)
# Output:
2.4825011602474483

# Example 3:
y3 = np.array([[0], [1], [1]])
x3 = np.array([[0, 2, 3, 4], [2, 4, 5, 5], [1, 3, 2, 7]])
theta3 = np.array([[-2.4], [-1.5], [0.3], [-1.4], [0.7]])
y_hat3 = logistic_predict_(x3, theta3)
log_loss_(y3, y_hat3)
# Output:
2.9938533108607053
\end{minted}

\info{
  This function is called \textbf{Cross-Entropy loss}, or \textbf{logistic loss}.
  For more information you can look at \href{https://en.wikipedia.org/wiki/Cross_entropy\#Cross-entropy\_error\_function\_and\_logistic\_regression}{this section}
  of the Cross entropy Wikipedia article.
}