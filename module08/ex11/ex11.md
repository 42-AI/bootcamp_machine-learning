# Exercise 11 - Other metrics

|                         |                         |
| -----------------------:| ----------------------- |
|   Turn-in directory :   |  ex11                   |
|   Files to turn in :    |  other_metrics.py       |
|   Forbidden functions : |  None                   |
|   Remarks :             |  n/a                    |

## Objectives:
The goal of this exercise is to write four metric functions (which are also available in **sklearn.metrics**) and to understand what they measure and how they are constructed.

You must implement the following fomulas: 

$$
\text{accuracy} = \frac{\text{tp} + \text{fn}}{\text{tp} + \text{fp} + \text{tn} + \text{fn}}
$$
$$
\text{precision} = \frac{\text{\text{tp}}}{\text{tp} + \text{fp}}
$$
$$
\text{recall} = \frac{\text{tp}}{\text{tp} + \text{fn}}
$$
$$
\text{F1score} = \frac{2 \times \text{precision} \times \text{recall}}{\text{precision} + \text{recall}}
$$

Where:
- $\text{tp}$ is the number of **true positives**
- $\text{fp}$ is the number of **false positives**
- $\text{tn}$ is the number of **true negatives**
- $\text{fn}$ is the number of **false negatives**

## Instructions:
For the sake of simplicity, we will only ask you to use two parameters.

In the `other_metrics.py` file, write the following functions as per the instructions below:
```python
def accuracy_score_(y, y_hat):
    """
    Compute the accuracy score.
    Args:
        y:a numpy.ndarray for the correct labels
        y_hat:a numpy.ndarray for the predicted labels
    Returns: 
        The accuracy score as a float.
        None on any error.
    Raises:
        This function should not raise any Exception.
    """

def precision_score_(y, y_hat, pos_label=1):
    """
    Compute the precision score.
    Args:
        y:a numpy.ndarray for the correct labels
        y_hat:a numpy.ndarray for the predicted labels
        pos_label: str or int, the class on which to report the precision_score (default=1)
    Returns: 
        The precision score as a float.
        None on any error.
    Raises:
        This function should not raise any Exception.
    """

def recall_score_(y, y_hat, pos_label=1):
    """
    Compute the recall score.
    Args:
        y:a numpy.ndarray for the correct labels
        y_hat:a numpy.ndarray for the predicted labels
        pos_label: str or int, the class on which to report the precision_score (default=1)
    Returns: 
        The recall score as a float.
        None on any error.
    Raises:
        This function should not raise any Exception.
    """

def f1_score_(y, y_hat, pos_label=1):
    """
    Compute the f1 score.
    Args:
        y:a numpy.ndarray for the correct labels
        y_hat:a numpy.ndarray for the predicted labels
        pos_label: str or int, the class on which to report the precision_score (default=1)
    Returns: 
        The f1 score as a float.
        None on any error.
    Raises:
        This function should not raise any Exception.
    """
```

## Examples:
```python
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score   

# Example 1:
y_hat = np.array([1, 1, 0, 1, 0, 0, 1, 1])
y = np.array([1, 0, 0, 1, 0, 1, 0, 0])

# Accuracy
## your implementation
accuracy_score_(y, y_hat)
## Output:
0.5
## sklearn implementation
accuracy_score(y, y_hat)
## Output:
0.5

# Precision
## your implementation
precision_score_(y, y_hat)
## Output:
0.4
## sklearn implementation
precision_score(y, y_hat)
## Output:
0.4

# Recall
## your implementation
recall_score_(y, y_hat)
## Output:
0.6666666666666666
## sklearn implementation
recall_score(y, y_hat)
## Output:
0.6666666666666666

# F1-score
## your implementation
f1_score_(y, y_hat)
## Output:
0.5
## sklearn implementation
f1_score(y, y_hat)
## Output:
0.5
```

```python
# Example 2:
y_hat = np.array(['norminet', 'dog', 'norminet', 'norminet', 'dog', 'dog', 'dog', 'dog'])
y = np.array(['dog', 'dog', 'norminet', 'norminet', 'dog', 'norminet', 'dog', 'norminet'])

# Accuracy
## your implementation
accuracy_score_(y, y_hat)
## Output:
0.625
## sklearn implementation
accuracy_score(y, y_hat)
## Output:
0.625

# Precision
## your implementation
precision_score_(y, y_hat, pos_label='dog')
## Output:
0.6
## sklearn implementation
precision_score(y, y_hat, pos_label='dog')
## Output:
0.6

# Recall
## your implementation
recall_score_(y, y_hat, pos_label='dog')
## Output:
0.75
## sklearn implementation
recall_score(y, y_hat, pos_label='dog')
## Output:
0.75

# F1-score
## your implementation
f1_score_(y, y_hat, pos_label='dog')
## Output:
0.6666666666666665
## sklearn implementation
f1_score(y, y_hat, pos_label='dog')
## Output:
0.6666666666666665
```


```python
# Example 3:
y_hat = np.array(['norminet', 'dog', 'norminet', 'norminet', 'dog', 'dog', 'dog', 'dog'])
y = np.array(['dog', 'dog', 'norminet', 'norminet', 'dog', 'norminet', 'dog', 'norminet'])

# Accuracy
## your implementation
accuracy_score_(y, y_hat)
## Output:
0.625
## sklearn implementation
accuracy_score(y, y_hat)
## Output:
0.625

# Precision
## your implementation
precision_score_(y, y_hat, pos_label='norminet')
## Output:
0.6666666666666666
## sklearn implementation
precision_score(y, y_hat, pos_label='norminet')
## Output:
0.6666666666666666

# Recall
## your implementation
recall_score_(y, y_hat, pos_label='norminet')
## Output:
0.5
## sklearn implementation
recall_score(y, y_hat, pos_label='norminet')
## Output:
0.5

# F1-score
## your implementation
f1_score_(y, y_hat, pos_label='norminet')
## Output:
0.5714285714285715
## sklearn implementation
f1_score(y, y_hat, pos_label='norminet')
## Output:
0.5714285714285715
```
