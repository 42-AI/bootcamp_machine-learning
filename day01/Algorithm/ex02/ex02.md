# Exercise 02 - Mutiples features and Normal Equation

|                         |                    |
| -----------------------:| ------------------ |
|   Turnin directory :    |  ex02              |
|   Files to turn in :    |  normal_equation_model.py  |
|   Authorize module :    |  numpy, matplotlib |
|   Forbidden module :    |  sklearn           |
|   Forbidden function :  |  LinearRegression  |
|   Remarks :             |  Read the doc      |

**Objectives:** 

* Reinforce the mathematical skills tackled in **Mathematical Delights**, especially the vectorized form of __matrix-matrix oparations__ (intermediate manipulation).
* Be able to perform  __linear cost function__ and what one names  __normal equation__ for a multiple features problem (and still knowing what you are doing of course obviously).

* Be able to implement vectorized methods **normal_eq** and **MAE** (for Mean absolute value) in order to perform a full linear regression.


**Instructions:**

We continue to $\stout{work}$ play with the same dataset (__spacraft_data.csv)__ given in the previous exercise. 
You will code a new method named **normal_eq** which will allows you to perform a fit of the dataset based on the method called __normal equation__.

The normal equation is:
$$
\pmb{\theta} = \left(\pmb{X}^{T} \pmb{X}\right)^{-1}\cdot\pmb{X}^T\pmb{Y}
$$

where $\pmb{X}$ is the training dataset matrix, $\pmb{\theta}$ the parameter's hypothesis vector, $\pmb{Y}$ are the output vector.

__For folks who do not understand what the meaning of the superscripts **T** and **-1** over a matrix, they respectively tell you we take the transpose and the inverse of the concerned matrix.
Do not worry, for those who do not know what it means or do not remember how to do, well you can still do the exercise because numpy will run the maths for you, but you will not understand what happens behind the scene for the moment.
Fortunaly, the mathematic workshops will be coming soon, what a wonderful opportunity to learn to do transposition and inversion of matrices, lucky you.__


You are expected to:
* Add the method **normal_eq** in the class **MyLinearRegression** following the prototype:
```python
def normalequation(self, X, Y))
	"""
	Description:
		Perform the normal equation to get the theta parameters of the hypothesis h.
	Args:
		X: __array_like__ or __sparse matrix__, shape(number of training examples, number of features)
		-> Samples(/training dataset) data from which we want to generate predicted values.
		Y: array, shape(number of training examples,)
		-> Target values of the training dataset.
	Returns:
		self: returns an instance of self.
		-> Will contains the attributes intercept_ and coef_ corresponding respectively to $\theta_0$ and the $N\times 1$ dimensional vector $[theta_1 \ldots \theta_N$. intercept_, coef_ are array and the latter is (number of features,) shaped. 

	"""
```

You will model the data and plot 2(/3) differents graphs:
* The graph with the data, the hypothesis $h_{\pmb{\theta}}^{LGD}(\pmb{X})$ obtained via linear gradient desent and $h_{\pmb{\theta}}^{NE}(\pmb{X})$ (see example figure 1),
* Plot of standard deviation $\sigma$ with respect to the theta's (figure 2).
* (Bonus, I dare you to do it !) A contour plot of the cost function $J(**\theta**)$ in function of the $\theta_1$ and $theta_2$ (see figure 3).

Standard deviation is identical to the  RMSE which is given by the following formula:
$$
\sigma = RMSE = \sqrt{\frac{1}{M}\sum{i=1}{M}\left(\hat{y}^{(i)} - y^{(i)}\right)^2}
$$
where $\hat{y}^{(i)}$ and $y^(i)$ are respectively the predicted output and the output with the ith training samples $x^{(i)}$.

Be sure to understand the underlying concept and be able to answer to those questions evaluators may ask:
* What are the advantages and drawbacks of the linear gradient descent and normal equation ?
* In which case, the methode normalequation cannot be used ?
* What infomation you get from the plot of standard deviation ?
