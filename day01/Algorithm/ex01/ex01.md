# Exercise 01 - Mutiples features and Linear Gradient Descent

|                         |                    |
| -----------------------:| ------------------ |
|   Turnin directory :    |  ex01              |
|   Files to turn in :    |  multi_linear\_model.py  |
|   Authorize module :    |  numpy, matplotlib |
|   Forbidden module :    |  sklearn           |
|   Forbidden function :  |  LinearRegression  |
|   Remarks :             |  Read the doc      |

**Objectives:** 

* Reinforce the mathematical skills tackled in **Mathematical Delights**, especially the vectorized form of __linear cost function__ and __gradient descent__ ($\text{\smallIf you were to lazy to do it exercise 00 \textit{cof cof}}$).
* Be able to perform  __linear cost function__ and the __linear gradient descent__ for a multiple features problem (and knowing what you are doing of course).

* Be able to implement vectorized methods **predict** and **fit** in order to perform a full linear regressions.


**Instructions:**

As you are able to perform a simple linear regression with one feature (well done!) it is time to dream bigger.
Lucky you are, we give you a new dataset with multiple features that you will find in the ressources attached.
The dataset is called __Spatial_General_Motor.csv__ which contains the prices of spacecrafts in function of multiples features.

Multiple features means you will need a multi-linear model as  hypothesis function h:
$$
h(X)= \pmb{X} \pmb{\theta}
  =\begin{bmatrix} x_0^{(1)} & \cdots & x_N^{(1)}\\ \vdots & \ddots & \vdots \\ x_0^{(m)} & \cdots & x_N^{(m)}  \end{bmatrix} \cdot \begin{bmatrix} \theta_0 \\ \vdots \\ \theta_N\end{bmatrix}
  = \sum{i=1}{N}\theta_ix_i
$$
where $\pmb{X}$ is the training dataset matrix, $\pmb{\theta}$ the parameter's vector, m is the number of training samples and N the number of features.


You are expected to:
* Add the method **fit** and *normalequation* in the class **MyLinearRegression** you started to create in the previous exercise (fit by hand is over), following the prototype:
```python
def fit(self, X, Y, n_cycle=100):
	"""
	Description:
		Fit the linear model by performing a gradient descent on the cost function.
	Args:
		X: __array_like__ or __sparse matrix__, shape(number of training examples, number of features)
		-> Samples(/training dataset) data from which we want to generate predicted values.
		Y: array, shape(number of training examples,)
		-> Target values of the training dataset.
		n_cycle: integer
		-> number of cycling in the descent gradient the method will perform.
	Returns:
		self: returns an instance of self.
		-> Will contains the attributes intercept_ and coef_ corresponding respectively to $\theta_0$ and the $N\times 1$ dimensional vector $[theta_1 \ldots \theta_N$. intercept_, coef_ are array and the latter is (number of features,) shaped. 
	Raises:
		This method should not raise any Exception.
	"""

def normalequation(self, X, Y))
	"""
	Description:
		Fit the linear model by calculating theta such as it minimize the cost function.
	Args:
		X: __array_like__ or __sparse matrix__, shape(number of training examples, number of features)
		-> Samples(/training dataset) data from which we want to generate predicted values.
		Y: array, shape(number of training examples,)
		-> Target values of the training dataset.
	Returns:
		self: returns an instance of self.
		-> Will contains the attributes intercept_ and coef_ corresponding respectively to $\theta_0$ and the $N\times 1$ dimensional vector $[theta_1 \ldots \theta_N$. intercept_, coef_ are array and the latter is (number of features,) shaped. 

	"""
```


You will model the data and plot 3 differents graphs:
* A contour plot of the cost function $J(**\theta**)$ in function of the $\theta_1$ and $theta_2$ (see example in figure 1),
* The graph with the data, the hypothesis $h_{\pmb{\theta}}^{LGD}(\pmb{X})$ obtained via linear gradient desent and $h_{\pmb{\theta}}^{NE}(\pmb{X})$ (see example figure 2),
* Plot of standard deviation $\sigma$ with respect to the theta's.

Standard deviation is identical to the  RMSE which is given by the following formula:
$$
\sigma = RMSE = \sqrt{\frac{1}{2M}\sum{i=1}{M}\left(\hat{y}^{(i)} - y^{(i)}\right)^2}
$$
where $\hat{y}^{(i)}$ and $y^(i)$ are respectively the predicted output and the output with the ith training samples $x^{(i)}$.

And finally, calculate the covariance of the differents features.

Be sure to understand the underlying concept and be able to answer to those questions evaluators may ask:
* What are the advantages and drawbacks of the linear gradient descent and normal equation ?
* What infomation you get from the plot of standard deviation ?
* What can you tell based on the differentes values of covariance ?
