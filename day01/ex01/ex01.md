# Exercise 01 - Mutiples features and Linear Gradient Descent

|                         |                    |
| -----------------------:| ------------------ |
|   Turnin directory :    |  ex01              |
|   Files to turn in :    |  multi_linear\_model.py  |
|   Authorize module :    |  numpy, matplotlib |
|   Forbidden module :    |  sklearn           |
|   Forbidden function :  |  LinearRegression  |
|   Remarks :             |  Read the doc      |

**Objectives:** 

* Reinforce the mathematical skills tackled in **Mathematical Delights**, especially the vectorized form of __linear cost function__ and __gradient descent__.
* Be able to manipulate the __linear cost function__ and the __linear gradient descent__ for a multiple features problem (and knowing what you are doing of course).
* Be able to implement vectorized methods **.fit** and **.rmse** in order to perform a full multi-linear regressions.
* Be able to visualize the different objects via graphics and extract basics informations based on its.


**Instructions:**

As you are able to perform a simple linear regression with one feature (well done!) it is time to dream bigger.
Lucky you are, we give you a new dataset with multiple features that you will find in the ressources attached.
The dataset is called __"spacecraft_data.csv"__ which contains the prices of spacecrafts in function of multiples features (multiple features means you will need a multi-linear model but hold on for the moment). A description of the dataset is provided in the file __"spacecraft_data_description.txt"__.


## Part One: single linear regression

As a starter, you will try to fit the data with a single linear regression and see what we get.
Thus, your hypothesis h(X) would be given by:
$$
h(X) = X \cdot \theta = \begin{bmatrix} 1 & x_1^{(1)} \\ \vdots & \vdots \\ 1 & x_1^{(M)}\end{bmatrix}\cdot\begin{bmatrix}\theta_0 \\ \theta_1 \end{bmatrix} = \begin{bmatrix} \theta_0 + \theta_1x_1^{(1)} \\ \vdots \\ \theta_0 + \theta_1x_1^{(M)} \end{bmatrix}
$$
assuming $X$ is an array of dimension $M \times (N+1)$ (M corresponding to the number of training examples and N to the number of features). Here N is equal to 1.
As you can notice, an extra column of 1 is add at the beginning of the X matrix such as the matrix product gives a vector of dimension $M \times 1$ where the ith component is $\theta_0 + \theta_1x_1^{(i)}$. This is just a trick to perform the calculation of h(X) in the vectorized way.

You are expected to:
* Add the methods **.fit** and **.rmse** in the class **__MyLinearRegression__** you started to create in the previous exercise (fit by hand is over), following the prototype:
```python
def fit(self, X, Y, alpha = 5e-3, n_cycle=10000):
	"""
	Description:
		Fit the linear model by performing a gradient descent on the cost function.
	Args:
		X: has to be a numpy.ndarray, a vector of dimension (number of training examples, number of features).
		Y: has to be a numpy.ndarray, a vector of dimension (number of training examples,1).
		alpha: has to be a float.
		n_cycle: has to be an integer.
	Returns:
		No return
	Raises:
		This method should not raise any Exception.
	"""

def rmse(self, X, Y):
	"""
	Description:
		Calculate the RMSE (Root Mean Squared Error) of the set of predicted values with respect to Y.
	Args:
		X: has to be a numpy.ndarray, a vector of dimension (number of training examples, number of features).
		Y: has to be a numpy.ndarray, a vector of dimension (number of training examples,1).
	Return:
		rmse: has to be a float.
	Raises:
		This method should not raise any Exception.
	"""
```
I strongly encourage you to code the **.fit** and **.rmse** methods in a vectorized way...

You are expected to :
* The graph with the data, the hypothesis $h_{{\theta}}^{LGD}(age)$ obtained via linear gradient desent versus age (see example figure 1),

<img src="day01/assets/ex01_price_vs_age_part1.png" />

* The graph with the data, the hypothesis $h_{{\theta}}^{LGD}(thrust)$ obtained via linear gradient desent versus thrust (see example figure 2),

<img src="day01/assets/ex01_price_vs_thrust_part1.png" />

* The graph with the data, the hypothesis $h_{{\theta}}^{LGD}(Tmeters)$ obtained via linear gradient desent versus Tmeters (see example figure 3),

<img src="day01/assets/ex01_price_vs_Tmeters_part1.png" />

* Calculate the RMSE for the different features.

**Remarks**
* You may obtain after the fit $\theta$=[nan, nan] of [inf, inf]. It may come from a too large learning rate. Be aware that you set the number of cycle of the fitting process, but it does not guaranty you the fit is the best or good. For this purpose, a tolerance parameter has to be used.
* You can try to reduce the RMSE by tuning the learning rate and the number of cycles of the fit.

**Examples**
```python
>>>import pandas as pd
>>>import numpy as np
>>>form mylinearregression import MyLinearRegression as MyLR
>>>
>>>data = pd.read_csv("spacecraft_data.csv")
>>>[...]
>>>myRL_age = MyLR([1.0, 1.0])
>>>myLRage.fit(X[:,0].reshape(-1,1), Y, alpha = 5e-3, n_cycle = 50000)
>>>
>>>RMSE_age = myLRage.rmse(X[:,0].reshape(-1,1),Y)
>>> print(RMSE_age)
220.29...
```

Are the fits with a single variable are precised ? Why ? (What did I say a the beginning ?)
What the purpose to represent the "rmse" in function of a feature ?


## Part Two: Multilinear Regression (A New Hope)

Now, it is time for your first multilenear regression !
As you might expected, the formula of the hyphothesis change a little and is given by:

$$
h(X)= {X} {\theta}
  =\begin{bmatrix} x_0^{(1)} & \cdots & x_N^{(1)}\\ \vdots & \ddots & \vdots \\ x_0^{(M)} & \cdots & x_N^{(M)}  \end{bmatrix} \cdot \begin{bmatrix} \theta_0 \\ \vdots \\ \theta_N\end{bmatrix}
  = \begin{bmatrix}\sum_{i=1}^{N}\theta_ix_i^{(1)} \\ \vdots \\ \sum_{i=1}^{N}\theta_ix_i^{(M)} \end{bmatrix}
$$

where ${X}$ is the training dataset matrix, ${\theta}$ the coefficients vector, m is the number of training samples and N the number of features.

But It should not change the methods you coded, hopefully !

**Examples**
```python
>>>import pandas as pd
>>>import numpy as np
>>>form mylinearregression import MyLinearRegression as MyLR
>>>
>>>data = pd.read_csv("spacecraft_data.csv")
>>>X = np.array(data[['Age','Thrust_power','Terameters']])
>>>Y = np.array(data[['Sell_price']])
>>>my_lreg = MyLR([1.0, 1.0, 1.0, 1.0])
>>>my_lreg.fit(X,Y, alpha = 1e-5, n_cycle = 500000)
>>>my_lreg.rmse(X,Y)
87.012...
```

You are expected to:
* Plot the output and predicted output on the same graph in function of the age(see figures 3a, 3b and 3c),
<img src="day01/assets/ex01_price_vs_age_part2.png" />

* Plot the output and predicted output on the same graph in function of the age, thrust power and distance (see figures 3a, 3b and 3c),
<img src="day01/assets/ex01_price_vs_thrust_part2.png" />

* Plot the output and predicted output on the same graph in function of the age, thrust power and distance (see figures 3a, 3b and 3c),
<img src="day01/assets/ex01_price_vs_Tmeters_part2.png" />

* Calculate the RMSE of your new fit, and compare it with the RMSE of the previous part.
What conclusion can you do ?

## Questions:

Be sure to understand the underlying concept and be able to answer to those questions evaluators may ask:
* What is the learning rate ?
* What allows the linear gradient descent ?
* What information standard deviation gives you ?

