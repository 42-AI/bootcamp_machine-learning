# Exercise 03 - Learning Rate and Quadratic Hypothesis

|                         |                    |
| -----------------------:| ------------------ |
|   Turnin directory :    |  ex03              |
|   Files to turn in :    |  so_much_hyp.py    |
|                         |  alpha.py          |
|   Authorize module :    |  numpy, matplotlib |
|   Forbidden module :    |  sklearn           |
|   Forbidden function :  |  LinearRegression  |
|   Remarks :             |  Read the doc      |

**Objectives:** 

* Discover the non-linear dependency to elaborate more complex hypothesis and reinforce mathematical skills at the same time.
* Gain a deeper understanding of the learning rate.


**Instructions:**
__ With great power comes great responsability __ (Uncle Ben).

Uncle Ben was right, a large power thrust comes a risk of spacecraft crash.
With the apparition of individual spacecraft, the number of wild rides exloses, especially in the asteroid Saturn Belt.
Experts are concerned with the impact of these rides and worried about the consequence on the number of asteroids orbiting and the number of asteroids leaving the orbit of Saturn !

For this last exercise you have to find a model to describe the number of asteroids leaving the orbit of Saturn due to wild rides.
The dataset related to this important problem is named __"saturn_asteroids.csv"__. The data description is in the file __"saturn_asteroids_description.txt"__.
Also, you have 2 really nice graphical representations just below.

<img src="day01/assets/ex03_Figure_1.png" />

<img src="day01/assets/ex03_Figure_2.png" />

You will use the methods you code during the day to fit the data.
The hypothesis function you will need is a more complex one. This time you have to find out the best model to fit the data.
Do not worry, it is not too complicated.

You have to justify your hypothesis (know what and why you are doing).

## Part One - Find the best hypothesis

For this part, you will write your code in the file __"so_much_hyp.py"__.
You can code new methods to help you to justify the choice of the hypothesis but it is not compulsory.
The major work for this exercise concerns the data.

You are expected to:
* Find a good hypothesis and justify that your model is a good one.

**Hints:**
Take a look to the metrics (the RMSE for example) with respect to the features.
Notice that the graphs $\sqrt{1/M(\widehat{y^{(i)}} - y^{(i)})^2}$ is not the rmse that we plot precisely, but the term in the sum.
In regard with what I just say, you should consider to add a method maybe named **rmse_components**, If you do not know what I mean maybe your neighbours know ...

Here, what you can get:
```python
>>>import pandas as pd
>>>import numpy as np
>>>data = pd.read_csv("data.csv")
[...]
>>>hypo.normalequation(X,Y)
>>>print("valeur du RMSE: ",hypo.rmse(X,Y))
valeur du RMSE: 1.4513e-11
```

**Bonus question**
Here a mathematical question, it is a bit challenging, more if you are not at ease with the linear algebra and the manipulation of the functions.
Even with the best hypothesis (if you have the same formula that the one I used to generate the data) you should have a difference between you predicted output and the real output of the order of $10^{-10}$ or even less. Can you explain where this trend come from ?

**HINT:** to see clearly the trend, you can visualize the difference between $\hat{y(X)} - y(X)$


## Part Two - The learning rate $\alpha$

For this part you will write your code in the file __alpha.py__.
We will focus on the concept of learning rate, in consequence, you will have to fit with the method __MyLinearRegression.fit__ you codedpreviously.

You are expected to:
* Find a good learning rate $\alpha$ to fit the data with the hypothesis you have chosen in the previous part, you choice will be motivated by the convergence reached in the minimum cycle (see figure 3).

__figure is missing__

Be sure to understand the underlying concept and be able to answer to those questions evaluators may ask:
* What happens if you choose a learning rate too big ?
* How the metric and the $\theta$'s evolve with a slighly too big learning rate ?
* How the convergence evolves in function of the cycle ? Do you have any idea to speed up the convergence as the numbre of cycle increase ? (It is an open question, be creative and smart).
