# Exercise 03 - Linear Gradient - Iterative Version

|                         |                    |
| -----------------------:| ------------------ |
|   Turn-in directory :   |  ex03              |
|   Files to turn in :    |  gradient.py       |
|   Forbidden functions : |  None              |
|   Remarks :             |  n/a               |

## Objectives:

You must write a function that computes the *__gradient__* of the cost function.  
It must compute a partial derivative with respect to each theta parameter separately, and return the vector gradient.  
The partial derivatives can be calculated with the following formulas:  

$$
\nabla(J)_0 = \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})
$$

$$
\nabla(J)_1 = \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})x^{(i)}
$$

Where:  
- $\nabla(J)$ is the gradient vector of size 2 * 1, (this strange symbol : $\nabla$ is called nabla)
- $x$ is a vector of dimension m * 1
- $y$ is a vector of dimension m * 1
- $x^{(i)}$ is the $i^{th}$ component of vector $x$
- $y^{(i)}$ is the $i^{th}$ component of vector $y$
- $\nabla(J)_j$ is the $j^{th}$ component of $\nabla(J)$
- $h_{\theta}(x^{(i)})$ corresponds to the model's prediction of $y^{(i)}$

## Hypothesis Notation:
$h_{\theta}(x^{(i)})$ is the same as what we previously noted $\hat{y}^{(i)}$.  
The two notations are equivalent. They represent the model's prediction (or estimation) of the ${y}^{(i)}$ value. If you follow Andrew Ng's course material on Coursera, you will see him using the former notation.

As a reminder :
$h_{\theta}(x^{(i)}) = \theta_0 + \theta_1x^{(i)}$

## Instructions:
In the gradient.py file create the following function as per the instructions given below:
```python
def simple_gradient(x, y, theta):
    """Computes a gradient vector from three non-empty numpy.ndarray, without any for-loop. The three arrays must have compatible dimensions.
    Args:
      x: has to be an numpy.ndarray, a vector of dimension m * 1.
      y: has to be an numpy.ndarray, a vector of dimension m * 1.
      theta: has to be an numpy.ndarray, a 2 * 1 vector.
    Returns:
      The gradient as a numpy.ndarray, a vector of dimension 2 * 1.
      None if x, y, or theta are empty numpy.ndarray.
      None if x, y and theta do not have compatible dimensions.
    Raises:
      This function should not raise any Exception.
    """
```

## Examples:
```python
import numpy as np
x = np.array([12.4956442, 21.5007972, 31.5527382, 48.9145838, 57.5088733])
y = np.array([37.4013816, 36.1473236, 45.7655287, 46.6793434, 59.5585554])

# Example 0:
theta1 = np.array([2, 0.7])
simple_gradient(x, y, theta1)
# Output:
array([21.0342574, 587.36875564])

# Example 1:
theta2 = np.array([1, -0.4])
simple_gradient(x, y, theta2)
# Output:
array([58.86823748, 2229.72297889])
```
