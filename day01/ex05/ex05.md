# Exercise 05 - Gradient Descent

|                         |                     |
| -----------------------:| ------------------  |
|   Turn-in directory :   |  ex05               |
|   Files to turn in :    |  fit.py             |
|   Authorized modules :  |  numpy              |
|   Forbidden functions : |  all functions that calculate derivatives for you  			      |

## Objectives: 
* Be able to explain what it means to __*fit*__ a Machine Learning model.
* Implement a function that performs **Linear Gradient Descent** (LGD).


## Instructions:
In this exercise, you will implement linear gradient descent to fit your model to the dataset. As previously explained, linear gradient descent is an iterative algorithm whereby at each step, both $\theta$ parameters are slightly moved in the opposite direction of the gradient (which explains the name *gradient descent*).

The basics idea of the algorithm is the following:

$$
\begin{matrix}
\text{repeat until convergence} \hspace{1cm}\{\\
	\theta_0 := \theta_0 - \alpha \nabla(J)_0  \\ 
	\theta_1 := \theta_1 - \alpha \nabla(J)_1\\
	\} \hspace{0.5cm} \text{ update $\theta_0$ and $\theta_1$ simultaneously}
\end{matrix}
$$
A few remarks on this algorithm:
- Here, $\alpha$ (alpha) is what we call the *learning rate*. It is a small number (usually between 0 and 1) that you will need to decrease the size of your steps and avoid overshooting getting *past* the minimum.
- In your implementation, you will not directly verify whether convergence has been reached at each iteration. You will instead set a number of cycles that is sufficiently large for your gradient descent to converge.


You are expected to code a function named **fit_** as per the instructions below:
``` python
def fit_(x, y, theta, alpha, n_cycles):
	"""
	Description:
		Performs a fit of y(output) with respect to x.
	Args:
		theta: has to be a numpy.ndarray, a vector of dimensions 2 * 1.
		x: has to be a numpy.ndarray, a vector of dimensions m * 1: (number of training examples, 1).
		y: has to be a numpy.ndarray, a vector of dimension m * 1: (number of training examples, 1).
		alpha: has to be a float, the learning rate
		n_cycles: has to be an int, the number of iterations done during the gradient descent
	Returns:
		new_theta: numpy.ndarray, a vector of dimension 2 * 1.
		None if there is a matching dimension problem.
	Raises:
		This function should not raise any Exception.
	"""
		... your code here ...
```
Hopefully, you have already written a function to calculate the linear gradient.  

## Examples:
```python
import numpy as np
x = np.array([[12.4956442], [21.5007972], [31.5527382], [48.9145838], [57.5088733]])
y = np.array([[37.4013816], [36.1473236], [45.7655287], [46.6793434], [59.5585554]])
theta= np.array([1, 1])

# Example 0:
theta1 = fit_(x, y, alpha=5e-8, n_cycle = 1500000)
theta1
# Output:
array([[1.40709365],
       [1.1150909 ]])

# Example 1:
predict(x, theta1)
# Output:
array([[15.3408728 ],
       [25.38243697],
       [36.59126492],
       [55.95130097],
       [65.53471499]])
```

## Remarks:
- You can generate more examples by generating an $x$ array with random values and declaring the corresponding $y$ as a linear expression of $x$. You can then find out what theta parameters your model comes up with. 
- It is possible that $\theta_0$ and $\theta_1$ become "[nan]". In that case, it means you probably used a learning rate that is too large.