# Exercise 05 - Gradient Descent

|                         |                     |
| -----------------------:| ------------------  |
|   Turn-in directory :   |  ex05               |
|   Files to turn in :    |  fit.py             |
|   Authorized modules :  |  numpy              |
|   Forbidden functions : |  all functions that perform derivative at your place         |
|   Helpfuk links :       |  https://www.coursera.org/learn/machine-learning/home/week/1 |
|                         | https://www.coursera.org/learn/machine-learning/home/week/2  |
|   Hint :                |  Focus on the "Parameter Learning" section of week 1         |
|                         | and "Multivariate Linear Regression" section of week 2"      |

**Objectives :** 
* Be able to explain what is a fit in the machine learning context.
* Be able to implement a funcion which will perform a linear gradient descent (LGD).


**Instructions :**
In this exercise you will be interested with the concept of linear gradient descent to perform fit of dataset. The linear gradient descent allows to correct the coefficients $\theta$ thanks to the gradient of the cost function.
You are expected to code a function named __fit\___ as per the instructions bellow:
``` python
def fit_(x, y, theta, alpha, n_cycles):
	"""
	Description:
		Performs a fit of y(output) with respect to x.
	Args:
		theta: has to be a numpy.ndarray, a vector of dimension 2 * 1.
		x: has to be a numpy.ndarray, a matrix of dimension m * 1: (number of training examples, 1).
		y: has to be a numpy.ndarray, a vector of dimension m * 1: (number of training examples, 1).
		alpha: has to be a float, the learning rate
		n_cycles: has to be an int, the number of iterations done during the gradient descent
	Returns:
		new_theta: numpy.ndarray, a vector of dimension (number of the features +1,1).
		None if there is a matching dimension problem.
	Raises:
		This function should not raise any Exception.
	"""
		... your code here ...
```
Hopefully, you have already code a function to calculate the linear gradient.  


**Examples :**
```python
import numpy as np
from mylinearregression import MyLinearRegression as MyLR
x = np.array([[12.4956442], [21.5007972], [31.5527382], [48.9145838], [57.5088733]])
y = np.array([[37.4013816], [36.1473236], [45.7655287], [46.6793434], [59.5585554]])
theta= np.array([1,1])

# Example 0:
theta1 = fit(x, y, alpha=5e-8, n_cycle = 1500000)
theta1
# Output:
array([[1.40709365],
       [1.1150909 ]])

# Example 1:
predict(x, theta1)
# Output:
array([[15.3408728 ],
       [25.38243697],
       [36.59126492],
       [55.95130097],
       [65.53471499]])
```

**Remarks :**
You can generate other examples by choosing arbitrary x array and declare y as linear expression of the x columns. Notice also that you can have the components of theta becoming "[nan]". In that case it means you probably used a too big learning rate.