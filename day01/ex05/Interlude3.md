## Gradient Descent

We calculated the *gradient*, which indicates whether we should increase or decrease $\theta_0$ and $\theta_1$ in order to reduce the cost.   
What we have to do next is to move the theta parameters in that direction, step by step, until we reach the minimum. This iterative process is called **Gradient Descent**. It is a very common way to improve the performance of Machine Learning models. 

### Helpful ressources: 
- **Parameter learning section:** https://www.coursera.org/learn/machine-learning/home/week/1

