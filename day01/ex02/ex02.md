# Exercise 02 - Mutiples features and Normal Equation

|                         |                    |
| -----------------------:| ------------------ |
|   Turnin directory :    |  ex02              |
|   Files to turn in :    |  normal_equation_model.py  |
|   Authorize module :    |  numpy, matplotlib |
|   Forbidden module :    |  sklearn           |
|   Forbidden function :  |  LinearRegression  |
|   Remarks :             |  Read the doc      |

**Objectives:** 

* Reinforce the mathematical skills tackled in **Mathematical Delights**, especially the vectorized form of __matrix-matrix oparations__ (intermediate manipulation).
* Be able to perform  __linear cost function__ and what one names  __normal equation__ for a multiple features problem (and still knowing what you are doing of course obviously).

* Be able to implement vectorized methods **normalequation** and **r2score** (for R2-score value) in order to perform a full linear regression.


**Instructions:**

We continue to play with the same dataset (__spacraft_data.csv__) given in the previous exercise. 
You will code a new method named **normalequation** which will allows you to perform a fit of the dataset based on the method called __normalequation__.

The normal equation is:

$$
{\theta}_{NE} = \left({X}^{T} {X}\right)^{-1}\cdot{X}^T{Y}
$$

where ${X}$ is the training dataset matrix, ${\theta}_{NE}$ the parameter's hypothesis vector obtained with the normal equation, ${Y}$ are the output vector.

__For folks who do not understand what the meaning of the superscripts **T** and **-1** over a matrix, they respectively tell you we take the transpose and the inverse of the concerned matrix.
Do not worry, for those who do not know what it means or do not remember how to do, well you can still do the exercise because numpy will run the maths for you, but you will not understand what happens behind the scene for the moment.
Fortunaly, the mathematic workshops will be coming soon, what a wonderful opportunity to learn to do transposition and inversion of matrices, lucky you.__


You are expected to:
* Add the methods **normalequation** and **r2score** in the class **__MyLinearRegression__** as per the instructions given below:
```python
def normalequation(self, X, Y))
	"""
	Description:
		Perform the normal equation to get the theta parameters of the hypothesis h and stock them in self.theta.
	Args:
		X: has to be a numpy.ndarray, a matrix of dimension (number of training examples, number of features)
		Y: has to be a numpy.ndarray, a vector of dimension (number of training examples,1)
	Returns:
		No return expected.
	Raises:
		This method should not raise any Exceptions.
	"""

def r2score(self, X, Y):
	"""
	Description:
		Calculate the R2-score between predicted values and Y.
	Args:
		X: has to be a numpy.ndarray, a matrix of dimension (number of training examples, number of features)
		Y: has to be a numpy.ndarray, a vector of dimension (number of training examples,1)
	Return:
		r2score: float.
		None if the dimension of Y and X does not match.
	Raises:
		This method should not raise any Exceptions.
	"""
```

You will model the data and plot differents graphs (yes again):
* The graph with the data, the hypothesis $h_{{\theta}}^{LGD}({X})$ obtained via linear gradient descent and $h_{{\theta}}^{NE}({X})$ (see example figure 1) with respect to the feature of your choice,

<img src="day01/assets/Figure_1_Sellprice_ne_lgd_vs_age.png" />

* Calculate the R2-score of the data and the predicted data generate with the model train with the linear gradient descent, then with normal equation technique.

The formula for the R2-score is given by the following formula:

$$
R^2 = 1 - \frac{\frac{1}{N}\sum_{i=1}^{N}\left(y^{(i)}-\widehat{y}^{(i)}\right)^2 }{\frac{1}{N}\sum_{i=1}^{N}\left(y^{(i)}-\mu\right)^2}
$$

where $\widehat{y}^{(i)}$ and $y^(i)$ are respectively the predicted output and the output with the ith training samples $x^{(i)}$. $\mu$ is the mean value of the output $y$.

**Example**
```python
>>>import pandas as pd
>>>import numpy as np
>>>from mylinearregression import MyLinearRegression as MyLR
>>>data = pd.read_csv("spacecraft_data.csv")
>>>[...]
>>>myLR_ne = MyLR([1., 1., 1., 1.])
>>>myLR_lgd = MyLR([1., 1., 1., 1.])
>>>myLR_lgd.fit(X,Y)
>>>myLR_ne.normalequation(X,Y)
>>>print("R2-score value: ",myLR_ne.r2score(X,Y))
0.787659...
>>>print("R2-score value: ",myLR_lgd.r2score(X,Y))
0.787619...
```

Be sure to understand the underlying concept and be able to answer to those questions evaluators may ask:
* What are the advantages and drawbacks of the linear gradient descent and normal equation ?
* In which case, the method normal equation cannot be used ?
* What is the information that the R2-score give you and what are the informations that the R2-score does not give you ?
