## Question time!

Are you able to clearly and simply explain:

1 - When we process the training examples, why are we adding a column of *ones* to the left of the $x$ vector (or $X$ matrix) when we use the linear algebra trick?   

2 - Why does the cost function square the distance between the data points and their predicted values?

3 - What does the cost function value represent?

4 - Toward which value would you like the cost function to tend to? What would it mean? 
