# Exercise 02 - Mutiples features and Normal Equation

|                         |                    |
| -----------------------:| ------------------ |
|   Turnin directory :    |  ex02              |
|   Files to turn in :    |  normal_equation_model.py  |
|   Authorize module :    |  numpy, matplotlib |
|   Forbidden module :    |  sklearn           |
|   Forbidden function :  |  LinearRegression  |
|   Remarks :             |  Read the doc      |

**Objectives:** 

* Reinforce the mathematical skills tackled in **Mathematical Delights**, especially the vectorized form of __matrix-matrix oparations__ (intermediate manipulation).
* Be able to perform  __linear cost function__ and what one names  __normal equation__ for a multiple features problem (and still knowing what you are doing of course obviously).

* Be able to implement vectorized methods **normalequation** and **rscore** (for R-score value) in order to perform a full linear regression.


**Instructions:**

We continue to __work__ play with the same dataset (__spacraft_data.csv__) given in the previous exercise. 
You will code a new method named **normalequation** which will allows you to perform a fit of the dataset based on the method called __normalequation__.

The normal equation is:

$$
{\theta}_{NE} = \left({X}^{T} {X}\right)^{-1}\cdot{X}^T{Y}
$$

where ${X}$ is the training dataset matrix, ${\theta}_{NE}$ the parameter's hypothesis vector obtained with the normal equation, ${Y}$ are the output vector.

__For folks who do not understand what the meaning of the superscripts **T** and **-1** over a matrix, they respectively tell you we take the transpose and the inverse of the concerned matrix.
Do not worry, for those who do not know what it means or do not remember how to do, well you can still do the exercise because numpy will run the maths for you, but you will not understand what happens behind the scene for the moment.
Fortunaly, the mathematic workshops will be coming soon, what a wonderful opportunity to learn to do transposition and inversion of matrices, lucky you.__


You are expected to:
* Add the methods **normalequation** and **rscore** in the class **MyLinearRegression** following the prototype:
```python
def normalequation(self, X, Y))
	"""
	Description:
		Perform the normal equation to get the theta parameters of the hypothesis h and stock them in self.theta.
	Args:
		X: __array_like__ or __sparse matrix__, shape(number of training examples, number of features)
		-> Samples(/training dataset) data from which we want to generate predicted values.
		Y: array, shape(number of training examples,)
		-> Target values of the training dataset.
	Returns:
		None.
	Raises:
		This method should not raise any Exceptions.
	"""

def rscore(self, X, Y):
	"""
	Description:
		Calculate the R-score of the set of predicted values with respect to Y.
	Args:
		X: __array_like__ or __sparse matrix__, shape(number of training examples, number of features)
		-> Samples(/training dataset) from which we want to generate predicted values.
		Y: array, shape(number of training examples,)
		->
	Return:
		rscore: float.
		-> R-score of self.predict(X) with respect to Y.
	"""
```

You will model the data and plot differents graphs (yes again):
* The graph with the data, the hypothesis $h_{{\theta}}^{LGD}({X})$ obtained via linear gradient desent and $h_{{\theta}}^{NE}({X})$ (see example figure 1) with respect to the feature of your choice,

<img src="day01/assets/Figure_1_Sellprice_ne_lgd_vs_age.png" />

* Calculate the R-score of the gradient descent model and of the normal equation with respect to the feature of your choice.

The formula for the R-score is given by the following formula:

$$
R^2 = 1 - \frac{\frac{1}{N}\sum_{i=1}^{N}\left(y^{(i)}-\hat{y}^{(i)}\right)^2 }{\frac{1}{N}\sum_{i=1}^{N}\left(y^{(i)}-\mu\right)^2}
$$

where $\hat{y}^{(i)}$ and $y^(i)$ are respectively the predicted output and the output with the ith training samples $x^{(i)}$. $\mu$ is the mean value of the output $y$.

**Example**
```python
>>>import pandas as pd
>>>import numpy as np
>>>from mylinearregression import MyLinearRegression as MyLR
>>>data = pd.read_csv("spacecraft_data.csv")
>>>[...]
>>>myLR_ne = MyLR(4)
>>>myLR_lgd = MyLR(4)
>>>myLR_lgd.fit(X,Y)
>>>myLR_ne.normalequation(X,Y)
>>>print("R2-score value: ",myLR_ne.rscore(X,Y))
0.787659...
>>>print("R2-score value: ",myLR_lgd.rscore(X,Y))
0.787619...
```

Be sure to understand the underlying concept and be able to answer to those questions evaluators may ask:
* What are the advantages and drawbacks of the linear gradient descent and normal equation ?
* In which case, the method normal equation cannot be used ?
* What is the information that the R-score give you and what are the informations that the R-score does not give you ?
