# Exercise 02 - Descent Gradient

|                         |                     |
| -----------------------:| ------------------  |
|   Turnin directory :    |  ex02               |
|   Files to turn in :    |  fit.py             |
|   Authorize module :    |  numpy              |
|   Forbidden function :  |  all functions that perform derivative at your place         |
|   Links :               |  https://www.coursera.org/learn/machine-learning/home/week/1 |
|                         | https://www.coursera.org/learn/machine-learning/home/week/2  |
|   Hint :                |  Focus on the "Parameter Learning" section of week 1         |
|                         | and "Multivariate Linear Regression" section of week 2"      |

**Objectives :** 

* Reinforce the mathematical skills tackled in **Mathematical Delights**, especially the __matrix-matrix operations__.
* Be able to explain what is the fit in the machine learning context.
* Be able to implement a funcion which will perform a linear gradient descent (LGD).


**Instructions :**

In this exercise you will be interested with the concept of linear gradient descent to perform fit of dataset.
The linear gradient descent allows to correct the coefficients $\theta$ thanks to the gradient of the cost function.

You are expected to code a function named __fit\___ as per the instructions bellow:

``` python
def fit_(theta, X, Y):
	"""
	Description:
		Perform a fit of Y with respect to X.
	Args:
		theta: has to be a numpy.ndarray, a vector of dimension (number of features + 1, 1).
		X: has to be a numpy.ndarray, a matrix of dimension (number of training examples, number of features).
		Y: has to be a numpy.ndarray, a vector of dimension (number of training examples, 1).
	Returns:
		new_theta: numpy.ndarray, a vector of dimension (number of the training examples,1).
		None if there is a matching dimension problem.
	Raises:
		This function should not raise any Exception.
	"""
		... your code here ...
```
Hopefully, you have already code a function to calculate the linear gradient (I say that because you might have forgot).

**Examples :**
```python
>>>import numpy as np
>>>X1 = np.array([[0.], [1.], [2.], [3.], [4.]])
>>>Y1 = np.array([[2.], [6.], [10.], [14.], [18.]])
>>>theta1 = np.array([[1.], [1.]])
>>>theta1 = fit_(theta1, X1, Y1, alpha = 0.01, n_cycle=2000)
>>>theta1
array([2.0023..], [6.002..], [10.0007..], [13.99988..], [17.9990..])
>>>
>>>X2 = np.array([[0.2, 2., 20.], [0.4, 4., 40.], [0.6, 6., 60.], [0.8, 8., 80.]])
>>>Y2 = np.array([[19.6.], [-2.8], [-25.2], [-47.6]])
>>>theta2 = np.array([[4.], [3.], [2.], [1.]])
>>>theta2 = fit_(theta2, X2, Y2, alpha = 0.005, n_cycle=2000)
>>>theta2
array([[19.5938..], [-2.8030..], [-25.1999..], [-477.5969..]])
```


**Remarks :**
You can generate other examples by choosing arbitrary X array and declare an Y an linear expression of the X column. Notice also that you can find [nan] the components of theta. In that case you probably used a too big learning rate.


**Questions :**
Be sure to understand the underlying concept and be able to answer to those questions during your evaluation:
* Can you explain the different step in the fit method (hint: you have to talk about J, it gradient and the theta)?
* What happens if you choose a too big learning rate ?
* What can you say if you choose a very small learning rate and a reasonnable number of cycle ?
