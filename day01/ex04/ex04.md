# Exercise 04 - Linear Regression

|                         |                    |
| -----------------------:| ------------------ |
|   Turn-in directory :   |  ex04              |
|   Files to turn in :    |  linear\_model.py  |
|   Authorized modules :  |  NumPy, Matplotlib |
|   Forbidden modules :   |  sklearn           |
|   Remarks :             |  Read the doc      |

## Objectives:

* Reinforce the mathematical skills tackled in **Mathematical Delights**, especially the __matrix-matrix operations__.
* Be able to perform a fit with a very small dataset, knowing the hypothesis function h.
* Manipulate the cost function J, plot it and briefly analyze the plot.


## Instructions:

You can find in the ressources a tiny dataset called __"are_blue_pills_magics.csv"__ which give you mathematical performance of patients in function of the quantity of the "blue pills" they took before the test. You have a description of the data in the file named __"are_blue_pills_magics.txt"__.
As hypothesis function h, you will choose:

$$
h(x) = \theta_0 + \theta_1x
$$

Where x is the variable, $\theta_0$ and $\theta_1$ are the coefficients of the hypothesis. The hypothesis is a function of x. It gives you the predicted values usually note $\hat{y^{(i)}}$ or $y_{pred}$.

You will model the data and plot 2 differents graphs:
* The graph with the data and the best hypothesis you find for the spacecraft piloting score versus the quantity of "blue pills" (see example figure 1)

<img src="day01/assets/ex04_score_vs_bluepills.png" />

Figure 1: Evolution of the space driving score in function of the quantity of blue pill (in micrograms). In blue the real values and in green the predicted values.

* The cost function $J(\theta)$ in function of the $\theta$ values (see example in figure 2),

<img src="day01/assets/ex04_J_vs_t1.png" />

Figure 2: Evolution of the cost function J in fuction of $\theta_1$ for different values of $\theta_0$.

* You will calculate the MSE of the hypothesis you chose (you know how to do it already) add it to your class **MyLinearRegression**. You can remark that **mse_** takes 2 arguments: X and Y, thus you will need to call **predict_** inside **mse_** to get the predicted outputs (and it will be also the case in the next exercises).


## Examples:

```python
import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error
from mylinearregression import MyLinearRegression as MyLR

data = pd.read_csv("are_blue_pills_magics.csv")
Xpill = np.array(data[Micrograms]).reshape(-1,1)
Yscore = np.array(data[Score]).reshape(-1,1)

linear_model1 = MyLR(np.array([[89.0], [-8]]))
linear_model2 = MyLR(np.array([[89.0], [-6]]))
Y_model1 = linear_model1.predict_(Xpill)
Y_model2 = linear_model2.predict_(Xpill)

print(linear_model1.mse_(Xpill, Yscore))
# 57.60304285714282

print(mean_squared_error(Yscore, Y_model1))
# 57.603042857142825

print(linear_model2.mse_(Xpill, Yscore))
# 232.16344285714285

print(mean_squared_error(Yscore, Y_model1))
# 232.16344285714285
```

### Clarifications and hints:

There is no method named __.mse\___ in the class LinearRegression of the module sklearn.linear_model but there is a method named __.score__. The __.score__ method correspond to the $R^2$ score. The metric MSE is available in the module sklearn.metrics.


## Questions:

Be sure to understand the underlying concepts and be able to answer those questions during the evaluation:
* What is a hypothesis and what is it goal? (It is a second chance to let you say something intelligible, no need to thank me)
* What is the cost function and what is it representing?
* What is the linear gradient descent and what is it doing?
* Can you explain the MSE and what is it spotting?
