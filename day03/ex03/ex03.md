# Exercise 3 - Code the DecisionTreeClassifier fit function

|                         |                    |
| -----------------------:| ------------------ |
|   Turnin directory :    |  ex03              |
|   Files to turn in :    |  decision_tree_classifier.py |
|   Forbidden module :    |  sklearn           |
|   Forbidden function :  |  Anything that makes the coffee for you. |
|   Remarks :             |  Read the doc      |


**Objective:**

The aim of this exercise is to code a simplified version of Sklearn's DecisionTreeClassifier.
This part focus on the fit function, ie. the training of the Decision tree given a dataset.


**Instruction:**
Fill in these classes. Please note that depending on a few implemetation choice the output can be very slightly different.
You should be able to explain why.   
Feel free to add other method and functions that you write (you can reuse the information_gain function that you already created)


```python
import pandas as pd


class Node:
    def __init__(self, data=None, labels=None,
                 is_leaf=False, split_feature=None, split_kind=None, split_criteria=None,
                 left=None, right=None,
                 depth=0):
        """
        :param pandas.Dataframe data: features
        :param pandas.Dataframe labels: labels

        :param bool is_leaf: True if the node is a leaf of the tree
        :param int split_feature: column of the feature
        :param str split_kind: ["<=" or "="]
        :param split_criteria: value of the criteria used to split data

        :param Node left: node child where criteria is True
        :param Node right: node child where criteria is False

        :param int depth: depth level of the node in the tree
        """
        # Your code here. You can add more things if needed


class DecisionTreeClassifier:
    def __init__(self, criterion='gini', max_depth=10):
        """

        :param str criterion: 'gini' or 'entropy'
        :param max_depth: max_depth of the tree (Decision tree creation stops splitting a node if node.depth >= max_depth)
        """
        self.root = None  # Root node of the tree
        # Your code here. You can add more things if needed

    def fit(self, X, y):
        """
        Build the decision tree from the training set (X, y). The training set has m data_points (examples).
        Each of them has n features.

        :param pandas.Dataframe X: Training input (m x n)
        :param pandas.Dataframe y: Labels (m x 1)
        :return object self: Trained tree
        """
        # Your code here. You can add more things if needed


if __name__ == '__main__':
    from sklearn.model_selection import train_test_split
    from sklearn.datasets import load_iris 
    # sklearn is not allowed in the classes.

    # Test on iris dataset
    iris = load_iris()
    X = pd.DataFrame(iris.data)
    y = pd.DataFrame(iris.target)
    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=1)

    dec_tree = DecisionTreeClassifier()
    dec_tree.fit(X_train, y_train)
    root = dec_tree.root
    print("TEST ON IRIS DATASET")
    print("Root split info = 'Feature_{}{}{}'\n".format(root.split_feature, root.split_kind, root.split_criteria))
    print("5 first lines of the labels of the left child of root =\n{}\n".format(root.left_child.y.head()))
    print("5 first lines of the labels of the right child of root =\n{}".format(root.right_child.y.head()))
```

**Output examples:**
```bash
TEST ON IRIS DATASET
Root split info = 'Feature_2 <= 1.9'

5 first lines of the labels of the left child of root =
    0
18  0
4   0
45  0
39  0
36  0

5 first lines of the labels of the right child of root =
     0
118  2
59   1
117  2
139  2
107  2

```

