# Exercise 05 - Regularized Logistic Loss Function

|                         |                         |
| -----------------------:| ----------------------- |
|   Turn-in directory :   |  ex05                   |
|   Files to turn in :    |  reg_log_loss.py        |
|   Forbidden functions : |  None                   |
|   Remarks :             |  n/a                    |
|   Helpful links :       |  [Machine Learning Mooc / Week 3 - Regularized Logistic Regression](https://www.coursera.org/learn/machine-learning/lecture/4BHEy/regularized-logistic-regression) |

## Objectives:

You must implement the following formula as a function:  

$$
J(\theta) = \frac{1}{m}\left((-y \cdot \log(h) - (1 - y) \cdot (1 - log(h))  + \lambda \theta \cdot \theta\right) 
$$

Which is the same as:

$$
J(\theta) = \frac{1}{m}\left((-y^{T}\log(h)) - (1 - y)^{T} (1 - log(h)) + \lambda \theta \cdot \theta\right)
$$

Where:
- $h = g(X\theta)$ is a vector of dimension (m, 1),
- $\theta$ is a vector of dimension (n+1, 1),
- $y$ is a vector of dimension (m, 1),
- $\lambda$ is a constant,


## Instructions:

In the vec_log_loss.py file create the following function as per the instructions below: 
```python
def reg_log_loss_(y_true, y_pred, m, lambda_, eps= 1e-15):
    """
    Compute the logistic loss value.
    Args:
        y_true: a scalar or a numpy ndarray for the correct labels
        y_pred: a scalar or a numpy ndarray for the predicted labels
        m: the length of y_true (should also be the length of y_pred)
        lambda_: a float for the regularization parameter
        eps: epsilon (default=1e-15)
    Returns:
        The logistic loss value as a float.
        None on any error.
    Raises:
        This function should not raise any Exception.
    """
```


## Examples:

```python
import numpy as np
from sigmoid import sigmoid_
from reg_log_loss import reg_log_loss_

# Test n.1
x_new = np.arange(1, 13).reshape((3, 4))
y_true = np.array([1, 0, 1])
theta = np.array([0, -1.5, 2.3, 1.4, 0.7])
y_pred = sigmoid_(np.dot(x_new, theta))
m = len(y_true)
print(reg_log_loss_(y_true, y_pred, m, theta, 0.0))     
# 7.233346147374828

# Test n.2
x_new = np.arange(1, 13).reshape((3, 4))
y_true = np.array([1, 0, 1])
theta = np.array([0, -1.5, 2.3, 1.4, 0.7])
y_pred = sigmoid_(np.dot(x_new, theta))
m = len(y_true)
print(reg_log_loss_(y_true, y_pred, m, theta, 0.5))     
# 8.898346147374827

# Test n.3
x_new = np.arange(1, 13).reshape((3, 4))
y_true = np.array([1, 0, 1])
theta = np.array([0, -1.5, 2.3, 1.4, 0.7])
y_pred = sigmoid_(np.dot(x_new, theta))
m = len(y_true)
print(reg_log_loss_(y_true, y_pred, m, theta, 1)) 

# 10.563346147374826

# Test n.4
x_new = np.arange(1, 13).reshape((3, 4))
y_true = np.array([1, 0, 1])
theta = np.array([0, -5.2, 2.3, -1.4, 8.9])
y_pred = sigmoid_(np.dot(x_new, theta))
m = len(y_true)
print(reg_log_loss_(y_true, y_pred, m, theta, 1))     
# 49.346258798303566

# Test n.5
x_new = np.arange(1, 13).reshape((3, 4))
y_true = np.array([1, 0, 1])
theta = np.array([0, -5.2, 2.3, -1.4, 8.9])
y_pred = sigmoid_(np.dot(x_new, theta))
m = len(y_true)
print(reg_log_loss_(y_true, y_pred, m, theta, 0.3))     
# 22.86292546497024

# Test n.6
x_new = np.arange(1, 13).reshape((3, 4))
y_true = np.array([1, 0, 1])
theta = np.array([0, -5.2, 2.3, -1.4, 8.9])
y_pred = sigmoid_(np.dot(x_new, theta))
m = len(y_true)
print(reg_log_loss_(y_true, y_pred, m, theta, 0.9))     
#  45.56292546497025
```
