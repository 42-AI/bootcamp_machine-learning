# Exercise 05 - Logistic Loss Function

|                         |                         |
| -----------------------:| ----------------------- |
|   Turn-in directory :   |  ex05                   |
|   Files to turn in :    |  log_loss.py            |
|   Forbidden libraries : |  Numpy                  |
|   Remarks :             |  n/a                    |

## Objectives:
You must implement the following formula as a function:  

$$
J( \theta) = -\frac{1} {m} \lbrack \sum_{i = 1}^{m} y^{(i)}\log(h_{ \theta }(x^{(i)})) + (1 - y^{(i)})\log(1 - h_{ \theta }(x^{(i)}))\rbrack
$$

Where:  
* $J( \theta)$ is the cost function with theta being the weights.
* $m$ is the length of $y$, i.e. the number of observations in our sample.
* $h_{\theta}(x)$ also called y_pred or y_hat, is the calculated hypothesis and it represents the predicted output (formula below).
* $y$, also called y_true, represents the desired output, either 1 or 0.


This function is called the Cross-Entropy loss or logistic loss.
We encourage you to get a look at [**this section**](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression) of the Cross entropy Wikipedia.

Be careful: 
- the x you will get as an input is a m * n matrix
- theta is an (n + 1) * 1 vector. 

## Instructions:
In the log_loss.py file create the following function as per the instructions below: 
```python
def log_loss_(x, y, theta, eps=1e-15):
    """
    Compute the logistic loss value.
    Args:
        x: has to be an numpy.ndarray, a matrice of dimension m * n.
        y: has to be an numpy.ndarray, a vector of dimension m * 1.
        theta: has to be an numpy.ndarray, a vector (n + 1) * 1.
        eps: epsilon (default=1e-15)
    Returns:
        The logistic loss value as a float.
        None on any error.
    Raises:
        This function should not raise any Exception.
    """
```

Hint: the purpose of epsilon (eps) is to avoid log(0) errors, it is a very small residual value we add to y_pred.

## Examples:
```python
from sigmoid import sigmoid_
from log_loss import log_loss_

# Example 1:
x1 = 4
y1 = 1
theta1 = 0.5
log_loss_(x1, y1, theta1)
# Output:
0.12692801104297152

# Example 2:
x2 = [1, 2, 3, 4]
y2 = 0
theta2 = [-1.5, 2.3, 1.4, 0.7]
log_loss_(x2, y2, theta2)
# Output:
10.100041078687479

# Example 3:
x3 = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]
y3_true = [1, 0, 1]
theta3 = [-1.5, 2.3, 1.4, 0.7]
log_loss_(x3, y3, theta3)
# Output:
7.233346147374828
```
