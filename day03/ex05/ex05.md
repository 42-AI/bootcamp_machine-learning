# Exercise 05 - Logistic Loss Function

|                         |                         |
| -----------------------:| ----------------------- |
|   Turn-in directory :   |  ex05                   |
|   Files to turn in :    |  log_loss.py            |
|   Forbidden libraries : |  Numpy                  |
|   Remarks :             |  n/a                    |

## Objectives:
You must implement the following formula as a function:  

$$
J( \theta) = -\frac{1} {m} \lbrack \sum_{i = 1}^{m} y^{(i)}\log(\hat{y}^{(i)})) + (1 - y^{(i)})\log(1 - \hat{y}^{(i)})\rbrack
$$

Where:
- $\hat{y}$ is a vector of dimension $m * 1$, the vector of predicted values
- $y$ is a vector of dimension $m * 1$, the vector of expected values
- $\hat{y}^{(i)}$ is the ith component of vector $\hat{y}$,
- $y^{(i)}$ is the ith component of vector $y$,

## Instructions:
In the log_loss.py file create the following function as per the instructions below: 
```python
def log_loss_(y, y_hat, eps=1e-15):
    """
    Compute the logistic loss value.
    Args:
        y: has to be an numpy.ndarray, a vector of dimension m * 1.
        y_hat: has to be an numpy.ndarray, a vector of dimension m * 1.
        eps: epsilon (default=1e-15)
    Returns:
        The logistic loss value as a float.
        None on any error.
    Raises:
        This function should not raise any Exception.
    """
```

**Hint:** the purpose of epsilon (eps) is to avoid log(0) errors, it is a very small residual value we add to y.

## Examples:
```python
# Example 1:
y1 = 1
x1 = 4
theta1 = 0.5
y_hat1 = logistic_predict(x1, theta1)
log_loss_(y1, y_hat1)
# Output:
0.12692801104297152

# Example 2:
y2 = 0
x2 = [1, 2, 3, 4]
theta2 = [-1.5, 2.3, 1.4, 0.7]
y_hat2 = logistic_predict(x2, theta2)
log_loss_(y2, y_hat2)
# Output:
10.100041078687479

# Example 3:
y3 = [1, 0, 1]
x3 = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]
theta3 = [-1.5, 2.3, 1.4, 0.7]
y_hat3 = logistic_predict(y3, theta3)
log_loss_(y3, y_hat3)
# Output:
7.233346147374828
```

## To go further:
This function is called the Cross-Entropy loss or logistic loss.
We encourage you to get a look at [**this section**](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression) of the Cross entropy Wikipedia.