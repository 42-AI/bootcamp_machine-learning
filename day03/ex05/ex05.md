# Exercise 05 - Logistic Loss Function

|                         |                         |
| -----------------------:| ----------------------- |
|   Turn-in directory :   |  ex05                   |
|   Files to turn in :    |  log_loss.py            |
|   Forbidden libraries : |  Numpy                  |
|   Remarks :             |  n/a                    |

## Objectives:
You must implement the following formula as a function:  

$$
J( \theta) = -\cfrac{1} {m} \lbrack \sum_{i = 1}^{m} y^{(i)}\log(\hat{y}^{(i)})) + (1 - y^{(i)})\log(1 - \hat{y}^{(i)})\rbrack
$$

Where:
- $\hat{y}$ is a vector of dimension $m * 1$, the vector of predicted values
- $\hat{y}^{(i)}$ is the $i^{th}$ component of the $\hat{y}$ vector ,
- $y$ is a vector of dimension $m * 1$, the vector of expected values
- $y^{(i)}$ is the $i^{th}$ component of the $y$ vector,

## Instructions:
In the `log_loss.py` file, write the following function as per the instructions below: 
```python
def log_loss_(y, y_hat, eps=1e-15):
    """
    Computes the logistic loss value.
    Args:
        y: has to be an numpy.ndarray, a vector of dimension m * 1.
        y_hat: has to be an numpy.ndarray, a vector of dimension m * 1.
        eps: has to be a float, epsilon (default=1e-15)
    Returns:
        The logistic loss value as a float.
        None on any error.
    Raises:
        This function should not raise any Exception.
    """
```

### **Hint:** 
The logarithmic function isn't defined in 0.  
This means that if $y^{(i)} = 0$ you will get an error when you try to compute $log(y^{(i)})$.   
The purpose of the `eps` argument is to avoid log(0) errors. It is a very small residual value we add to `y`.

## Examples:
```python
# Example 1:
y1 = np.array([1])
x1 = np.array([4])
theta1 = np.array([[2], [0.5]])
y_hat1 = logistic_predict(x1, theta1)
log_loss_(y1, y_hat1)
# Output:
0.01814992791780973

# Example 2:
y2 = np.array([[1], [0], [1], [0], [1]])
x2 = np.array([[4], [7.16], [3.2], [9.37], [0.56]])
theta2 = np.array([[2], [0.5]])
y_hat2 = logistic_predict(x2, theta2)
log_loss_(y2, y_hat2)
# Output:
2.4825011602474483

# Example 3:
y3 = np.array([[0], [1], [1]])
x3 = np.array([[0, 2, 3, 4], [2, 4, 5, 5], [1, 3, 2, 7]])
theta3 = np.array([[-2.4], [-1.5], [0.3], [-1.4], [0.7]])
y_hat3 = logistic_predict(x3, theta3)
log_loss_(y3, y_hat3)
# Output:
2.9938533108607053
```

## To go further:
This function is called **Cross-Entropy loss**, or **logistic loss**.  
For more information you can look at [**this section**](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression) of the Cross entropy Wikipedia.
