 # Exercise 10 - Practicing Logistic Regression

|                         |                         |
| -----------------------:| ----------------------- |
|   Turn-in directory :   |  ex10                   |
|   Files to turn in :    |  log_reg_model.py       |
|   Authorized modules :  |  Numpy              |
|   Forbidden modules :   |  sklearn            |

## Objectives:

Now it is time to test your logistic regression on real data! 
You will use the **solar_system_census_dataset**. 

## Instructions:

**Some words about the datasets you will use:**
* The dataset, is divided into two files which can be found in the ressources folder: 'solar_system_census.csv' and 'solar_system_census_planets.csv'. It is a part of the solar system census dataset. 
* The first file contains biometric informations such as the height, weight, and bone density of solar system citizens.
* The second file contains the homelands of each citizens using the space zipcode representation (i.e. one number for each planet... :)). 
As you should know solar citizens come from four registered area: The flying cities of Venus, United Nations of Earth, Mars Republic, and the Asteroids' Belt colonies.

# Part 1 - One label to discriminate them all

0) Split your dataset into a training and a test set.

1) Select your favorite space zipcode and generate a new numpy.ndarray corresponding for each citizen to come or not from the selected zipcode area.   
Congratulation you just created new label!

2) Train your logistic regression to predict if a citizen come from the selected area or not, using your brand new label.  
   **You can use normalization on your dataset. The question is should you?**

Your are able now to discriminate if each citizen come from a specific area or not given its biometric informations. It is a first step, a good one, but it is not enough.  

We want to classify between the four space zipcode area and not only between one and the others!  

So how can we do to be able to do multiclass logistic regression?  

# Part 2 - One versus all

The idea now is to apply what is called a **one-versus-all classification**.  
It is quite straightforward: 
1) Train a logistic regression classifier for each label against all the others (the way you did in part one).
   
2) For each examples, use all the classifiers to predict the class and take the label which got the highest classification probability. 

## Example:
If an example got the following classification probabilities: 
- label 1 vs all: 0.38
- label 2 vs all: 0.51
- label 3 vs all: 0.12
- label 4 vs all: 0.89  

Then the example should be classified as 'label 4'. 