# Exercise 07 - Logistic Gradient

|                         |                         |
| -----------------------:| ----------------------- |
|   Turn-in directory :   |  ex07                   |
|   Files to turn in :    |  log_gradient.py            |
|   Forbidden libraries : |  numpy                  |
|   Remarks :             |  n/a                    |

## Objectives:
You must implement the following formula as a function:  

$$
\begin{matrix}
\nabla(J)_0 &  = &\cfrac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)}) & \\
\nabla(J)_j & = &\cfrac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})x_{j}^{(i)} & \text{ for j = 1, ..., n}    
\end{matrix}
$$

Where:
- $\nabla(J)$ is a vector of size $(n + 1) * 1$, the gradient vector
- $\nabla(J)_j$ is the $j^{th}$ component of $\nabla(J)$, the partial derivative of $J$ with respect to $\theta_j$
- $y$ is a vector of dimension $m * 1$, the vector of expected values
- $y^{(i)}$ is a scalar, the $i^{th}$ component of vector $y$
- $x^{(i)}$ is the feature vector of the $i^{th}$ example
- $x^{(i)}_j$ is a scalar, the $j^{th}$ feature value of the $i^{th}$ example
- $h_{\theta}(x^{(i)})$ is a scalar, the model's estimation of $y^{(i)}$

Remember that with logistic regression, the hypothesis is slightly different:  

$$
h_{\theta}(x^{(i)}) = sigmoid( \theta \cdot x'^{(i)})
$$


## Instructions:
In the `log_gradient.py` file, write the following function as per the instructions below: 
```python
def log_gradient(x, y, theta):
    """Computes a gradient vector from three non-empty numpy.ndarray, with a for-loop. The three arrays must have compatible dimensions.
    Args:
      x: has to be an numpy.ndarray, a matrix of dimension m * n.
      y: has to be an numpy.ndarray, a vector of dimension m * 1.
      theta: has to be an numpy.ndarray, a vector (n +1) * 1.
    Returns:
      The gradient as a numpy.ndarray, a vector of dimensions n * 1, containing the result of the formula for all j.
      None if x, y, or theta are empty numpy.ndarray.
      None if x, y and theta do not have compatible dimensions.
    Raises:
      This function should not raise any Exception.
    """
```
  
## Examples:
```python
# Example 1:
y1 = np.array([1])
x1 = np.array([4])
theta1 = np.array([[2], [0.5]])

log_gradient(x1, y1, theta1)
# Output:
array([[-0.01798621],
       [-0.07194484]])

# Example 2: 
y2 = np.array([[1], [0], [1], [0], [1]])
x2 = np.array([[4], [7.16], [3.2], [9.37], [0.56]])
theta2 = np.array([[2], [0.5]])

log_gradient(x2, y2, theta2)
# Output:
array([[0.3715235 ],
       [3.25647547]])

# Example 3: 
y3 = np.array([[0], [1], [1]])
x3 = np.array([[0, 2, 3, 4], [2, 4, 5, 5], [1, 3, 2, 7]])
theta3 = np.array([[-2.4], [-1.5], [0.3], [-1.4], [0.7]])

log_gradient(x3, y3, theta3)
# Output:
array([[-0.55711039],
       [-0.90334809],
       [-2.01756886],
       [-2.10071291],
       [-3.27257351]])
```
