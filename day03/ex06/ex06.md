# Exercise 06 - Vectorized Logistic Loss Function

|                         |                         |
| -----------------------:| ----------------------- |
|   Turn-in directory :   |  ex06                   |
|   Files to turn in :    |  vec_log_loss.py        |
|   Forbidden functions : |  None                   |
|   Remarks :             |  n/a                    |

## Objectives:
You must implement the following formula as a function:  

$$
J( \theta) = -\frac{1} {m} \lbrack y \cdot \log(\hat{y}) + (1 - y) \cdot \log(1 - \hat{y})\rbrack
$$

Where:
- $\hat{y}$ is a vector of dimension $m * 1$, the vector of predicted values
- $y$ is a vector of dimension $m * 1$, the vector of expected values


## Instructions:
In the log_loss.py file create the following function as per the instructions below: 
```python
def vec_log_loss_(y, y_hat, eps=1e-15):
    """
    Compute the logistic loss value.
    Args:
        y: has to be an numpy.ndarray, a vector of dimension m * 1.
        y_hat: has to be an numpy.ndarray, a vector of dimension m * 1.
        eps: epsilon (default=1e-15)
    Returns:
        The logistic loss value as a float.
        None on any error.
    Raises:
        This function should not raise any Exception.
    """
```

**Hint:** the purpose of epsilon (eps) is to avoid log(0) errors, it is a very small residual value we add to y.

## Examples:
```python
# Example 1:
y1 = 1
x1 = 4
theta1 = 0.5
y_hat1 = logistic_predict(x1, theta1)
vec_log_loss_(y1, y_hat1)
# Output:
0.12692801104297152

# Example 2:
y2 = 0
x2 = [1, 2, 3, 4]
theta2 = [-1.5, 2.3, 1.4, 0.7]
y_hat2 = logistic_predict(x2, theta2)
vec_log_loss_(y2, y_hat2)
# Output:
10.100041078687479

# Example 3:
y3 = [1, 0, 1]
x3 = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]
theta3 = [-1.5, 2.3, 1.4, 0.7]
y_hat3 = logistic_predict(y3, theta3)
vec_log_loss_(y3, y_hat3)
# Output:
7.233346147374828
```