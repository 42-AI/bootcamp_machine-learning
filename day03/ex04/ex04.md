# Exercise 04 - Logistic Hypothesis

|                         |                         |
| -----------------------:| ----------------------- |
|   Turn-in directory :   |  ex04                   |
|   Files to turn in :    |  log_pred.py            |
|   Forbidden libraries : |  Numpy                  |
|   Remarks :             |  n/a                    |

## Objectives:
You must implement the following formula as a function:  
$$
\begin{matrix}
\hat{y}^{(i)} = h_{ \theta }(x^{(i)}) = \frac{1} {1 + e^{-\theta \cdot x^{(i)}}} & &\text{ for i = 1, \dots, m}    
\end{matrix}
$$

Where:
- $x$ is a matrix of dimension m * n, the matrix of examples
- $\hat{y}$ is a vector of dimension m * 1, the vector of predicted values
- $\hat{y}^{(i)}$ is the *ith* component of vector $y$
- $x^{(i)}$ is the *ith* component of matrix $x$ 

As you may have noticed, this is our sigmoid function with $\theta \cdot x^{(i)}$ as argument.

Be careful: 
- the x you will get as an input is a m * n matrix
- theta is an (n + 1) * 1 vector. 

## Instructions:
In the log_pred.py file create the following function as per the instructions below: 
```python
def logistic_predict_(x, theta):
    """
    Compute the logistic hypthesis.
    Args:
        x: 
        y_true: a scalar or a list for the correct labels
        y_pred: a scalar or a list for the predicted labels
        m: the length of y_true (should also be the length of y_pred)
        eps: epsilon (default=1e-15)
    Returns:
        The logistic loss value as a float.
        None on any error.
    Raises:
        This function should not raise any Exception.
    """
```

Hint: the purpose of epsilon (eps) is to avoid log(0) errors, it is a very small residual value we add to y_pred.

**Examples:**
```python
from sigmoid import sigmoid_
from log_loss import log_loss_


# Test n.1
x = 4
y_true = 1
theta = 0.5
y_pred = sigmoid_(x * theta)
m = 1   # length of y_true is 1
print(log_loss_(y_true, y_pred, m))
# 0.12692801104297152

# Test n.2
x = [1, 2, 3, 4]
y_true = 0
theta = [-1.5, 2.3, 1.4, 0.7]
x_dot_theta = sum([a*b for a, b in zip(x, theta)])
y_pred = sigmoid_(x_dot_theta)
m = 1
print(log_loss_(y_true, y_pred, m))
# 10.100041078687479

# Test n.3
x_new = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]
y_true = [1, 0, 1]
theta = [-1.5, 2.3, 1.4, 0.7]
x_dot_theta = []
for i in range(len(x_new)):
    my_sum = 0
    for j in range(len(x_new[i])):
        my_sum += x_new[i][j] * theta[j]
    x_dot_theta.append(my_sum)
y_pred = sigmoid_(x_dot_theta)
m = len(y_true)
print(log_loss_(y_true, y_pred, m))
# 7.233346147374828
```
