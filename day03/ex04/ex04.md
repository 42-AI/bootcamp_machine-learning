# Exercise 04 - Logistic Hypothesis

|                         |                         |
| -----------------------:| ----------------------- |
|   Turn-in directory :   |  ex04                   |
|   Files to turn in :    |  log_pred.py            |
|   Forbidden libraries : |  None                |
|   Remarks :             |  n/a                    |

## Objectives:
You must implement the following formula as a function:  
$$
\begin{matrix}
\hat{y}^{(i)} = sigmoid(h_{ \theta }(x^{(i)})) = \frac{1} {1 + e^{-\theta \cdot x^{(i)}}} & &\text{ for i = 1, \dots, m}    
\end{matrix}
$$

Where:
- $x^{(i)}$ is the feature vector of the $i^{th}$ example, with an extra $1$ in the first position (dimension $n + 1$)
- $\hat{y}^{(i)}$ is the prediction for the $i^{th}$ example
- $h_{\theta}(x^{(i)})$ is the linear hypothesis applied to the $i^{th}$ feature vector. It corresponds to $\theta \cdot x^{(i)}$
- $\hat{y}$ is a vector of dimension m * 1, the vector of predicted values

To summarize, **this is the sigmoid function with $\theta \cdot x^{(i)}$ as an argument.**

**A quick note: Do you need to transpose $\theta$ or not?**  
If we apply pure matrix multiplication to two vectors, we technically need to transpose the first one to obtain a scalar (a real number).  
However, NumPy might let you get away without it because it will apply **dot product** (also known as scalar product). Most NumPy vectors aren't set as either row or column vectors, which can lead to unexpected results. Watch [this video](https://www.youtube.com/watch?v=V2QlTmh6P2Y) if you want to know more.


#### Be careful: 
- the x argument you will get as an input to your function is an m * n matrix
- theta is an $(n + 1) * 1$ vector. 

## Instructions:
In the log_pred.py file, create the following function as per the instructions below: 
```python
def logistic_predict_(x, theta):
    """
    Compute the logistic hypthesis.
    Args:
        x: has to be an numpy.ndarray, a matrix of dimension m * n.
        theta: has to be an numpy.ndarray, a vector (n +1) * 1.
    Returns:
        The prediction between 0 and 1 as a float.
        None on any error.
    Raises:
        This function should not raise any Exception.
    """
```

## Examples: MUST BE CHANGED
```python
# Example 1
x = 4
theta = 0.5
# Output: 
0.8807970779778823

# Example 1
x = 4
theta = 0.5
# Output: 


# Example 1
x = 4
theta = 0.5
# Output: 

y_pred = sigmoid_(x * theta)
m = 1   # length of y_true is 1
print(log_loss_(y_true, y_pred, m))
# 0.12692801104297152

# Test n.2
x = [1, 2, 3, 4]
y_true = 0
theta = [-1.5, 2.3, 1.4, 0.7]
x_dot_theta = sum([a*b for a, b in zip(x, theta)])
y_pred = sigmoid_(x_dot_theta)
m = 1
print(log_loss_(y_true, y_pred, m))
# 10.100041078687479

# Test n.3
x_new = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]
y_true = [1, 0, 1]
theta = [-1.5, 2.3, 1.4, 0.7]
x_dot_theta = []
for i in range(len(x_new)):
    my_sum = 0
    for j in range(len(x_new[i])):
        my_sum += x_new[i][j] * theta[j]
    x_dot_theta.append(my_sum)
y_pred = sigmoid_(x_dot_theta)
m = len(y_true)
print(log_loss_(y_true, y_pred, m))
# 7.233346147374828
```
