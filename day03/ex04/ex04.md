# Exercise 04 - Logistic Hypothesis

|                         |                         |
| -----------------------:| ----------------------- |
|   Turn-in directory :   |  ex04                   |
|   Files to turn in :    |  log_pred.py            |
|   Forbidden libraries : |  None                |
|   Remarks :             |  n/a                    |

## Objectives:
You must implement the following formula as a function:  

$$
\begin{matrix}
\hat{y} & = & \text{sigmoid}(X' \cdot \theta) & =  &\frac{1} {1 + e^{-X' \cdot \theta}}    
\end{matrix}
$$

Where:
- $X$ is a matrix of dimension $m * n$, the design matrix
- $X'$ is a matrix of dimension $m * (n + 1)$, the design matrix onto which a column of ones is added as a first column
- $\hat{y}$ is a vector of dimension $m * 1$, the vector of predicted values
- $\theta$ is a vector of dimension $(n + 1) * 1$, the vector of parameters

Be careful: 
- the *x*  you will get as an input corresponds to $X$, the $m * n$ matrix. Not $X'$. 
- $\theta$ is an $(n + 1) * 1$ vector. 

You have to transform *x* to fit theta's dimension!

## Instructions:
In the log_pred.py file, create the following function as per the instructions below: 
```python
def logistic_predict_(x, theta):
    """
    Compute the logistic hypthesis.
    Args:
        x: has to be an numpy.ndarray, a matrix of dimension m * n.
        theta: has to be an numpy.ndarray, a vector (n + 1) * 1.
    Returns:
        The prediction between 0 and 1 as a float.
        None on any error.
    Raises:
        This function should not raise any Exception.
    """
```

## Examples: MUST BE CHANGED
```python
# Example 1
x = 4
theta = 0.5
# Output: 
0.8807970779778823

# Example 1
x = 4
theta = 0.5
# Output: 


# Example 1
x = 4
theta = 0.5
# Output: 

y_pred = sigmoid_(x * theta)
m = 1   # length of y_true is 1
print(log_loss_(y_true, y_pred, m))
# 0.12692801104297152

# Test n.2
x = [1, 2, 3, 4]
y_true = 0
theta = [-1.5, 2.3, 1.4, 0.7]
x_dot_theta = sum([a*b for a, b in zip(x, theta)])
y_pred = sigmoid_(x_dot_theta)
m = 1
print(log_loss_(y_true, y_pred, m))
# 10.100041078687479

# Test n.3
x_new = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]
y_true = [1, 0, 1]
theta = [-1.5, 2.3, 1.4, 0.7]
x_dot_theta = []
for i in range(len(x_new)):
    my_sum = 0
    for j in range(len(x_new[i])):
        my_sum += x_new[i][j] * theta[j]
    x_dot_theta.append(my_sum)
y_pred = sigmoid_(x_dot_theta)
m = len(y_true)
print(log_loss_(y_true, y_pred, m))
# 7.233346147374828
```
