# Exercise 08 - Ridge Regression Method

|                         |                    |
| -----------------------:| ------------------ |
|   Turn-in directory :   |  ex08              |
|   Files to turn in :    |  ridge_reg.py      |
|   Forbidden functions : |  \*.sum()           |
|   Forbidden classes :   | sklearn.linear_model.Ridge |
|   Authorized classes :  | sklearn.model_selection.LeaveOneOut |
|                         | sklearn.model_selection.KFold |
|   Remarks :             |  n/a               |

**Objectives:**
* Understand the limit of a hypothesis.
* Understand of the notions of biais and variance.
* Implementation of a class named **MyRidge** similar to the class of the same name in **sklearn.linear_model**.


## Part One - __Back to the $\stout{Future}$ Past__

__If I had an hour to solve a problem I would spend 55 minutes thinking about the problem and 5 minutes thinking about the solutions. (Albert Einstein)__ 
Thus, be $\stout{lazy}$ smart ! This should remind you something you already coded.

**Instructions:**
For this part, your class **MyRidge**  will have several methods:
* __.\_\_init\_\_ __, special method, identical to the one of the class **MyLinearRegression** (Day01),
* __.get\_params\_ __, which get the parameters of the estimator, 
* __.set\_params\_ __, which set the parameters of the estimator,
* __.predict\_ __, which predict output using a linear model,
* __.fit\_ __, which fit Ridge regression model
* a few metric methods: __.mse\_ __,  __.rmse\_ __, __.rscore\_ __.

Except for __.fit\___ the methods are identicals to the ones of your class **MyLinearRegression**.
__You should consider inheritance__

The difference between the method __ .fit\_ __ of **MyRidge** class and the method __ .fit\_ __ of **MyLinearRegression** is the use of a regularization term.
The cost function:

$$
J(\mathbf{\theta}) & = & \frac{1}{2M}\sum_{i=1}^{M}(h_{\theta}(x^{(i)})-y^{(i)})^2+\frac{\lambda}{2M}\sum_{j=1}^{N}\theta_j^2
$$

where:
* M is the number of training examples (or the length of the dataset),
* N is the number of features,
* $x^{(i)}$ is the input (all features values) of the ith training example,
* $y^{(i)}$ is is the output of the ith training example,
* $\lambda is regularization parameter (or Tikhonov parameter).
Beware of in the second summation term, the index j start at 1 and not 0 ($\theta_0$ is not penalized).

Code your class **MyRidge** as per the instructions below:
```python
class MyRidge($\mathit{Parent\ Class}$):
	fit_(self, lambda=1.0, max_iter=1000, tol=0.001):
		"""
		Fit the linear model by performing Ridge regression (Tikhonov regularization).
		Args:
		  lambda: has to be a float.
                  max_iter: has to be integer.
		  tol: has to be float.
		Returns:
		  Nothing.
		Raises:
		  This method should not raise any Exception.
		"""
```
Regularization parameter must be a positive float.


Try your method with the dataset named **dataset.csv**.
The dataset is made of 3 columns: 2 features and the output (x1, x2 and y).
You have representations of the dataset in the following figure:
<img src=../assets/figure1_3Dplot_dataset.png>

The best hypothesis for your data has the following form:
$$
h(x_1,x_2) & = & \theta_0 + \theta_1.x_1 + \theta_2.x_1^3 + \theta_3.x_2 + \theta_4.x_2^2
$$


### Good practice:
In **day01** you probably fit your data with the entire dataset.
This is not a good practice !
You should at list divide the dataset in 2 sets : a training set and a test set.
A better practice is to divide your dataset in 3 sets: training set, cross-validation set and a test set. 
Several techniques exist to do a good training/cross-validation process: the __LeaveOneOut__, __KFold__ and so on(which you can find in sklearn: **sklearn.model_selection.LeaveOneOut** or **sklearn.model_selection.KFold**).
See documentation on internet about it to get more information.

For the exercise, we will simplify the process:
* You will split your data set into 3 sets: training set, cross-validation set and testing set.
* The training set will be represent 60 % of the dataset and the cross-validation set will correspond to 20 % of the dataset.
* Perform few training with different values of $\lambda$ (choose the values but stay coherent !).
* Choose the best set of $\theta$ coefficients based on the error results (RMSE and R2 score for example) of your cross-validation.
* And verify if your model is a good generalization by calculating the error results  on the testing set.


Plot a graph with 3 curves:
* The output with respect to one feature,
* The model you get when you fit the data with the method **.fit()** of your class **MyLinearRegression**,
* The model you get when you fit the data with the method **.fit\_()** of your class **MyRidge**,

Answer to the following question (with quantitative arguments):
* Which model is better ?


## Questions:
* What is the biais and the variance ?
* A variation of the regularization parameter leads to an evolution of the biais and the variance of a model. How the variance and the biais evolve with the regularization parameter ?