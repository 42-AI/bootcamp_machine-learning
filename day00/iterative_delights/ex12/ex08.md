# Exercise 08 - Gradient

|                         |                    |
| -----------------------:| ------------------ |
|   Turnin directory :    |  ex08              |
|   Files to turn in :    |  gradient.py       |
|   Forbidden function :  |  \*.sum()           |
|   Remarks :             |  n/a               |

**Objective:**
You must implement the following formula as a function:  

$$
\nabla(J)_j = \frac{\alpha}{m}\sum_{i=1}^{m}(h_{\theta}(x_i) - y_i)x_{i}^{(j)}
$$

where  
- $\nabla(J)$ is a vector of size n 
- $x$ is a matrix of size m * n (i.e. a matrix containing m vectors of size n)
- $y$ is a vector of size m
- $\theta$ is a vector of size n 
- $x_i$ is the ith vector of x
- $y_i$ is the ith elements of y
- $\nabla(J)_j$ is the jth element of $\nabla(J)$
- $h_{\theta}(x_i)$ is the result of the dot product of the vector $\theta$ and the vector $x_i$
- $\alpha$ is a constant

**Instructions:**
In the gradient.py file create the following function as per the instructions given below:
```python
def gradient(x, y, theta):
    """Computes the mean squared error of two non-empty numpy.ndarray, using a for-loop. The two arrays must have the same dimensions.
    Args:
      y: has to be an numpy.ndarray, a vector.
      y_hat: has to be an numpy.ndarray, a vector.
    Returns:
      The mean squared error of the two vectors as a float.
      None if y or y_hat are non-empty numpy.ndarray.
      None if y and y_hat does not share the same dimensions.
    Raises:
      This function shouldn't raise any Exception.
    """
```

**Examples** 
Create a function called `gradient` which takes four arguments : 
  - an array which correspond to the vector x in the previous formula,
  - an array which correspond to the vector y in the previous formula,
  - an array which correspond to the vector \theta which is used to compute $$ h_{\theta}(x_i) $$ // ![image info](../assets/hth.png),
  - a double corresponding to ![image info](../assets/alpha.png)
  
Your function must use a for loop and returns a vector containg the result of the formula for all j.

```python
>>> X = [0, 15, -9, 7, 12, 3, -21]
>>> Y = [2, 14, -13, 5, 12, 4, -19]
>>> h = lambda x : x * 0.8
>>> gradient(X, Y, h, 1)
-11.271428571428569
>>> gradient(X, Y, h, 0.1)
-1.127142857142857
>>> gradient(X, X, lambda x: x, 0.5)
0.0
```

