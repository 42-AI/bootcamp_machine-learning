# Exercise 11 - Regularized Gradient

|                         |                    |
| -----------------------:| ------------------ |
|   Turnin directory :    |  ex11              |
|   Files to turn in :    |  reg_grad.py       |
|   Forbidden function :  |  *.sum()           |
|   Remarks :             |  n/a               |

You must implement the following formula as a function:  
  
$$\nabla(J)_j = \frac{\alpha}{m}\sum_{i=1}^{m}(h_{\theta}(x_i) - y_i)x_{i}^{(j)} + \lambda $$

where  
- \nabla(J) is a vector of size n   
- x is a matrix of size m * n (i.e. a matrix containing m vectors of size n)  
- y is a vector of size m  
- \theta is a vector of size n   
- i is the training example's index
- j is the feature's index 
- x_i is the ith vector of x
- y_i is the ith elements of y
- \nabla(J)_j is the jth element of \nabla(J)
- ![image info](../assets/hth.png) // $$ h_{\theta}(x_i) $$ is the result of the dot product of the vector \theta and the vector x_i
- alpha is a constant

Create a function called `gradient` which takes four arguments : 
  - an array which correspond to the vector x in the previous formula,
  - an array which correspond to the vector y in the previous formula,
  - an array which correspond to the vector \theta which is used to compute $$ h_{\theta}(x_i) $$ // ![image info](../assets/hth.png),
  - a double corresponding to ![image info](../assets/alpha.png)
  
Your function must use a for loop and returns a vector containg the result of the formula for all j.

```python
>>> X = [0, 15, -9, 7, 12, 3, -21]
>>> Y = [2, 14, -13, 5, 12, 4, -19]
>>> h = lambda x : x * 0.8
>>> gradient(X, Y, h, 1)
-11.271428571428569
>>> gradient(X, Y, h, 0.1)
-1.127142857142857
>>> gradient(X, X, lambda x: x, 0.5)
0.0
```

