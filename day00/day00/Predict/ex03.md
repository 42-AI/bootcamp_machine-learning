### AI Classics

# EX00 The Vector

# EX01 The Matrix

# EX02 TinyStatistician


### Predict, Evaluate, Improve

```
“A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.” - Tom Mitchell, Machine Learning, 1997
```

To be said to learn you have to improve.  
To improve you have to evaluate your performance.  
To evaluate your performance you have to start to perform the task you want to be good at.  
  


One of the most common task in Machine Learning is **prediction**.  
This will be the task of your algorithms.  
This will be your task.  

[IMG CYCLE]



### Predict 

[IMG PRED]


## A very simple model
[Intro : simple data set + plot
We have some data.   We want to model it. First we need to make some assumptions, some hypothesis about the relationship tying together]

Let's start with a very simple **assumption**: the relation between our variables can be represented using a linear equation.  

This means that we are trying to represent $\hat{y} = ax + b$.  

We are putting this symbol '^' over the $y$ to represent the fact that $\hat{y}$ is a **prediction** of the true value of $y$ given the **parameters** $a$ and $b$ and the input value $x$.  

For example, if $a = 5$ and $b = 33$, then $\hat{y} = 5x + 33$.  

We are going to do a slightly change in our notation now.  
Instead of $\hat{y} = ax + b$ we will use the following notation:  
$$\hat{y} = \theta_0 + \theta_1 x$$.  

You probably have two questions now:  
- WTF is that weird $\theta$ symbol?
This strange symbol, $theta$, is called "theta".  

- Why change for this notation?  
We are using the theta notation because, despite the fact it will a first seems more complicated, it is actually done to simplify the notations later.  
Why?  
Imagine we have to build a more complex model using a lot of parameters.  
Something like $\hat{y} = ax1 + bx2 + ... + z$. How do you handle more than 23 paremeters?  
On the other hand, if you describe all your parameters using the theta notation, you can use as much parameters as you need.  

So now, we have $\hat{y} = \theta_0 + \theta_1 x$.  
If $\theta_0 = 33$ and $\theta_1 = 5$, then $\hat{y} = 33+ 5x$.    

This simple equation is now our **model**. This model draws a **relation between $y$ and $x$**.  
Using directly $y$ and not $\hat{y}$ in the previous sentence was done on purpose: **the relation we are looking for is not between our prediction and our inputs values, but between what we want to predict and our inputs values**.  

Then if $x = 7$ we can calculate that $\hat{y} = 33 + 5 \times 7 = 68$.
We can now say that according to our linear model, the **predicted value** of $y$ given $x  = 7$ is 68.     

To go a little further, lets consider a dataset containing $m$ data points called **examples**.  

What we have now are not single values for $x$ and $hat{y}$ but vectors of dimensions m * 1. The relation between our vectors can then be represented by the following formula:  
$$\hat{y}_i = \theta_0 + \theta_1 x_i \text{ for i = 1, ..., m}$$
  
where:
- $y_i$ is the *ith* component of vector $y$
- $x_i$ is the *ith* component of vector $x$   

Which can be experessed as:  
$$\theta = \begin{bmatrix}\theta_0 + \theta_1 \times x_1\ \vdots\  \theta_0 + \theta_1 \times x_m\ \end{bmatrix}$$  

For example,
$$\text{given } \theta = \begin{bmatrix}33 \ 5 \end{bmatrix} \text{ and } x = \begin{bmatrix}1 \ 3 \end{bmatrix} \text{: }$$    
$$\hat{y} = \begin{bmatrix} 33 +  1 \times 5 \ 33 + 3 \times 5\theta_1 \end{bmatrix}  = \begin{bmatrix} 38 \ 48 \end{bmatrix} $$    



# EX04 : PREDICT - ITERATIVE 
    def predict(x, theta)


--------------


## A simple trick

Let's do a little linear algebra trick to optimise the way we calculate predictions.  
If we add a column full of ones to our vector of examples $x$ we can create the following matrix: 

$$X = \begin{bmatrix} 1 & x_1 \ \vdots & \vdots \ 1 & x_m\end{bmatrix}$$
  
We can now get exactly the same result as in the previous exercice using only a product between our brand new matrix $X$ and the vector $\theta$! 

$$h(X) = X \cdot \theta = \begin{bmatrix} 1 & x_1 \ \vdots & \vdots \ 1 & x_m\end{bmatrix}\cdot\begin{bmatrix}\theta_0 \ \theta_1 \end{bmatrix} = \begin{bmatrix} \theta_0 + \theta_1x_1 \ \vdots \ \theta_0 + \theta_1x_m \end{bmatrix} $$

# EX05 Add intercept

# EX06 Prediction vectorized 


## Prediction 

It is prediction time!

By the way, the kind of tasks we are performing here is called a **regression** because we are predicting continuous numerical values.



# EX07: plot predicted value against several theta value


### Evaluate

[IMG EVAL]

## Introducing the cost function

How good is our model ?
It is hard to say by just looking at the plot. We can obviously observe that some lines seem to fit our data better than  others but it is not enough. 

[Ex mean (which is bad VS an opposite line which is the worst possible)]

To evaluate our model, we are going to use a **metric**, called a **cost function** (sometimes called **loss function**). The cost function tells us how bad is our model, how much it *costs* us to use it, how much we *loose* when we use it.  
If the model is good, we will not lost that much, if it's terrible it will cost us a lot!    

The metric you choose will impact deeply the results of your model.   

A very usual way to evaluate the performances of a regression model is to measure the distance between each predicted value and the true value it tries to predict. The smaller, the better.  

[img]

# EX08 Cost Function
MSE - it + MSE - vec 

PLOT


# EX09 Cost Function using X and Theta

MSE as cost - it + MSE as cost - vec 

# EX10 Questions: 

Explain why we are adding a column of ones to the examples 

Explain why the cost function squares the distance 


# Bonus I: Ex11 Other cost functions 

# Bonus II: Ex12 K-nearest neigbhors (\w Numpy) 









Ex03 : 
pred1(x, theta) with a for loop

Ex04 