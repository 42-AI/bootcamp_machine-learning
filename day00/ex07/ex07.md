# Exercise 07 - Cost Function

|                         |                    |
| -----------------------:| ------------------ |
|   Turn-in directory :   |  ex07              |
|   Files to turn in :    |  cost.py           |
|   Forbidden functions : |  None              |
|   Remarks :             |  n/a               |

## Objective:
You must implement the following formula as a function (and another one very close to it):

$$
J(\hat{y}, y) = \frac{1}{2m}\sum_{i=1}^{m}(\hat{y}_i - y_i)^2
$$

Where:
- $y$ is a vector of dimension m * 1,
- $\hat{y}$ is a vector of dimension m * 1,
- $\hat{y}_i$ is the ith component of vector $\hat{y}$,
- $y_i$ is the ith component of vector $y$,


By the way, this cost function is very close to the one called **"Mean Squared Error"**. The difference is in the denominator as you can see in the formula of the $MSE = \frac{1}{m}\sum_{i=1}^{m}(\hat{y}_i - y_i)^2$.  

Except the division by $2m$ instead of $m$, these functions are rigourously identical: $J(\hat{y}, y) = \frac{MSE}{2}$.  

MSE is called like that because it represents the mean of the errors (i.e.: the differences between the predicted value and the true value) squared.

You might wonder why we choose to divide by two instead of simply using the MSE?  
*(It is a good question, by the way.)*
- First, it does not change the overall evaluation of the model: if all the performance evaluations are divided by two, we can still measure the performance of our various models.
- Second, it will be convenient when we will calculate the gradient tomorow. Be patient, and trust us ;)

## Instructions:
In the cost.py file create the following functions as per the instructions given below:
``` python
def cost_elem_(x, y, theta):
	"""
	Description:
		Calculates all the elements (1/2*M)*(y_pred - y)^2 of the cost function.
	Args:
		x: has to be a numpy.ndarray, a matrix of dimension m * n: (number of training examples, number of features).
		y: has to be a numpy.ndarray, a vector of dimension m * 1: (number of training examples, 1).
		theta: has to be a numpy.ndarray, a vector of dimension (n + 1) * 1: (number of features + 1, 1).
	Returns:
		J_elem: numpy.ndarray, a vector of dimension (number of the training examples,1).
		None if there is a dimension matching problem between X, Y or theta.
	Raises:
		This function should not raise any Exception.
	"""
		... your code here ...

def cost_(x, y, theta):
	"""
	Description:
		Calculates the value of cost function.
	Args:
		x: has to be a numpy.ndarray, a matrix of dimension m * n: (number of training examples, number of features).
		y: has to be a numpy.ndarray, a vector of dimension m * 1: (number of training examples, 1).
		theta: has to be a numpy.ndarray, a vector of dimension (n + 1) * 1: (number of features + 1, 1).
	Returns:
		J_value : has to be a float.
		None if there is a dimension matching problem between X, Y or theta.
	Raises:
		This function should not raise any Exception.
	"""
		... your code here ...
```

## Examples:
```python
import numpy as np

x1 = np.array([[0.], [1.], [2.], [3.], [4.]])
y1 = np.array([[2.], [7.], [12.], [17.], [22.]])
theta1 = np.array([[2.], [4.]])

# Example 1:
cost_elem_(x1, y1, theta1)

# Output:
array([[0.], [0.1], [0.4], [0.9], [1.6]])

# Example 2:
cost_(x1, y1, theta1)

# Output:
3.0

x2 = np.array([[0.2, 2., 20.], [0.4, 4., 40.], [0.6, 6., 60.], [0.8, 8., 80.]])
y2 = np.array([[19.], [42.], [67.], [93.]])
theta2 = np.array([[0.05], [1.], [1.], [1.]])

# Example 3:
cost_elem_(x2, y2, theta2)

# Output:
array([[1.3203125], [0.7503125], [0.0153125], [2.1528125]])

# Example 4:
cost_(x2, y2, theta2)

# Output:
4.238750000000004

x3 = np.array([0, 15, -9, 7, 12, 3, -21])
y3 = np.array([2, 14, -13, 5, 12, 4, -19])
theta3 = np.array([[0.], [1.]])

# Example 5:
cost(x3, y3, theta3)

# Output:
4.285714285714286

# Example 6:
cost(x3, x3, theta3)

# Output:
0.0
```
