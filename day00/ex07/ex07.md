# Exercise 07 - Cost Function

|                         |                    |
| -----------------------:| ------------------ |
|   Turn-in directory :   |  ex07              |
|   Files to turn in :    |  cost.py           |
|   Forbidden functions : |  None              |
|   Remarks :             |  n/a               |

## Objective:
You must implement the following formula as a function (and another one very close to it):

$$
J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(\hat{y}^{(i)} - y^{(i)})^2
$$

Where:
- $\hat{y}$ is a vector of dimension $m * 1$, the vector of predicted values
- $y$ is a vector of dimension $m * 1$, the vector of expected values
- $\hat{y}^{(i)}$ is the ith component of vector $\hat{y}$,
- $y^{(i)}$ is the ith component of vector $y$,

## Instructions:

The implementation of the cost function has been split in two functions:
-  *cost_elem_( )*, which computes the squared distances for all examples
-  *cost_( )*, which averages the distances across all examples

In the cost.py file create the following functions as per the instructions given below:
``` python
def cost_elem_(y, y_hat):
	"""
	Description:
		Calculates all the elements (1/2*M)*(y_pred - y)^2 of the cost function.
	Args:
      y: has to be an numpy.ndarray, a vector.
      y_hat: has to be an numpy.ndarray, a vector.
	Returns:
		J_elem: numpy.ndarray, a vector of dimension (number of the training examples,1).
		None if there is a dimension matching problem between X, Y or theta.
	Raises:
		This function should not raise any Exception.
	"""
		... your code here ...

def cost_(y, y_hat):
	"""
	Description:
		Calculates the value of cost function.
	Args:
      y: has to be an numpy.ndarray, a vector.
      y_hat: has to be an numpy.ndarray, a vector.
	Returns:
		J_value : has to be a float.
		None if there is a dimension matching problem between X, Y or theta.
	Raises:
		This function should not raise any Exception.
	"""
		... your code here ...
```

## Examples:
```python
import numpy as np

x1 = np.array([[0.], [1.], [2.], [3.], [4.]])
theta1 = np.array([[2.], [4.]])
y_hat1 = predict(x1, theta1)
y1 = np.array([[2.], [7.], [12.], [17.], [22.]])

# Example 1:
cost_elem_(y1, y_hat1)

# Output:
array([[0.], [0.1], [0.4], [0.9], [1.6]])

# Example 2:
cost_(y1, y_hat1)

# Output:
3.0

x2 = np.array([[0.2, 2., 20.], [0.4, 4., 40.], [0.6, 6., 60.], [0.8, 8., 80.]])
theta2 = np.array([[0.05], [1.], [1.], [1.]])
y_hat2 = predict_(x2, theta2)
y2 = np.array([[19.], [42.], [67.], [93.]])

# Example 3:
cost_elem_(y2, y_hat2)

# Output:
array([[1.3203125], [0.7503125], [0.0153125], [2.1528125]])

# Example 4:
cost_(y2, y_hat2)

# Output:
4.238750000000004

x3 = np.array([0, 15, -9, 7, 12, 3, -21])
theta3 = np.array([[0.], [1.]])
y_hat3 = predict_(x3, theta3)
y3 = np.array([2, 14, -13, 5, 12, 4, -19])

# Example 5:
cost_(y3, y_hat3)

# Output:
4.285714285714286

# Example 6:
cost_(y3, y3)

# Output:
0.0
```
## More Information:
This cost function is very close to the one called **"Mean Squared Error"**, which is frequently mentioned in Machine Learning resources. The difference is in the denominator as you can see in the formula of the $MSE = \frac{1}{m}\sum_{i=1}^{m}(\hat{y}^{(i)} - y^{(i)})^2$.  

Except the division by $2m$ instead of $m$, these functions are rigourously identical: $J(\theta) = \frac{MSE}{2}$.  

MSE is called like that because it represents the mean of the errors (i.e.: the differences between the predicted values and the true values), squared.

You might wonder why we choose to divide by two instead of simply using the MSE?  
*(It's a good question, by the way.)*
- First, it does not change the overall model evaluation: if all performance measures are divided by two, we can still compare different models and their performance ranking will remain the same.
- Second, it will be convenient when we will calculate the gradient tomorow. Be patient, and trust us ;)
