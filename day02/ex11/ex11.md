# Exercise 11 - Let's Train Polynomial Models! 

|                         |                     |
| -----------------------:| ------------------  |
|   Turn-in directory :   |  ex11               |
|   Files to turn in :    |  polynomial_train.py      |
|   Authorized modules :  |  numpy              |
|   Forbidden modules :   |  sklearn            |

## Objectives:  
It's training time!  
Let's train some polynomial models, and see if those with higher polynomial degree perform better!

## Instructions:
- Take your `are_blue_pills_magic.csv` dataset and train **nine** separate Linear Regression models with polynomial hypotheses with degrees ranging from 2 to 10.  
- Evaluate the cost of each of the nine models.  
- To properly visualize your results, make a bar plot showing the cost of the models given their polynomial degree.  

According to your evaluations, what is the best hypothesis (or model) you can get?

#### Teminology Note:  

The **degree** of a polynomial expression is its highest exponent.  
E.g.: The polynomial degree of $5x^3 - x^6 + 2 x^2$ is $6$.  

Here in this equation, you don't see any terms with $x$, $x^4$ and $x^5$, but we can still say they exist. It's just that their coefficient is $0$. This means that a polynomial linear regression model can lower the impact of any term by bringing its corresponding $\theta_j$ closer to $0$. 


