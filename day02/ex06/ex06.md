# Exercise 06 - Multivariate Gradient Descent

|                         |                     |
| -----------------------:| ------------------  |
|   Turn-in directory :   |  ex06               |
|   Files to turn in :    |  fit.py             |
|   Authorized modules :  |  numpy              |
|   Forbidden functions : |  all functions that perform derivative for you        |


## Objectives: 
* Be able to explain what is a fit in the machine learning context.
* Be able to implement a funcion which will perform a linear gradient descent (LGD).


## Instructions:
You are expected to code a function named __fit\___ as per the instructions bellow:
``` python
def fit_(x, y, theta, alpha, n_cycles):
	"""
	Description:
		Performs a fit of y(output) with respect to x.
	Args:
		theta: has to be a numpy.ndarray, a vector of dimension (n + 1) * 1: (number of features + 1, 1).
		x: has to be a numpy.ndarray, a matrix of dimension m * n: (number of training examples, number of features).
		y: has to be a numpy.ndarray, a vector of dimension m * 1: (number of training examples, 1).
		alpha: has to be a float, the learning rate
		n_cycles: has to be an int, the number of iterations done during the gradient descent
	Returns:
		new_theta: numpy.ndarray, a vector of dimension (number of the features +1,1).
		None if there is a matching dimension problem.
	Raises:
		This function should not raise any Exception.
	"""
		... your code here ...
```
Hopefully, you have already code a function to calculate the multivariate gradient.

## Examples:
```python
import numpy as np
x = np.array([[0.2, 2., 20.], [0.4, 4., 40.], [0.6, 6., 60.], [0.8, 8., 80.]])
y = np.array([[19.6], [-2.8], [-25.2], [-47.6]])
theta = np.array([[42.], [1.], [1.], [1.]])

# Example 0:
theta2 = fit_(X2, Y2, theta2,  alpha = 0.0005, n_cycle=42000)
theta2
# Output:
array([[41.99..],[0.97..], [0.77..], [-1.20..]])

# Example 1:
predict_(X2, theta2)
# Output:
array([[19.5992..], [-2.8003..], [-25.1999..], [-47.5996..]])
```

## Remarks:
You can generate other examples by choosing arbitrary x array and declare y as linear expression of the x columns. Notice also that you can have the components of theta becoming "[nan]". In that case it means you probably used a too big learning rate.
