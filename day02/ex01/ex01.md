# Exercise 01 - Logistic Loss Function

|                         |                         |
| -----------------------:| ----------------------- |
|   Turning directory :   |  ex01                   |
|   Files to turn in :    |  log_loss.py            |
|   Forbidden library :   |  Numpy                  |
|   Remarks :             |  n/a                    |

You must implement the following formula as a function:  

$$
J( \theta) = -\frac{1} {m} * \lbrack \sum_{i = 1}^{m} y^{(i)}log(h_{ \theta }(x^{(i)})) + (1 - y^{(i)})log(1 - h_{ \theta }(x^{(i)}))\rbrack
$$

Where:  
- J(θ) is the cost function with theta being the weights
- m is the length of y, i.e. the number of observations in our sample
- hθ(x) is the calculated hypothesis, also called y_hat or y_pred, it represents the predicted output (formula below)
- y, also called y_true, represents the desired output, either 1 or 0.


This function is called the Cross-Entropy loss or logistic loss.
We encourage you to get a look at
[this section](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression)
of the Cross entropy Wikipedia.

Formula for hypothesis:

$$
\hat{y} = h_{ \theta }(x) = \frac{1} {1 + e^{-\theta \cdot x}}
$$

As you may have noticed, this is our sigmoid function with θ·x as argument.

**Instruction:**

Create a function called `log_loss_` which takes four arguments: 
```python
def log_loss_(y_true, y_pred, m, eps=1e-15):
    """
    :param y_true: a scalar or a vector for the correct labels
    :param y_pred: a scalar or a vector for the predicted labels
    :param m: the length of y_true
    :param eps: epsilon
    :return: the log loss as a scalar
    """
```

Hint : the purpose of epsilon (eps) is to avoid log(0) errors, it is a very small residual value we add to y_pred

!!! The number of rows in y_pred and y_true MUST be the same !!!

**Output examples:**
```python
from sigmoid import sigmoid_
from log_loss import log_loss_

# Test n.1
x = 4
y_true = 1
theta = 0.5
y_pred = sigmoid_(x * theta)
m = 1   # length of y_true is 1
print(log_loss_(y_true, y_pred, m))
# 0.12692801104297152

# Test n.2
x = [1, 2, 3, 4]
y_true = 0
theta = [-1.5, 2.3, 1.4, 0.7]
x_dot_theta = sum([a*b for a, b in zip(x, theta)])
y_pred = sigmoid_(x_dot_theta)
m = 1
print(log_loss_(y_true, y_pred, m))
# 10.100041078687479

# Test n.3
x_new = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]
y_true = [1, 0, 1]
theta = [-1.5, 2.3, 1.4, 0.7]
x_dot_theta = []
for i in range(len(x_new)):
    my_sum = 0
    for j in range(len(x_new[i])):
        my_sum += x_new[i][j] * theta[j]
    x_dot_theta.append(my_sum)
y_pred = sigmoid_(x_dot_theta)
m = len(y_true)
print(log_loss_(y_true, y_pred, m))
# 7.233346147374828
```


