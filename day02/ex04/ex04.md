# Exercise 04 - Vectorized Logistic Gradient

|                         |                         |
| -----------------------:| ----------------------- |
|   Turning directory :   |  ex04                   |
|   Files to turn in :    |  vec_log_grad.py        |
|   Forbidden function :  |  None                   |
|   Remarks :             |  n/a                    |

Same thing as in ex02 but this time use numpy to calculate the gradient.
You should only need one line of code to do this (return ...).

**Instruction:**

Create a function called `vec_log_grad` which takes three arguments:
```python
def vec_log_grad_(x, y_true, y_pred):
    """
    :param x: a vector or a matrix for the samples
    :param y_true: a scalar or a vector for the correct labels
    :param y_pred: a scalar or a vector for the predicted labels
    :return: the gradient as a scalar or a vector of the width of x
    """
```
  
x length (shape[0]) should match y_true and y_pred length, i.e. we should have the same
number of observations.<br>
x width (shape[1]) will be 1 (for the intercept) + the number of coefficients. 
It should match theta length, but this is for later when we will update theta in our gradient descent algorithm.

**Output examples:**
```python
import numpy as np
from sigmoid import sigmoid_
from vec_log_grad import vec_log_grad_

# Test n.1
x = np.array([1, 4.2])  # 1 represent the intercept
y_true = 1
theta = np.array([0.5, -0.5])
y_pred = sigmoid_(np.dot(x, theta))
print(vec_log_grad_(x, y_pred, y_true))
# [0.83201839 3.49447722]

# Test n.2
x = np.array([1, -0.5, 2.3, -1.5, 3.2])
y_true = 0
theta = np.array([0.5, -0.5, 1.2, -1.2, 2.3])
y_pred = sigmoid_(np.dot(x, theta))
print(vec_log_grad_(x, y_true, y_pred))
# [ 0.99999686 -0.49999843  2.29999277 -1.49999528  3.19998994]

# Test n.3
x_new = np.arange(2, 14).reshape((3, 4))
x_new = np.insert(x_new, 0, 1, axis=1)
# first column of x_new are now intercept values initialized to 1
y_true = np.array([1, 0, 1])
theta = np.array([0.5, -0.5, 1.2, -1.2, 2.3])
y_pred = sigmoid_(np.dot(x_new, theta))
print(vec_log_grad_(x_new, y_true, y_pred))
# [0.99994451 5.99988885 6.99983336 7.99977787 8.99972238]
```
