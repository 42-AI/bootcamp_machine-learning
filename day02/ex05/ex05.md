 ### Exercise 05 - Logistic Regression

|                         |                         |
| -----------------------:| ----------------------- |
|   Turning directory :   |  ex05                   |
|   Files to turn in :    |  log_reg.py             |
|   Forbidden function :  |  None                   |
|   Remarks :             |  n/a                    |

Now it's time to use everything you built so far to implement a logistic regression classifier using gradient descent algorithm.

You must have seen the power of numpy for vectorized operations. Well let's make something more concrete with that.

The most curious of you must have noticed that scikit-learn implementation offers a lot of options.
Here we will make a simplified but nonetheless useful and powerful version with fewer options.

**Instruction:**

Implement the class LogisticRegressionBatchGd:
```python
class LogisticRegressionBatchGd:
    def __init__(self, alpha=0.001, max_iter=1000, verbose=False, learning_rate='constant'):
        self.alpha = alpha
        self.max_iter = max_iter
        self.verbose = verbose
        self.learning_rate = learning_rate  # can be 'constant' or 'invscaling'
        self.thetas = []
        # Your code here

    def fit(self, x_train, y_train):
        """
        Perform the gradient descent algorithm an save the weights of the model in the class object
        :param x_train: a vector or a matrix, the training samples
        :param y_train: a scalar or a vector for the desired output, the true labels for x_train
        :return: an object
        """
        # Your code here
    
    def predict(self, x_train):
        """
        :param x_train: a vector or a matrix, the training samples
        :return: y_pred, the predicted class label per sample
        """
        # Your code here
    
    def score(self, x_train, y_train):
        """
        :param x_train: a vector or a matrix, the training samples
        :param y_train: a scalar or a vector for the desired output, the true labels for x_train
        :return: the mean accuracy of self.predict(x_train) with respect to y_true
        """
        # Your code here
```

It would be a good idea to use the functions you created so far in the mathematical part.
A good way to do that is to put them in your class as private methods.

Create a test.py file with the following code to test your implementation:

**Output examples:**
```python
import pandas as pd
from log_reg import LogisticRegressionBatchGd

# We assume our data to be correct (non missing values etc...) and normalized.
# Otherwise you would have to do some data wrangling to get x_train and y_train.
# We also assume we have another dataset to test our model so we don't need to split
# our data into x_train, x_test, y_train, y_test.
df_train = pd.read_csv('dataset_train.csv', index_col="Index")
logreg = LogisticRegressionBatchGd(alpha=0.01, max_iter=1000, 
                                   verbose=False, learning_rate='constant')
x_train, y_train = df_train.loc[:, df_train.columns != 'y_true'], df_train['y_true']
logreg.fit(x_train, y_train)
print(logreg.predict(x_train)) # this is your y_pred
print(logreg.score)
# You can compare your score with the one of scikit-learn.
# Both should match or be really close.
```
