 # Exercise 05 - Logistic Regression

|                         |                         |
| -----------------------:| ----------------------- |
|   Turn-in directory :   |  ex05                   |
|   Files to turn in :    |  log_reg.py             |
|   Forbidden functions : |  None                   |
|   Remarks :             |  n/a                    |

## Objectives:

The time to use everything you built so far has come! Demonstrate your knowledge by implementing a logistic regression classifier using the gradient descent algorithm.
You must have seen the power of NumPy for vectorized operations. Well let's make something more concrete with that.

The most curious among you may have noticed that scikit-learn implementation of the logistic regression offers a lot of options.

The goal of this exercise is to make a simplified but nonetheless useful and powerful version with fewer options.


## Instructions:

In the log_reg.py file create the following class with the following methods as per the instructions below:
```python
class LogisticRegressionBatchGd:
    def __init__(self, alpha=0.001, max_iter=1000, verbose=False, learning_rate='constant'):
        self.alpha = alpha
        self.max_iter = max_iter
        self.verbose = verbose
        self.learning_rate = learning_rate  # can be 'constant' or 'invscaling'
        self.thetas = []
        # Your code here (e.g. a list of loss for each epochs...)

    def fit(self, x_train, y_train):
        """
        Fit the model according to the given training data.
        Args:
            x_train: a 1d or 2d numpy ndarray for the samples
            y_train: a scalar or a numpy ndarray for the correct labels
        Returns: 
            self : object
            None on any error.
        Raises:
            This method should not raise any Exception.
        """
        # Your code here
    
    def predict(self, x_train):
        """
        Predict class labels for samples in x_train.
        Arg:
            x_train: a 1d or 2d numpy ndarray for the samples
        Returns: 
            y_pred, the predicted class label per sample.
            None on any error.
        Raises:
            This method should not raise any Exception.
        """
        # Your code here
    
    def score(self, x_train, y_train):
        """
        Returns the mean accuracy on the given test data and labels.
        Arg:
            x_train: a 1d or 2d numpy ndarray for the samples
            y_train: a scalar or a numpy ndarray for the correct labels
        Returns:
            Mean accuracy of self.predict(x_train) with respect to y_true
            None on any error.
        Raises:
            This method should not raise any Exception.
        """
        # Your code here
```

It would be a good idea to use the functions you created so far in the mathematical part and a good way to do that is to add them to your class as private methods.

After doing this, test your model with both a train dataset and a test dataset.


## Some words about the datasets you will use:

* Resource links are in the resources folder.

* The dataset called 'train_dataset_clean.csv' is a modified version of the Census Income Dataset (https://archive.ics.uci.edu/ml/datasets/census+income).
All the categorical variables have been modified to dummy variables, several columns were dropped ('fnlwgt' and 'education'), the numerical variables we already had ('age', 'education-num', 'capital-gain', 'capital-loss' and 'hours-per-week') were standardized by removing the mean and scaling to unit variance (see sklearn.preprocessing.StandardScaler) and missing values have been imputed taking the mode (the most frequent value).

* The dataset called 'test_dataset_clean.csv' is the same as the dataset 'train_dataset_clean.csv' but with fewer and most importantly different rows.

The goal is to give you two datasets on which you can just train and test your own logistic regression class but if you want to dive a bit further on how the preprocessing was done, I advise you to look at [this](https://towardsdatascience.com/logistic-regression-classifier-on-census-income-data-e1dbef0b5738) article on TDS. 

(Oh, and while I'm at it, TDS is a helpful website with great articles if you wish to learn more about data science or machine learning. I would strongly advise you to put it on your top list of sites you have to visit everyday if you wish to work on a data related position in the future. PS: private navigation in your browser is the way to go for unlimited reading)


## Regarding both datasets:

* The 1st column correspond to the 'income' variable : 0 if '<=50K' and 1 if '>50k'. 
* The 2nd to the 6th columns correspond to the standardized numerical variables : 'age', 'education-num' 'capital-gain', 'capital-loss' and 'hours-per-week'.
* The 7th to the 81th columns correspond to dummy variables for the following categorical variables : 'workClass', 'marital-status', 'occupation', 'relationship', 'race', 'sex' and 'native-country'.

So what we try to do here is to predict with the 81 variables (columns 1 to 81) if the income (column 0) of the person (one person is one row in our data) is lower or equal to 50k ('income' == 0) or is greater than 50k ('income' == 1).


## Examples:

```python
import pandas as pd
import numpy as np
from log_reg import LogisticRegressionBatchGd


# We load and prepare our train and test dataset into x_train, y_train and x_test, y_test
df_train = pd.read_csv('train_dataset_clean.csv', delimiter=',', header=None, index_col=False)
x_train, y_train = np.array(df_train.iloc[:, 1:82]), df_train.iloc[:, 0]
df_test = pd.read_csv('test_dataset_clean.csv', delimiter=',', header=None, index_col=False)
x_test, y_test = np.array(df_test.iloc[:, 1:82]), df_test.iloc[:, 0]

# We set our model with our hyperparameters : alpha, max_iter, verbose and learning_rate
model = LogisticRegressionBatchGd(alpha=0.01, max_iter=1500, verbose=True, learning_rate='constant')

# We fit our model to our dataset and display the score for the train and test datasets
model.fit(x_train, y_train)
print(f'Score on train dataset : {model.score(x_train, y_train)}')
y_pred = model.predict(x_test)
print(f'Score on test dataset  : {(y_pred == y_test).mean()}')

# epoch 0     : loss 2.711028065632692
# epoch 150   : loss 1.760555094793668
# epoch 300   : loss 1.165023422947427
# epoch 450   : loss 0.830808383847448
# epoch 600   : loss 0.652110347325305
# epoch 750   : loss 0.555867078788320
# epoch 900   : loss 0.501596689945403
# epoch 1050  : loss 0.469145216528238
# epoch 1200  : loss 0.448682476966280
# epoch 1350  : loss 0.435197719530431
# epoch 1500  : loss 0.425934034947101
# Score on train dataset : 0.7591904425539756
# Score on test dataset  : 0.7637737239727289

# This is an example with verbose set to True, you could choose to display your loss at the epochs you want.
# Here I choose to only display 11 rows no matter how many epochs I had.
# Your score should be pretty close to mine.
# Your loss may be quite different weither you choose different hyperparameters, if you add an intercept to your x_train
# or if you shuffle your x_train at each epochs (this introduce stochasticity !) etc...
# You might not get a score as good as sklearn.linear_model.LogisticRegression because it uses a different algorithm and
# more optimized parameters that would require more time to implement.
```


## Optional part

**To go further here is what you could do:**

* Keep your loss(self.loss_list ?) at each epoch and plot it after you fitted the model to see how it decreases.
* Keep your learning-rate (self.alpha_list ?) at each epoch and plot it after you fitted the model to see if it has the good behavior (constant vs. invscaling).
