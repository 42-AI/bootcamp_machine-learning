 ### Exercise 05 - Logistic Regression

|                         |                         |
| -----------------------:| ----------------------- |
|   Turning directory :   |  ex05                   |
|   Files to turn in :    |  log_reg.py             |
|   Forbidden function :  |  None                   |
|   Remarks :             |  n/a                    |

Now it's time to use everything you built so far to implement a logistic regression classifier using gradient descent algorithm.

You must have seen the power of numpy for vectorized operations. Well let's make something more concrete with that.

The most curious of you must have noticed that scikit-learn implementation offers a lot of options.
Here we will make a simplified but nonetheless useful and powerful version with fewer options.

**Instruction:**

Implement the class LogisticRegressionBatchGd:
```python
class LogisticRegressionBatchGd:
    def __init__(self, alpha=0.001, max_iter=1000, verbose=False, learning_rate='constant'):
        self.alpha = alpha
        self.max_iter = max_iter
        self.verbose = verbose
        self.learning_rate = learning_rate  # can be 'constant' or 'invscaling'
        self.thetas = []
        # Your code here

    def fit(self, x_train, y_train):
        """
        Perform the gradient descent algorithm an save the weights of the model in the class object
        :param x_train: a vector or a matrix, the training samples
        :param y_train: a scalar or a vector for the desired output, the true labels for x_train
        :return: an object
        """
        # Your code here
    
    def predict(self, x_train):
        """
        :param x_train: a vector or a matrix, the training samples
        :return: y_pred, the predicted class label per sample
        """
        # Your code here
    
    def score(self, x_train, y_train):
        """
        :param x_train: a vector or a matrix, the training samples
        :param y_train: a scalar or a vector for the desired output, the true labels for x_train
        :return: the mean accuracy of self.predict(x_train) with respect to y_true
        """
        # Your code here
```

It would be a good idea to use the functions you created so far in the mathematical part.
A good way to do that is to put them in your class as private methods.

Now that you have done this, you will test your model on a dataset.

The dataset called 'train_dataset_clean.csv' is a modified version of the Census Income Dataset (https://archive.ics.uci.edu/ml/datasets/census+income).
<br>All the categorical variables have been modified to dummy variables, several columns were dropped ('fnlwgt' and 'education'), the numerical variables we already had 
('age', 'education-num', 'capital-gain', 'capital-loss' and 'hours-per-week') were standardized by removing the mean and scaling to unit variance (see sklearn.StandardScaler) and 
missing values have been imputed taking the mode (the most frequent value).

The dataset called 'test_dataset_clean.csv' is the same as the dataset 'train_dataset_clean.csv' but with fewer and most importantly different rows.

The goal is to give you two datasets on which you can just train and test your own logistic regression class but if you want to dive a bit further I advise you to look at this article on TWS 
(https://towardsdatascience.com/logistic-regression-classifier-on-census-income-data-e1dbef0b5738).

The 1st column correspond to the 'income' variable : 0 if '<=50K' and 1 if '>50k'. 
The 2nd to the 6th columns correspond to the standardized numerical variables : 'age', 'education-num', 'capital-gain', 'capital-loss' and 'hours-per-week'.
The 7th to the 81th columns correspond to dummy variables for the following categorical variables : 'workClass', 'marital-status', 'occupation', 'relationship', 'race', 'sex' and 'native-country'

So what we try to do here is to predict with the 81 variables (columns 1 to 81) if the income (column 0) of the person (one person is one row in our data) 
is lower or equal to 50k ('income' == 0) or is greater than 50k ('income' == 1).

Create a test.py file with the following code to test your implementation:

**Output examples:**
```python
import pandas as pd
import numpy as np
from log_reg import LogisticRegressionBatchGd

# We assume our data to be correct (non missing values etc...) and normalized.
# Otherwise you would have to do some data wrangling to get x_train and y_train.
# We also assume we have another dataset to test our model so we don't need to split
# our data into x_train, x_test, y_train, y_test.

# We load and prepare our train and test dataset into x_train, y_train and x_test, y_test
df_train = pd.read_csv('train_dataset_clean.csv', delimiter=',', header=None, index_col=False)
x_train, y_train = np.array(df_train.iloc[:, 1:82]), df_train.iloc[:, 0]
df_test = pd.read_csv('test_dataset_clean.csv', delimiter=',', header=None, index_col=False)
x_test, y_test = np.array(df_test.iloc[:, 1:82]), df_test.iloc[:, 0]

# We set our model with our hyperparameters : alpha, max_iter, verbose and learning_rate
model = LogisticRegressionBatchGd(alpha=0.01, max_iter=1500, verbose=True, learning_rate='constant')

# We fit our model to our dataset and display the score for train dataset and test dataset
model.fit(x_train, y_train)
print(f'Score on train dataset : {model.score(x_train, y_train)}')
y_pred = model.predict(x_test)
print(f'Score on test dataset  : {(y_pred == y_test).mean()}')

# epoch 0     : loss 2.711028065632692
# epoch 150   : loss 1.760555094793668
# epoch 300   : loss 1.165023422947427
# epoch 450   : loss 0.830808383847448
# epoch 600   : loss 0.652110347325305
# epoch 750   : loss 0.555867078788320
# epoch 900   : loss 0.501596689945403
# epoch 1050  : loss 0.469145216528238
# epoch 1200  : loss 0.448682476966280
# epoch 1350  : loss 0.435197719530431
# epoch 1500  : loss 0.425934034947101
# Score on train dataset : 0.7591904425539756
# Score on test dataset  : 0.7637737239727289

# This is an example, you could choose to display your loss at every epoch, or every 100 epochs ...
# Here I choose to only display 10 rows no matter how many epochs I had.
# Your score should be pretty close to mine.
# Your loss may be quite different weither you choose different hyperparameters, if you add an intercept to your x_train,
# if you shuffle you x_train at each epochs etc...
# You might not get a score as good as sklearn.linear_model.LogisticRegression because it uses a different algorithm and more optimized parameters
# that would require more time to implement.


# You can compare your score with the one of scikit-learn.
# Both should match or be really close.
```
