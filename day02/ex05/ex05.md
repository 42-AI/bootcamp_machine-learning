 ### Exercise 05 - Logistic Regression

|                         |                         |
| -----------------------:| ----------------------- |
|   Turning directory :   |  ex05                   |
|   Files to turn in :    |  log_reg.py             |
|   Forbidden function :  |  None                   |
|   Remarks :             |  n/a                    |

Now it's time to use everything you built so far to implement a logistic regression classifier using gradient descent algorithm.

You must have seen the power of numpy for vectorized operations. Well let's make something more concrete with that.

The most curious of you must have noticed that scikit-learn implementation offers a lot of options.
Here we will make a simplified but nonetheless useful and powerful version with fewer options.

Implement the class LogisticRegressionBatchGd:

Parameters of the class to initialize:
  - alpha: float, optional (default=0.001)
  - penalty: str, ‘l2’ or ‘none’, optional (default=’none’)
  - max_iter: int, optional (default=1000)
  - verbose: bool, optional (default=False)
  - learning_rate: str, 'constant' or 'invscaling', optional (default='constant')

Methods to implement inside the class:
```python
def fit(self, x_train, y_train):
```
In this method you will perform the gradient descent algorithm and save your weights.<br>
- x_train: train samples
- y_train: true labels for x_train
- returns: an object

```python
def predict(self, x_train):
```
- x_train: train samples
- returns: y_pred, the predicted class label per sample


```python
def score(self, x_train, y_train):
```
- x_train: train samples
- y_train: true labels for x_train
- returns: mean accuracy of self.predict(X) with respect to y_true

--------
You will need a "__init__" method to initialize the parameters 
used in others methods (i.e.: alpha, max_iter ...).
    
It would be a good idea to use the functions you created so far in the mathematical part.
A good way to do that is to put them in your class as private methods.

Create a test.py file with the following code to test your implementation:
```python
import pandas as pd
from log_reg import LogisticRegressionBatchGd

# We assume our data to be correct (non missing values etc...) and normalized.
# Otherwise you would have to do some data wrangling to get x_train and y_train.
# We also assume we have another dataset to test our model so we don't need to split
# our data into x_train, x_test, y_train, y_test.
df_train = pd.read_csv('dataset_train.csv', index_col="Index")
logreg = LogisticRegressionBatchGd(alpha=0.01, penalty='none', max_iter=1000, 
                                   verbose=False, learning_rate='constant')
x_train, y_train = df_train.loc[:, df_train.columns != 'y_true'], df_train['y_true']
logreg.fit(x_train, y_train)
print(logreg.predict(x_train)) # this is your y_pred
print(logreg.score)
# You can compare your score with the one of scikit-learn.<br>
# Both should match or be really close.
```
