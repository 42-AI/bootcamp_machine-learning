# Exercise 03 - Vectorized Logistic Loss Function

|                         |                         |
| -----------------------:| ----------------------- |
|   Turning directory :   |  ex03                   |
|   Files to turn in :    |  vec_log_loss.py        |
|   Forbidden function :  |  None                   |
|   Remarks :             |  n/a                    |

You must implement the following formula as a function:  

$$
J( \theta) = -\frac{1} {m} * \lbrack \sum_{i = 1}^{m} y^{(i)}log(h_{ \theta }(x^{(i)})) + (1 - y^{(i)})log(1 - h_{ \theta }(x^{(i)}))\rbrack
$$

Where:  
- J(θ) is the cost function with theta being the weights
- m is the length of y, i.e. the number of observations in our sample
- hθ(x) is the calculated hypothesis, also called y_hat or y_pred, it represents the predicted output (formula below)
- y, also called y_true, represents the desired output, either 1 or 0.


This function is called the Cross-Entropy loss or logistic loss.
We encourage you to get a look at
[this section](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression)
of the Cross entropy Wikipedia.

Formula for hypothesis:

$$
\hat{y} = h_{ \theta }(x) = \frac{1} {1 + e^{-\theta \cdot x}}
$$

As you may have noticed, this is our sigmoid function with θ·x as argument.

Create a function called `log_loss_` which takes two arguments: 
  - y_true : a scalar value or a vector for the desired output
  - y_pred : a scalar value or a vector for the hypothesis
  - m : the length of y_true
  - eps : optional, with value 1e-15 by default
  
Hint : the purpose of epsilon (eps) is to avoid log(0) errors, it is a very small residual value we add to y_pred

!!! The number of rows in y_pred and y_true MUST be the same !!!
```python
def vec_log_loss_(y_true, y_pred, m, eps=1e-15):
```

The function must return a scalar value:

```python
>>> import numpy as np
>>> from vec_log_loss import vec_log_loss_
>>> from sigmoid import sigmoid_
>>> x = 4
>>> y_true = 1
>>> theta = 0.5
>>> y_pred = sigmoid_(x * theta)
>>> m = 1   # lenght of y_true is 1
>>> vec_log_loss_(y_true, y_pred, m)
0.126928011043
>>> x = np.array([1, 2, 3, 4])
>>> y_true = 0
>>> theta = np.array([-1.5, 2.3, 1.4, 0.7])
>>> y_pred = sigmoid_(x * theta)
>>> m = 1
>>> vec_log_loss_(y_true, y_pred, m)
11.885332011
>>> x_new = np.arange(1, 13).reshape((3, 4))
>>> y_true = np.array([1, 0, 1])
>>> theta = np.array([-1.5, 2.3, 1.4, 0.7])
>>> y_pred = sigmoid_(np.dot(x_new, theta))
>>> m = len(y_true)
>>> vec_log_loss_(y_true, y_pred, m)
21.7000384421
```
