# Exercise 11 - Let's Train Polynomial Models! 

|                         |                     |
| -----------------------:| ------------------  |
|   Turn-in directory :   |  ex11               |
|   Files to turn in :    |  polynomial_train.py      |
|   Authorized modules :  |  numpy              |
|   Forbidden modules :   |  sklearn            |

## Objectives:  
It's training time!  
Let's train some polynomial models, and see if those with higher polynomial degree perform better!

## Instructions:
- Take your `are_blue_pills_magic.csv` dataset and train **six** separate Linear Regression models with polynomial hypotheses with degrees ranging from 1 to 6. The implementation of the method fit\_ based on the simple gradient descent lakes of efficiency and sturdiness, which will lead to the impossibility of converging for polynomial models with high degree or with features having several orders of magnitude of difference. See the starting value for some theta below to help you to get acceptable parameters values for the models.
- Evaluate the cost of each of the six models.
- To properly visualize your results, make a bar plot showing the cost of the models given their polynomial degree.  

According to your evaluations, what is the best hypothesis (or model) you can get?

#### Starting points:
You will not be able to get acceptable parameters for models 4, 5 and 6. Thus you can start the fit process for those models with:
```python
theta4 = np.array([-20, 160, -80, 10, -1]).reshape(-1,1)
theta5 = np.array([1140, -1850, 1110, -305, 40, -2]).reshape(-1,1)
theta6 = np.array([9110, -18015, 13400, -4935, 966, -96.4, 3.86]).reshape(-1,1)
```

#### Teminology Note:  

The **degree** of a polynomial expression is its highest exponent.  
E.g.: The polynomial degree of $5x^3 - x^6 + 2 x^2$ is $6$.  

Here in this equation, you don't see any terms with $x$, $x^4$ and $x^5$, but we can still say they exist. It's just that their coefficient is $0$. This means that a polynomial linear regression model can lower the impact of any term by bringing its corresponding $\theta_j$ closer to $0$. 


