# Exercise 11 - Let's Train Polynomial Models! 

|                         |                      |
| -----------------------:| -------------------- |
|   Turn-in directory :   |  ex11/               |
|   Files to turn in :    |  polynomial_train.py |
|   Authorized modules :  |  numpy               |
|   Forbidden modules :   |  sklearn             |

## Objective:
Manipulation of polynomial hypothesis.

It's training time!
Let's train some polynomial models, and see if those with higher polynomial degree perform better!

## Instructions:
Write a program which:
- Reads and loads `are_blue_pills_magic.csv`,
- Trains **six** separate linear regression models with polynomials hypothesis where their degree are ranging from 1 to 6,
- Evaluates and prints the evaluation score (MSE) of each of the six models,
- Plots a barplot showing the MSE score of the models in functions of the polynomial degree of the hypothesis,
- Plots the 6 models and the data points on the same figure. Use lineplot style for the models and scaterplot for the data points. Add more prediction points to have smooth curves for the models.

You will used `Micrograms` as feature and `Score` as target. The implementation of the method `fit_` based on the simple gradient descent lakes of efficiency and sturdiness, which will lead to the impossibility of converging for polynomial models with high degree or with features having several orders of magnitude of difference. See the starting values for some thetas below to help you to get acceptable parameters values for the models.


According to your evaluation scores only, what is the best hypothesis (or model) between the trained models? According to the last plot, why is is not true? Which phenomenom do you observed here?

### Starting points:
You will not be able to get acceptable parameters for models 4, 5 and 6. Thus you can start the fit process for those models with:
```python
theta4 = np.array([-20, 160, -80, 10, -1]).reshape(-1,1)
theta5 = np.array([1140, -1850, 1110, -305, 40, -2]).reshape(-1,1)
theta6 = np.array([9110, -18015, 13400, -4935, 966, -96.4, 3.86]).reshape(-1,1)
```

### Teminology Note:  

The **degree** of a polynomial expression is its highest exponent.
E.g.: The polynomial degree of $5x^3 - x^6 + 2 x^2$ is $6$.

Here in this equation, you don't see any terms with $x$, $x^4$ and $x^5$, but we can still say they exist. It's just that their coefficient is $0$. This means that a polynomial linear regression model can lower the impact of any term by bringing its corresponding $\theta_j$ closer to $0$.