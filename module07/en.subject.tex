% vim: set ts=4 sw=4 tw=80 noexpandtab:

\documentclass{42-en}

%******************************************************************************%
%                                                                              %
%                                   Prologue                                   %
%                                                                              %
%******************************************************************************%
\usepackage[
    type={CC},
    modifier={by-nc-sa},
    version={4.0},
]{doclicense}
\usepackage{amsmath} % The amsmath package provides commands to typeset matrices with different delimiters. 
\usepackage{epigraph}
\setlength\epigraphwidth{.95\textwidth}
%****************************************************************%
%                  Re/definition of commands                     %
%****************************************************************%

\newcommand{\ailogo}[1]{\def \@ailogo {#1}}\ailogo{assets/42ai_logo.pdf}

%%  Redefine \maketitle
\makeatletter
\def \maketitle {
  \begin{titlepage}
    \begin{center}
	%\begin{figure}[t]
	  %\includegraphics[height=8cm]{\@ailogo}
	  \includegraphics[height=8cm]{assets/42ai_logo.pdf}
	%\end{figure}
      \vskip 5em
      {\huge \@title}
      \vskip 2em
      {\LARGE \@subtitle}
      \vskip 4em
    \end{center}
    %\begin{center}
	  %\@author
    %\end{center}
	%\vskip 5em
  \vfill
  \begin{center}
    \emph{\summarytitle : \@summary}
  \end{center}
  \vspace{2cm}
  %\vskip 5em
  %\doclicenseThis
  \end{titlepage}
}
\makeatother

\makeatletter
\def \makeheaderfilesforbidden
{
  \noindent
  \begin{tabularx}{\textwidth}{|X X  X X|}
    \hline
  \multicolumn{1}{|>{\raggedright}m{1cm}|}
  {\vskip 2mm \includegraphics[height=1cm]{assets/42ai_logo.pdf}} &
  \multicolumn{2}{>{\centering}m{12cm}}{\small Exercise : \@exnumber } &
  \multicolumn{1}{ >{\raggedleft}p{1.5cm}|}
%%              {\scriptsize points : \@exscore} \\ \hline
              {} \\ \hline

  \multicolumn{4}{|>{\centering}m{15cm}|}
              {\small \@extitle} \\ \hline

  \multicolumn{4}{|>{\raggedright}m{15cm}|}
              {\small Turn-in directory : \ttfamily
                $ex\@exnumber/$ }
              \\ \hline
  \multicolumn{4}{|>{\raggedright}m{15cm}|}
              {\small Files to turn in : \ttfamily \@exfiles }
              \\ \hline

  \multicolumn{4}{|>{\raggedright}m{15cm}|}
              {\small Forbidden functions : \ttfamily \@exforbidden }
              \\ \hline

%%  \multicolumn{4}{|>{\raggedright}m{15cm}|}
%%              {\small Remarks : \ttfamily \@exnotes }
%%              \\ \hline
\end{tabularx}
%% \exnotes
\exrules
\exmake
\exauthorize{None}
\exforbidden{None}
\extitle{}
\exnumber{}
}
\makeatother

%%  Syntactic highlights
\makeatletter
\newenvironment{pythoncode}{%
  \VerbatimEnvironment
  \usemintedstyle{emacs}
  \minted@resetoptions
  \setkeys{minted@opt}{bgcolor=black,formatcom=\color{lightgrey},fontsize=\scriptsize}
  \begin{figure}[ht!]
    \centering
    \begin{minipage}{16cm}
      \begin{VerbatimOut}{\jobname.pyg}}
{%[
      \end{VerbatimOut}
      \minted@pygmentize{c}
      \DeleteFile{\jobname.pyg}
    \end{minipage}
\end{figure}}
\makeatother

\usemintedstyle{native}

\begin{document}

% =============================================================================%
%                     =====================================                    %

\title{Machine Learning - Module 02}
\subtitle{Multivariate Linear Regression}
\author{
  Maxime Choulika (cmaxime), Pierre Peign√© (ppeigne), Matthieu David (mdavid)
}

\summary
{
  Building on what you did on the previous modules you will extend the linear regression to handle more than one features.
  Then you will see how to build polynomial models and how to detect overfitting.
}

\maketitle
\input{usefull_ressources.tex}
\input{en.py_proj.tex}
\newpage
\tableofcontents
\startexercices

%                     =====================================                    %
% =============================================================================%


%******************************************************************************%
%                                                                              %
%                                   Exercises                                  %
%                                                                              %
%******************************************************************************%

% ============================================== %
% ===========================(start ex 00)       %
\chapter{Exercise 00}
\input{en.ex00_interlude.tex}
\newpage
\extitle{Multivariate Hypothesis - Iterative Version}
\turnindir{ex00}
\exnumber{00}
\exfiles{prediction.py}
\exforbidden{None}
\makeheaderfilesforbidden

% ================================== %
\section*{Objective}
% ---------------------------------- %
Manipulate the hypothesis to make prediction.\
You must implement the following formula as a function:

$$
\begin{matrix}
  \hat{y}^{(i)} = \theta_0 + \theta_1 x_{1}^{(i)}  + \dots + \theta_n x_{n}^{(i)} && & \text{ for i = 1, ..., m}
\end{matrix}
$$
  
Where:
\begin{itemize}
  \item $\hat{y}$ is a vector of dimension $m$: the vector of predicted values,
  \item $\hat{y}^{(i)}$ is the i$^\text{th}$ component of the $\hat{y}$ vector: the predicted value for the i$^\text{th}$ example,
  \item $\theta$ is a vector of dimension $(n + 1)$: the parameter vector,
  \item $\theta_j$ is the j$^\text{th}$ component of the parameter vector,
  \item $X$ is a matrix of dimensions $(m \times n)$: the design matrix,
  \item $x^{(i)}$ is the i$^\text{th}$ row of the $X$ matrix: the feature vector of the i$^\text{th}$ example,
  \item $x_{j}$ is the j$^\text{th}$ column of the $X$ matrix,
  \item $x_j^{(i)}$ is the element at the intersection of the i$^\text{th}$ row and the j$^\text{th}$ column of the $X$ matrix: the j$^\text{th}$ feature of the i$^\text{th}$ example
\end{itemize}


% ================================== %
\section*{Instructions}
% ---------------------------------- %

In the \texttt{prediction.py} file, create the following function as per the instructions given below:
\par
\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
  def simple_predict(x, theta):
      """Computes the prediction vector y_hat from two non-empty numpy.array.
      Args:
        x: has to be an numpy.array, a matrix of shape m * n.
        theta: has to be an numpy.array, a vector of shape (n + 1) * 1.
      Return:
        y_hat as a numpy.array, a vector of shape m * 1.
        None if x or theta are empty numpy.array.
        None if x or theta shapes are not appropriate.
        None if x or theta is not of expected type.
      Raises:
        This function should not raise any Exception.
      """
      ... Your code ...
\end{minted}

% ================================== %
\section*{Examples}
% ---------------------------------- %

\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
import numpy as np
x = np.arange(1,13).reshape((4,))


# Example 0:
theta1 = np.array([[5],[ 0],[ 0],[ 0]])
simple_predict(x, theta1)
# Ouput:
array([[5.],[ 5.],[ 5.],[ 5.]])
# Do you understand why y_hat contains only 5's here?  


# Example 1:
theta2 = np.array([[0],[ 1],[ 0],[ 0]])
simple_predict(x, theta2)
# Output:
array([[ 1.],[  4.],[  7.],[ 10.]])
# Do you understand why y_hat == x[:,0] here?  


# Example 2:
theta3 = np.array([[-1.5],[ 0.6],[ 2.3],[ 1.98]])
simple_predict(X, theta3)
# Output:
array([[ 9.64],[ 24.28],[ 38.92],[ 53.56]])


# Example 3:
theta4 = np.array([[-3],[ 1],[ 2],[ 3.5]])
simple_predict(x, theta4)
# Output:
array([[12.5],[ 32. ],[ 51.5],[ 71. ]])
\end{minted}

% ===========================(fin ex 00)         %
% ============================================== %

\newpage

% ============================================== %
% ===========================(start ex 01)       %
\chapter{Exercise 01}
\input{en.ex01_interlude.tex}
\newpage
\extitle{Multivariate hypothesis - vectorized version}
\turnindir{ex01}
\exnumber{01}
\exfiles{prediction.py}
\exforbidden{None}
\makeheaderfilesforbidden

% ================================= %
\section*{Objective}
% --------------------------------- %
Manipulate the hypothesis to make prediction.\
You must implement the following formula as a function:  

$$
\hat{y} = X' \cdot \theta = 
\begin{bmatrix} 
1 & x_{1}^{(1)} & \dots & x_{n}^{(1)}\\
\vdots & \vdots & \ddots & \vdots\\
1 & x_{1}^{(m)} & \dots &  x_{n}^{(m)}\end{bmatrix}
\cdot
\begin{bmatrix}
\theta_0 \\ 
\theta_1 \\
\vdots \\
\theta_n
\end{bmatrix} 
= 
\begin{bmatrix} 
\theta_0 + \theta_{1} x_{1}^{(1)} + \dots + \theta_{n} x_{n}^{(1)}\\ 
\vdots \\ 
\theta_0 + \theta_{1} x_{1}^{(m)} + \dots + \theta_{n} x_{n}^{(m)}
\end{bmatrix}
$$

Where:
\begin{itemize}
  \item $\hat{y}$ is a vector of dimension $m$: the vector of predicted values,
  \item $X$ is a matrix of dimensions $(m \times n)$: the design matrix,
  \item $X'$ is a matrix of dimensions $(m \times (n + 1))$: the design matrix onto which a column of $1$'s was added as a first column,
  \item $\theta$ is a vector of dimension $(n + 1)$: the parameter vector,
  \item $x^{(i)}$ is the i$^\text{th}$ row of the $X$ matrix,
  \item $x_{j}$ is the j$^\text{th}$ column of the $X$ matrix,
  \item $x_j^{(i)}$ is the intersection of the i$^\text{th}$ row and the j$^\text{th}$ column of the $X$ matrix: the j$^\text{th}$ feature of the i$^\text{th}$ training example.
\end{itemize}


Be careful: 
\begin{itemize}
  \item The \texttt{x} argument your function will receive as an input corresponds to $X$, the $(m \times n)$ matrix.
        Not $X'$.
  \item \texttt{theta} is an $(n + 1)$ vector.
\end{itemize}

% ================================= %
\section*{Instructions}
% --------------------------------- %
In the \texttt{prediction.py} file, write the \texttt{predict\_} function as per the instructions given below:

\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
def predict_(x, theta):
    """Computes the prediction vector y_hat from two non-empty numpy.array.
    Args:
      x: has to be an numpy.array, a vector of shapes m * n.
      theta: has to be an numpy.array, a vector of shapes (n + 1) * 1.
    Return:
      y_hat as a numpy.array, a vector of shapes m * 1.
      None if x or theta are empty numpy.array.
      None if x or theta shapes are not appropriate.
      None if x or theta is not of expected type.
    Raises:
      This function should not raise any Exception.
    """
    ... Your code ...
\end{minted}

% ================================= %
\section*{Examples}
% --------------------------------- %

\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
import numpy as np
x = np.arange(1,13).reshape((4,))


# Example 0:
theta1 = np.array([[5],[ 0],[ 0],[ 0]])
predict_(x, theta1)
# Ouput:
array([[5.],[ 5.],[ 5.],[ 5.]])
# Do you understand why y_hat contains only 5's here?  


# Example 1:
theta2 = np.array([[0],[ 1],[ 0],[ 0]])
predict_(x, theta2)
# Output:
array([[ 1.],[  4.],[  7.],[ 10.]])
# Do you understand why y_hat == x[:,0] here?  


# Example 2:
theta3 = np.array([[-1.5],[ 0.6],[ 2.3],[ 1.98]])
predict_(X, theta3)
# Output:
array([[ 9.64],[ 24.28],[ 38.92],[ 53.56]])


# Example 3:
theta4 = np.array([[-3],[ 1],[ 2],[ 3.5]])
predict_(x, theta4)
# Output:
array([[12.5],[ 32. ],[ 51.5],[ 71. ]])
\end{minted}

% ===========================(fin ex 01)         %
% ============================================== %

\newpage

% ============================================== %
% ===========================(start ex 02)       %
\chapter{Exercise 02}
\input{en.ex02_interlude.tex}
\newpage
\extitle{Vectorized Loss Function}
\turnindir{ex02}
\exnumber{02}
\exfiles{loss.py}
\exforbidden{None}
\makeheaderfilesforbidden

% ================================= %
\section*{Objective}
% --------------------------------- %
Understand and manipulate loss function for multivariate linear regression.\
You must implement the following formula as a function:  

$$
\begin{matrix}
J(\theta) &  = & \frac{1}{2m}(\hat{y} - y) \cdot(\hat{y}- y)
\end{matrix}
$$  

Where:
\begin{itemize}
  \item $\hat{y}$ is a vector of dimension $m$, the vector of predicted values,
  \item $y$ is a vector of dimension $m$, the vector of expected values.
\end{itemize}


% ================================= %
\section*{Instructions}
% --------------------------------- %
In the \texttt{loss.py} file create the following function as per the instructions given below:

\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
def loss_(y, y_hat):
    """Computes the mean squared error of two non-empty numpy.array, without any for loop.
    The two arrays must have the same shapes.
    Args:
      y: has to be an numpy.array, a vector.
      y_hat: has to be an numpy.array, a vector.
    Return:
      The mean squared error of the two vectors as a float.
      None if y or y_hat are empty numpy.array.
      None if y and y_hat does not share the same shapes.
      None if y or y_hat is not of expected type.
    Raises:
      This function should not raise any Exception.
    """
    ... Your code ...
\end{minted}

% ================================= %
\section*{Examples}
% --------------------------------- %
\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
import numpy as np
X = np.array([[0],[ 15],[ -9],[ 7],[ 12],[ 3],[ -21]])
Y = np.array([[2],[ 14],[ -13],[ 5],[ 12],[ 4],[ -19]])


# Example 0:
loss_(X, Y)
# Output:
2.1428571428571436


# Example 1:
loss_(X, X)
# Output:
0.0
\end{minted}


% ===========================(fin ex 02)         %
% ============================================== %

\newpage

% ============================================== %
% ===========================(start ex 03)       %
\chapter{Exercise 03}
\extitle{Multivariate Linear Gradient}
\input{en.ex03_interlude.tex}
\newpage
\turnindir{ex03}
\exnumber{03}
\exfiles{gradient.py}
\exforbidden{None}
\makeheaderfilesforbidden

% ================================= %
\section*{Objective}
% --------------------------------- %
Understand and manipulate concept of gradient in the case of multivariate formulation.\
You must implement the following formula as a function:    

$$
\nabla(J) = \frac{1}{m} {X'}^T(X'\theta - y)
$$  

Where:  
\begin{itemize}
  \item $\nabla(J)$ is a vector of dimension $(n + 1)$, the gradient vector,
  \item $X$ is a matrix of dimensions $(m \times n)$, the design matrix,
  \item $X'$ is a matrix of dimensions $(m \times (n + 1))$, the design matrix onto which a column of $1$'s was added as a first column,
  \item $\theta$ is a vector of dimension $(n + 1)$, the parameter vector,
  \item $y$ is a vector of dimension $m$, the vector of expected values.
\end{itemize}


% ================================= %
\section*{Instructions}
% --------------------------------- %
In the \texttt{gradient.py} file, create the following function as per the instructions given below:
\par
\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
def gradient(x, y, theta):
    """Computes a gradient vector from three non-empty numpy.array, without any for-loop.
    The three arrays must have the compatible shapes.
    Args:
      x: has to be an numpy.array, a matrix of shape m * n.
      y: has to be an numpy.array, a vector of shape m * 1.
      theta: has to be an numpy.array, a vector (n +1) * 1.
    Return:
      The gradient as a numpy.array, a vector of shapes n * 1,
        containg the result of the formula for all j.
      None if x, y, or theta are empty numpy.array.
      None if x, y and theta do not have compatible shapes.
      None if x, y or theta is not of expected type.
    Raises:
      This function should not raise any Exception.
    """
    ... Your code ...
\end{minted}

% ================================= %
\section*{Examples}
% --------------------------------- %
\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
import numpy as np
x = np.array([
        [ -6,  -7,  -9],
        [ 13,  -2,  14],
        [ -7,  14,  -1],
        [ -8,  -4,   6],
        [ -5,  -9,   6],
        [  1,  -5,  11],
        [  9, -11,   8]])
y = np.array([[2],[ 14],[ -13],[ 5],[ 12],[ 4],[ -19]])
theta1 = np.array([[0],[ 3],[ 0.5],[ -6]])


# Example 0:
gradient(x, y, theta1)
# Output:
array([[ -33.71428571],[  -37.35714286],[ 183.14285714],[ -393.]])


# Example 1:
theta2 = np.array([[0],[ 0],[ 0],[ 0]])
gradient(x, y, theta2)
# Output:
array([[ -0.71428571],[   0.85714286],[ 23.28571429],[ -26.42857143]])
\end{minted}

% ===========================(fin ex 03)         %
% ============================================== %

\newpage

% ============================================== %
% ===========================(start ex 04)       %
\chapter{Exercise 04}
\extitle{Multivariate Gradient Descent}
\turnindir{ex04}
\exnumber{04}
\exfiles{fit.py}
\exforbidden{any function that performs derivatives for you}
\makeheaderfilesforbidden


% ================================= %
\section*{Objective}
% --------------------------------- %
Understand and manipulate the concept of gradient descent in the case of multivariate linear regression.\
Implement a function to perform linear gradient descent (LGD) for multivariate linear regression.

% ================================= %
\section*{Instructions}
% --------------------------------- %
In this exercise, you will implement linear gradient descent to fit your multivariate model to the dataset.

The pseudocode of the algorithm is the following:

$$
\begin{matrix}
    &   \text{repeat until convergence} \hspace{1cm} &  \{  \\
    &   \text{compute } \nabla{(J)}  \\
    &	\theta := \theta - \alpha \nabla(J)                 \\ 
\} 
\end{matrix}
$$

Where:
\begin{itemize}
  \item $\nabla{(J)}$ is the entire gradient vector,
  \item $\theta$ is the entire parameter vector,
  \item $\alpha$ (alpha) is the learning rate (a small number, usually between 0 and 1).
\end{itemize}


You are expected to write a function named \textit{fit\_} as per the instructions bellow:

\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
def fit_(x, y, theta, alpha, max_iter):
  """
  Description:
    Fits the model to the training dataset contained in x and y.
  Args:
    x: has to be a numpy.array, a matrix of shape m * n:
                  (number of training examples, number of features).
    y: has to be a numpy.array, a vector of shape m * 1:
                  (number of training examples, 1).
    theta: has to be a numpy.array, a vector of shape (n + 1) * 1:
                  (number of features + 1, 1).
    alpha: has to be a float, the learning rate
    max_iter: has to be an int, the number of iterations done during the gradient descent
  Return:
    new_theta: numpy.array, a vector of shape (number of features + 1, 1).
    None if there is a matching shape problem.
    None if x, y, theta, alpha or max_iter is not of expected type.
  Raises:
    This function should not raise any Exception.
  """
    ... your code here ...
\end{minted}

Hopefully, you have already implemented a function to calculate the multivariate gradient.

% ================================= %
\section*{Examples}
% ================================= %
\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
import numpy as np
x = np.array([[0.2, 2., 20.], [0.4, 4., 40.], [0.6, 6., 60.], [0.8, 8., 80.]])
y = np.array([[19.6], [-2.8], [-25.2], [-47.6]])
theta = np.array([[42.], [1.], [1.], [1.]])


# Example 0:
theta2 = fit_(X2, Y2, theta2,  alpha = 0.0005, max_iter=42000)
theta2
# Output:
array([[41.99..],[0.97..], [0.77..], [-1.20..]])


# Example 1:
predict_(X2, theta2)
# Output:
array([[19.5992..], [-2.8003..], [-25.1999..], [-47.5996..]])
\end{minted}

\info{
  \begin{itemize}
    \item You can create more training data by generating an $x$ array with random values and computing the corresponding $y$ vector as a linear expression of $x$.
          You can then fit a model on this artificial data and find out if it comes out with the same $\theta$ coefficients that first you used.
    \item It is possible that $\theta_0$ and $\theta_1$ become \texttt{"nan"}.
          In that case, it means you probably used a learning rate that is too large.
  \end{itemize}
}

% ===========================(fin ex 04)         %
% ============================================== %

\newpage

% ============================================== %
% ===========================(start ex 05)       %
\chapter{Exercise 05}
\extitle{Multivariate Linear Regression with Class}
\turnindir{ex05}
\exnumber{05}
\exfiles{mylinearregression.py}
\exforbidden{sklearn}
\makeheaderfilesforbidden

% ================================= %
\section*{Objective}
% --------------------------------- %
Upgrade your Linear Regression class so it can handle multivariate hypotheses.

% ================================= %
\section*{Instructions}
% --------------------------------- %
You are expected to upgrade your own \texttt{MyLinearRegression} class from \textbf{Module01}.
You will upgrade (at least) the following methods to support multivariate linear regression:
\begin{itemize}
  \item \texttt{predict\_(self, x)}, 
  \item \texttt{fit\_(self, x, y)}.
\end{itemize}
Depending on how you implement your methods, you might need to update other methods.

% ================================= %
\section*{Examples}
% --------------------------------- %
\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
import numpy as np
from mylinearregression import MyLinearRegression as MyLR
X = np.array([[1., 1., 2., 3.], [5., 8., 13., 21.], [34., 55., 89., 144.]])
Y = np.array([[23.], [48.], [218.]])
mylr = MyLR([[1.], [1.], [1.], [1.], [1]])


# Example 0:
mylr.predict_(X)
# Output:
array([[8.], [48.], [323.]])


# Example 1:
mylr.loss_elem_(X,Y)
# Output:
array([[225.], [0.], [11025.]])


# Example 2:
mylr.loss_(X,Y)
# Output:
1875.0


# Example 3:
mylr.alpha = 1.6e-4
mylr.max_iter = 200000
mylr.fit_(X, Y)
mylr.theta
# Output:
array([[18.188..], [2.767..], [-0.374..], [1.392..], [0.017..]])


# Example 4:
mylr.predict_(X)
# Output:
array([[23.417..], [47.489..], [218.065...]])


# Example 5:
mylr.loss_elem_(X,Y)
# Output:
array([[0.174..], [0.260..], [0.004..]])


# Example 6:
mylr.loss_(X,Y)
# Output:
0.0732..
\end{minted}

% ===========================(fin ex 05)         %
% ============================================== %

\newpage

% ============================================== %
% ===========================(start ex 06)       %
\chapter{Exercise 06}
\extitle{Practicing Multivariate Linear Regression}
\turnindir{ex06}
\exnumber{06}
\exfiles{multivariate\_linear\_model.py}
\exforbidden{sklearn}
\makeheaderfilesforbidden

% ================================= %
\section*{Objective}
% --------------------------------- %
Fit a linear regression model to a dataset with multiple features.\
Plot the model's predictions and interpret the graphs.

% ================================= %
\section*{Dataset}
% --------------------------------- %
During last module, you performed a univariate linear regression on a dataset to make predictions based on ONE feature (well done!).
Now, it's time to dream bigger.
Lucky you are, we give you a new dataset with multiple features that you will find in the resources attached.
The dataset is called \texttt{spacecraft\_data.csv} and it describes a set of spacecrafts with their price, as well as a few other features.
A description of the dataset is provided in the file named \texttt{spacecraft\_data\_description.txt}.

% ================================= %
\subsection*{Part One: Univariate Linear Regression}
% --------------------------------- %
To start, we'll build on the previous module's work and see how a univariate model can predict spaceship prices.
As you know, univariate models can only process ONE feature at a time.
So to train each model, you need to select a feature and ignore the other ones.
\newpage
% ================================= %
\subsubsection*{Instructions}
% --------------------------------- %
In the first part of the exercise, you will train three different univariate models to predict spaceship prices.
Each model will use a different feature of the spaceships.
For each feature, your program has to perform a gradient descent from a new set of thetas, plot or generate a plot,
print the final value of the thetas and the MSE of the corresponding model.

% ================================= %
\subsubsection*{Age}
% --------------------------------- %
Select the \texttt{Age} feature as your $x$ vector, and \texttt{Sell\_price} as your $y$ vector.
Train a first model, \texttt{myLR\_age}, and generate price predictions ($\hat{y}$).
Output a scatter plot with both sets of data points on the same graph, as follows:
\begin{itemize}
  \item The actual prices, given by $(x_{age}^{(i)}$, $y^{(i)})$  for $i=0....m$,
  \item The predicted prices, represented by  $(x_{age}^{(i)}$, $\hat{y}^{(i)})$  for $i=0....m$ (see example below).
\end{itemize}

\begin{figure}[!h]
  \centering
  \includegraphics[scale=0.6]{assets/ex07_price_vs_age_part1.png}
  \caption{Plot of the selling prices of spacecrafts with respect to their age, as well as our first model's price predictions.}
\end{figure}

% ================================= %
\subsubsection*{Thrust}
% --------------------------------- %
Select the \textit{Thrust\_power} feature as your $x$ vector, and \textit{Sell\_price} as your $y$ vector.
Train a second model, \texttt{myLR\_thrust}, and generate price predictions ($\hat{y}$).
Output a scatter plot with both sets of data points on the same graph, as follows:
\begin{itemize}
  \item The actual prices, given by $(x_{thrust}^{(i)}$, $y^{(i)})$  for $i=0....m$,
  \item The predicted prices, represented by  $(x_{thrust}^{(i)}$, $\hat{y}^{(i)})$  for $i=0....m$ (see example below).
\end{itemize}

\begin{figure}[!h]
  \centering
  \includegraphics[scale=0.6]{assets/ex07_price_vs_thrust_part1.png}
  \caption{Plot of the selling prices of spacecrafts with respect to the thrust power of their engines, as well as our second model's price predictions.}
\end{figure}
\newpage
% ================================= %
\subsubsection*{Total distance}
% --------------------------------- %
Select the \textit{Terameters} feature as your $x$ vector, and \textit{Sell\_price} as your $y$ vector.
Train a third model, \texttt{myLR\_distance}, and make price predictions ($\hat{y}$).
Output a scatter plot with both sets of data points on the same graph, as follows:
\begin{itemize}
  \item The actual prices, given by $(x_{distance}^{(i)}$, $y^{(i)})$  for $i=0....m$, 
  \item The predicted prices, represented by  $(x_{distance}^{(i)}$, $\hat{y}^{(i)})$  for $i=0....m$  (see example below),
\end{itemize}

\begin{figure}[!h]
  \centering
  \includegraphics[scale=0.6]{assets/ex07_price_vs_Tmeters_part1.png}
  \caption{Plot of the selling prices of spacecrafts with respect to the terameters driven, as well as our third model's price predictions.}
\end{figure}

% ================================= %
\subsubsection*{Reminder}
% --------------------------------- %
\begin{itemize}
  \item After executing the \texttt{fit\_} method, you may obtain  $\theta = array([["nan", "nan"]])$.
    If it happens, try reducing your learning rate.
  \item Be aware that you also need to set the appropriate number of cycles for the \texttt{fit\_} function.
        If it's too low, you might not have allowed enough cycles for the gradient descent to carry out properly.
        Try to find a value that gets you the best score, but that doesn't make the training last forever.
\end{itemize}

\hint{
  First, try plotting the data points $(x_{j},y)$.
  Then you can guess initial theta values that are not too far off.
  This will help your algorithm converge more easily.
}

% ================================= %
\subsubsection*{Examples}
% --------------------------------- %

\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
import pandas as pd
import numpy as np
from mylinearregression import MyLinearRegression as MyLR

data = pd.read_csv("spacecraft_data.csv")
X = np.array(data[['Age']])
Y = np.array(data[['Sell_price']])
myLR_age = MyLR(theta = [[1000.0], [-1.0]], alpha = 2.5e-5, max_iter = 100000)
myLR_age.fit_(X[:,0].reshape(-1,1), Y)

myLR_age.mse_(X[:,0].reshape(-1,1),Y)
#Output
57636.77729...
\end{minted}

How accurate is your model when you only take one feature into account?

% ================================= %
\subsection*{Part Two: Multivariate Linear Regression (A New Hope)}
% --------------------------------- %

Now, it's time for your first multivariate linear regression!

% ================================= %
\subsubsection*{Instructions}
% --------------------------------- %
Here, you will train a single model that will take all features into account.
Your program is expected to perform steps similar to the ones in the part one (fitting, displaying or generating 3 graphs, printing the thetas and the MSE).

% ================================= %
\subsubsection*{Training the model}
% --------------------------------- %
\begin{itemize}
  \item Train a single multivariate linear regression model on all three features.
  \item Display and interpret the resulting theta parameters.
        What can you say about the role that each feature plays in the price prediction?
  \item Evaluate the model with the Mean Squared Error.
        How good is your model doing, compared to the other three that you trained in Part One of this exercise?
\end{itemize}

\info{
  You can obtain a better fit if you increase the number of cycles.
}

% ================================= %
\subsubsection*{Examples}
% --------------------------------- %
\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
import pandas as pd
import numpy as np
from mylinearregression import MyLinearRegression as MyLR

data = pd.read_csv("spacecraft_data.csv")
X = np.array(data[['Age','Thrust_power','Terameters']])
Y = np.array(data[['Sell_price']])
my_lreg = MyLR(theta = [1.0, 1.0, 1.0, 1.0], , alpha = 1e-4, max_iter = 600000)


# Example 0:
my_lreg.mse_(X,Y)
# Output:
144044.877...


# Example 1:
my_lreg.fit_(X,Y)
my_lreg.theta
# Output:
array([[334.994...],[-22.535...],[5.857...],[-2.586...]])


# Example 2:
my_lreg.mse_(X,Y)
# Output:
586.896999...
\end{minted}

% ================================= %
\subsubsection*{Plotting the predictions}
% --------------------------------- %
Here we'll plot the model's predictions just like we did in Part One.
We'll make three graphs, each one displaying the predictions and the actual prices as a function of ONE of the features.

\begin{itemize}
  \item On the same graph, plot the actual and predicted prices on the $y$ axis , and the $age$ feature on the $x$ axis. (see figure below)
  \begin{figure}[!h]
    \centering
    \includegraphics[scale=0.6]{assets/ex07_price_vs_age_part2.png}
    \caption{Spacecraft sell prices of and predicted sell prices  with the multivariate hypothesis, with respect to the \textit{age} feature}
  \end{figure}
  
  \item On the same graph, plot the actual and predicted prices on the $y$ axis , and the $thrust power$ feature on the $x$ axis. (see figure below)
  \begin{figure}[!h]
    \centering
    \includegraphics[scale=0.6]{assets/ex07_price_vs_thrust_part2.png}
    \caption{Spacecraft sell prices predicted sell prices with the multivariate hypothesis, with respect to the thrust power of the engines}
  \end{figure}
  
  \item On the same graph, plot the actual and predicted prices on the $y$ axis , and the $distance$ feature on the $x$ axis. (see figure below)
  \begin{figure}[!h]
    \centering
    \includegraphics[scale=0.6]{assets/ex07_price_vs_Tmeters_part2.png}
    \caption{Spacecraft sell prices and predicted sell prices with the multivariage hypothesis, with respect to the driven distance (in terameters)}
  \end{figure}
\end{itemize}

Can you see any improvement on these three graphs, compared to the three that you obtained in Part One?
Can you relate your observations to the MSE value that you just calculated?

% ===========================(fin ex 06)         %
% ============================================== %

\newpage

% ============================================== %
% ===========================(start ex 07)       %
\chapter{Exercise 07}
\extitle{Polynomial models}
\input{en.ex07_interlude.tex}
\newpage
\turnindir{ex07}
\exnumber{07}
\exfiles{polynomial\_model.py}
\exforbidden{sklearn}
\makeheaderfilesforbidden


% ================================= %
\section*{Objective}
% --------------------------------- %
Broaden the comprehension of the notion of hypothesis.\
Create a function that takes a vector $x$ of dimension $m$ and an integer $n$ as input, and returns a matrix of dimensions $(m \times n)$.
Each column of the matrix contains $x$ raised to the power of $j$, for $j = 1, 2, ..., n$:

$$
\begin{matrix}
x &|& x^2 &|& x^3 &|& \ldots &|& x^n
\end{matrix}
$$

Such a matrix is called a \textbf{Vandermonde matrix}.

% ================================= %
\section*{Instructions}
% --------------------------------- %
In the texttt{polynomial\_model.py} file, create the following function as per the instructions given below:

\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
def add_polynomial_features(x, power):
    """Add polynomial features to vector x by raising its values up to the power given in argument.  
    Args:
      x: has to be an numpy.array, a vector of shape m * 1.
      power: has to be an int, the power up to which the components of vector x are going to be raised.
    Return:
      The matrix of polynomial features as a numpy.array, of shape m * n,
        containing the polynomial feature values for all training examples.
      None if x is an empty numpy.array.
      None if x or power is not of expected type.
    Raises:
      This function should not raise any Exception.
    """
    ... Your code ...
\end{minted}

% ================================= %
\section*{Examples}
% --------------------------------- %
\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
import numpy as np
x = np.arange(1,6).reshape(-1, 1)


# Example 0:
add_polynomial_features(x, 3)
# Output:
array([[  1,   1,   1],
       [  2,   4,   8],
       [  3,   9,  27],
       [  4,  16,  64],
       [  5,  25, 125]])


# Example 1:
add_polynomial_features(x, 6)
# Output:
array([[    1,     1,     1,     1,     1,     1],
       [    2,     4,     8,    16,    32,    64],
       [    3,     9,    27,    81,   243,   729],
       [    4,    16,    64,   256,  1024,  4096],
       [    5,    25,   125,   625,  3125, 15625]])
\end{minted}

% ===========================(fin ex 07)         %
% ============================================== %

\newpage

% ============================================== %
% ===========================(start ex 08)       %
\chapter{Exercise 08}
\extitle{Let's Train Polynomial Models!}
\input{en.ex08_interlude.tex}
\newpage
\turnindir{ex08}
\exnumber{08}
\exfiles{polynomial\_train.py}
\exforbidden{sklearn}
\makeheaderfilesforbidden


% ================================= %
\section*{Objective}
% --------------------------------- %
Manipulation of polynomial hypothesis.\
It's training time!\
Let's train some polynomial models, and see if those with higher polynomial degree perform better!

Write a program which:
\begin{itemize}
  \item Reads and loads \texttt{are\_blue\_pills\_magics.csv} dataset,
  \item Trains \textbf{six} separate Linear Regression models with polynomial hypothesis with degrees ranging from 1 to 6,
  \item Evaluates and prints evaluation score (MSE) of each of the six models,
  \item Plots a bar plot showing the MSE score of the models in function of the polynomial degree of the hypothesis,
  \item Plots the 6 models and the data points on the same figure.
        Use lineplot style for the models and scaterplot for the data points.
        Add more prediction points to have smooth curves for the models.
\end{itemize}

You will used \texttt{Micrograms} as feature and \texttt{Score} as target.
The implementation of the method \texttt{fit\_} based on the simple gradient descent lakes of efficiency and sturdiness,
which will lead to the impossibility of converging for polynomial models with high degree or with features having several orders of magnitude of difference.
See the starting values for some thetas below to help you to get acceptable parameters values for the models.

According to evaluation score only, what is the best hypothesis (or model) between the trained models?
According to the last plot, why it is not true?
Which phenomenom do you observed here?

% ================================= %
\subsection*{Starting points}
% --------------------------------- %
You will not be able to get acceptable parameters for models 4, 5 and 6.
Thus you can start the fit process for those models with:

\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
theta4 = np.array([[-20],[ 160],[ -80],[ 10],[ -1]]).reshape(-1,1)
theta5 = np.array([[1140],[ -1850],[ 1110],[ -305],[ 40],[ -2]]).reshape(-1,1)
theta6 = np.array([[9110],[ -18015],[ 13400],[ -4935],[ 966],[ -96.4],[ 3.86]]).reshape(-1,1)
\end{minted}

% ================================= %
\subsection*{Teminology Note}
% --------------------------------- %
The \textbf{degree} of a polynomial expression is its highest exponent.  
E.g.: The polynomial degree of $5x^3 - x^6 + 2 x^2$ is $6$.  


Here in this equation, you don't see any terms with $x$, $x^4$ and $x^5$,but we can still say they exist. It's just that their coefficient is $0$.
This means that a polynomial linear regression model can lower the impact of any term by bringing its corresponding $\theta_j$ closer to $0$.

% ================================= %
\subsection*{Remark}
% --------------------------------- %
When you will be evaluated, it will be wised to run your program at the beginning of the evaluation as it can take several minutes to train the different models.

% ===========================(fin ex 08)         %
% ============================================== %

\newpage

% ============================================== %
% ===========================(start ex 09)       %
\chapter{Exercise 09}
\extitle{DataSpliter}
\input{en.ex09_interlude.tex}
\newpage
\turnindir{ex09}
\exnumber{09}
\exfiles{data\_spliter.py}
\exforbidden{sklearn}
\makeheaderfilesforbidden


% ================================= %
\section*{Objective}
% --------------------------------- %
Learn how to split a dataset into a \textbf{training set} and a \textbf{test set}.

% ================================= %
\section*{Instructions}
% --------------------------------- %
You must implement a function that \textbf{shuffles} and \textbf{splits} a dataset it in two parts: a \textbf{training set} and a \textbf{test set}.
\begin{itemize}
  \item Your function will shuffle and split the $X$ matrix while keeping a certain \textbf{proportion} of the examples for training, and the rest for testing.
  \item Your function will also shuffle and split the $y$ vector while making sure that the order of the rows in the output match the order of the rows in the split $X$ output.
\end{itemize}


In the \texttt{data\_spliter.py} file create the following function as per the instructions given below:

\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
def data_spliter(x, y, proportion):
    """Shuffles and splits the dataset (given by x and y) into a training and a test set,
      while respecting the given proportion of examples to be kept in the training set.
    Args:
      x: has to be an numpy.array, a matrix of shape m * n.
      y: has to be an numpy.array, a vector of shape m * 1.
      proportion: has to be a float, the proportion of the dataset that will be assigned to the
        training set.
    Return:
      (x_train, x_test, y_train, y_test) as a tuple of numpy.array
      None if x or y is an empty numpy.array.
      None if x and y do not share compatible shapes.
      None if x, y or proportion is not of expected type.
    Raises:
      This function should not raise any Exception.
    """
    ... Your code ...
\end{minted}

\warn{
  \begin{itemize}
    \item The dataset has to be randomly shuffled \textit{before} it is split into training and test sets.
    \item Unless you use the same seed in your randomization algorithm, you won't get the same results twice.
  \end{itemize}
}

% ================================= %
\section*{Examples}
% --------------------------------- %
The following examples are just an indication of possible outputs. As long as you have shuffled datasets with their corresponding y values, your function is working correctly.

\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
import numpy as np
x1 = np.array([[1],[ 42],[ 300],[ 10],[ 59]])
y = np.array([[0],[1],[0],[1],[0]])

# Example 0:
data_spliter(x1, y, 0.8)
# Output:
(array([[  10],[  42],[  1],[ 300]]), array([[59]]), array([[1],[ 1],[ 0],[ 0]]), array([[0]]))

# Example 1:
data_spliter(x1, y, 0.5)
# Output:
(array([[42],[ 10]]), array([[ 59],[ 300],[  1]]), array([[1],[ 1]]), array([[0],[ 0],[ 0]]))

x2 = np.array([ [  1,  42],
                [300,  10],
                [ 59,   1],
                [300,  59],
                [ 10,  42]])
y = np.array([[0],[1],[0],[1],[0]])

# Example 2:
data_spliter(x2, y, 0.8)
# Output:
(array([[ 10,  42],
        [ 59,   1],
        [  1,  42],
        [300,  10]]), array([[300,  59]]), array([[0],[ 0],[ 0],[ 1]]),array([[1]]))

# Example 3:
data_spliter(x2, y, 0.5)
# Output:
(array([[300,  10],
        [  1,  42]]),
array([[ 10,  42],
       [300,  59],
       [ 59,   1]]),
 array([[1],[ 0]]),
 array([[0],[ 1],[ 0]]))
\end{minted}


% ===========================(fin ex 09)         %
% ============================================== %

\newpage

% ============================================== %
% ===========================(start ex 10)       %
\chapter{Exercise 10}
\extitle{Machine Learning for Grown-ups: Training and Test Sets}
\turnindir{ex10}
\exnumber{10}
\exfiles{space\_avocado.py, benchmark\_train.py,  models.[csv/yml/pickle]}
\exforbidden{sklearn}
\makeheaderfilesforbidden


% ================================= %
\section*{Objective}
% --------------------------------- %
Let's do Machine Learning for "real"!

% ================================= %
\section*{Introduction}
% --------------------------------- %
The dataset is constituted of 5 columns:
\begin{itemize}
  \item \textbf{index}: not relevant,
  \item \textbf{weight}: the avocado weight order (in ton),
  \item \textbf{prod\_distance}: distance from where the avocado ordered is produced (in Mkm),
  \item \textbf{time\_delivery}: time between the order and the receipt (in days),
  \item \textbf{target}: price of the order (in trantorian unit).
\end{itemize}
It contains the data of all the avocado purchase made by Trantor administration (guacamole is a serious business there).

% ================================= %
\section*{Instructions}
% --------------------------------- %
You have to explore different models and select the best you find.
To do this:
\begin{itemize}
  \item Split your \texttt{space\_avocado.csv} dataset into a training and a test set.
  \item Use your \texttt{polynomial\_features} method on your training set.
  \item Consider several Linear Regression models with polynomial hypothesis with a maximum degree of 4.
  \item Evaluate your models on the test set.
\end{itemize}

According to your model evaluations, what is the best hypothesis you can get?
\begin{itemize}
  \item Plot the evaluation curve which help you to select the best model (evaluation metrics vs models).
  \item Plot the true price and the predicted price obtain via your best model (3D representation or 3 scatterplots).
\end{itemize}


The training of all your models can take a long time.
Thus you need to train only the best one during the correction.
But, you should return in \texttt{benchmark\_train.py} the program which perform the training of all the models and save the parameters of the different models into a file.
In \texttt{models.[csv/yml/pickle]} one must find the parameters of all the models you have explored and trained.
In \texttt{space\_avocado.py} train the model based on the best hypothesis you find and load the other models from \texttt{models.[csv/yml/pickle]}.
Then evaluate and plot the different graphics as asked before.

% ===========================(fin ex 10)         %
% ============================================== %

\newpage

% ============================================== %
% ===========================(start ex 11)       %
\chapter{Conclusion - What you have learned}

The excercises serie is finished, well done!
Based on all the knowledges tackled today, you should be able to discuss and answer the following questions:

\begin{enumerate}
  \item What is the main (obvious) difference between univariate and multivariate linear regression?
  \item Is there a minimum number of variables needed to perform a multivariate linear regression?
  \item Is there a maximum number of variables needed to perform a multivariate linear regression?
        In theory and in practice?
  \item Is there a difference between univariate and multivariate linear regression in terms of performance evaluation?
  \item What does it mean geometrically to perform a multivariate gradient descent with two variables?
  \item Can you explain what is overfitting?
  \item Can you explain what is underfitting?
  \item Why is it important to split the data set in a training and a test set?
  \item If a model overfits, what will happen when you compare its performance on the training set and the test set?
  \item If a model underfits, what do you think will happen when you compare its performance on the training set and the test set?
\end{enumerate}

% ===========================(fin ex 11)         %
% ============================================== %

\newpage

% ================================= %
\section*{Contact}
% --------------------------------- %
You can contact 42AI association by email: contact@42ai.fr\\
You can join the association on \href{https://join.slack.com/t/42-ai/shared_invite/zt-ebccw5r7-YPkDM6xOiYRPjqJXkrKgcA}{42AI slack}
and/or posutale to \href{https://forms.gle/VAFuREWaLmaqZw2D8}{one of the association teams}.

% ================================= %
\section*{Acknowledgements}
% --------------------------------- %
The modules Python \& ML is the result of a collective work, we would like to thanks:
\begin{itemize}
  \item Maxime Choulika (cmaxime),
  \item Pierre Peign√© (ppeigne),
  \item Matthieu David (mdavid).
\end{itemize}
who supervised the creation, the enhancement and this present transcription.

\begin{itemize}
  \item Amric Trudel (amric@42ai.fr)
  \item Benjamin Carlier (bcarlier@student.42.fr)
  \item Pablo Clement (pclement@student.42.fr)
\end{itemize}
for your investment for the creation and development of these modules.

\begin{itemize}
  \item Richard Blanc (riblanc@student.42.fr)
  \item Solveig Gaydon Ohl (sgaydon-@student.42.fr)
  \item Quentin Feuillade Montixi (qfeuilla@student.42.fr)
\end{itemize}
who betatest the first version of the modules of Machine Learning.
\vfill
\doclicenseThis

\end{document}
