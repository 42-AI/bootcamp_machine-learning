\chapter{Exercise 04}
\extitle{Multivariate Gradient Descent}
\input{exercises/en.ex04_interlude.tex}
\newpage
\turnindir{ex04}
\exnumber{04}
\exfiles{fit.py}
\exforbidden{any function that performs derivatives for you}
\makeheaderfilesforbidden


% ================================= %
\section*{Objective}
% --------------------------------- %
Understand and manipulate the concept of gradient descent in the case of multivariate linear regression.\\
\newline
Implement a function to perform linear gradient descent (LGD) for multivariate linear regression.

% ================================= %
\section*{Instructions}
% --------------------------------- %
In this exercise, you will implement linear gradient descent to fit your multivariate model to the dataset.\\
\newline
The pseudocode of the algorithm is the following:

$$
\begin{matrix}
    &   \text{repeat until convergence} \hspace{1cm} &  \{  \\
    &   \text{compute } \nabla{(J)}  \\
    &	\theta := \theta - \alpha \nabla(J)                 \\ 
\} 
\end{matrix}
$$
Where:
\begin{itemize}
  \item $\nabla{(J)}$ is the entire gradient vector
  \item $\theta$ is the entire parameter vector
  \item $\alpha$ (alpha) is the learning rate (a small number, usually between 0 and 1)
\end{itemize}
% ================================= %
\newpage
\noindent{You are expected to write a function named \textit{fit\_} as per the instructions bellow:\\}
\newline
\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
def fit_(x, y, theta, alpha, max_iter):
  """
  Description:
    Fits the model to the training dataset contained in x and y.
  Args:
    x: has to be a numpy.array, a matrix of dimension m * n:
                  (number of training examples, number of features).
    y: has to be a numpy.array, a vector of dimension m * 1:
                  (number of training examples, 1).
    theta: has to be a numpy.array, a vector of dimension (n + 1) * 1:
                  (number of features + 1, 1).
    alpha: has to be a float, the learning rate
    max_iter: has to be an int, the number of iterations done during the gradient descent
  Return:
    new_theta: numpy.array, a vector of dimension (number of features + 1, 1).
    None if there is a matching dimension problem.
    None if x, y, theta, alpha or max_iter is not of expected type.
  Raises:
    This function should not raise any Exception.
  """
    ... your code here ...
\end{minted}
\\
Hopefully, you have already implemented a function to calculate the multivariate gradient.
% ================================= %
\section*{Examples}
% ================================= %
\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
import numpy as np
x = np.array([[0.2, 2., 20.], [0.4, 4., 40.], [0.6, 6., 60.], [0.8, 8., 80.]])
y = np.array([[19.6], [-2.8], [-25.2], [-47.6]])
theta = np.array([[42.], [1.], [1.], [1.]])

# Example 0:
theta2 = fit_(x, y, theta,  alpha = 0.0005, max_iter=42000)
theta2
# Output:
array([[41.99..],[0.97..], [0.77..], [-1.20..]])

# Example 1:
predict_(x, theta2)
# Output:
array([[19.5992..], [-2.8003..], [-25.1999..], [-47.5996..]])
\end{minted}

\info{
  \begin{itemize}
    \item You can create more training data by generating an $x$ array with random values and computing the corresponding $y$ vector as a linear expression of $x$.
          You can then fit a model on this artificial data and find out if it comes out with the same $\theta$ coefficients that first you used.
    \item It is possible that $\theta_0$ and $\theta_1$ become \texttt{"nan"}.
          In that case, it means you probably used a learning rate that is too large.
  \end{itemize}
}