# Exercise 06 - Multivariate Gradient Descent

|                         |                     |
| -----------------------:| ------------------  |
|   Turn-in directory :   |  ex06               |
|   Files to turn in :    |  fit.py             |
|   Authorized modules :  |  numpy              |
|   Forbidden functions : |  any function that performs derivatives for you        |


## Objectives: 
* Implement a function to perform linear gradient descent (LGD) for multivariate linear regression.


## Instructions:

In this exercise, you will implement linear gradient descent to fit your multivariate model to the dataset. 

The pseudocode of the algorithm is the following:

$$
\begin{matrix}
    &   \text{repeat until convergence} \hspace{1cm} &  \{  \\
    &   \text{compute } \nabla{(J)}  \\
    &	\theta := \theta - \alpha \nabla(J)                 \\ 
\} 
\end{matrix}
$$

Where:
- $\nabla{(J)}$ is the entire gradient vector
- $\theta$ is the entire parameter vector
- $\alpha$ (alpha) is the learning rate (a small number, usually between 0 and 1)


You are expected to write a function named __fit\___ as per the instructions bellow:
``` python
def fit_(x, y, theta, alpha, n_cycles):
	"""
	Description:
		Fits the model to the training dataset contained in x and y.
	Args:
		x: has to be a numpy.ndarray, a matrix of dimension m * n: (number of training examples, number of features).
		y: has to be a numpy.ndarray, a vector of dimension m * 1: (number of training examples, 1).
		theta: has to be a numpy.ndarray, a vector of dimension (n + 1) * 1: (number of features + 1, 1).
		alpha: has to be a float, the learning rate
		n_cycles: has to be an int, the number of iterations done during the gradient descent
	Returns:
		new_theta: numpy.ndarray, a vector of dimension (number of features + 1, 1).
		None if there is a matching dimension problem.
	Raises:
		This function should not raise any Exception.
	"""
		... your code here ...
```
Hopefully, you have already implemented a function to calculate the multivariate gradient.

## Examples:
```python
import numpy as np
x = np.array([[0.2, 2., 20.], [0.4, 4., 40.], [0.6, 6., 60.], [0.8, 8., 80.]])
y = np.array([[19.6], [-2.8], [-25.2], [-47.6]])
theta = np.array([[42.], [1.], [1.], [1.]])

# Example 0:
theta2 = fit_(X2, Y2, theta2,  alpha = 0.0005, n_cycle=42000)
theta2
# Output:
array([[41.99..],[0.97..], [0.77..], [-1.20..]])

# Example 1:
predict_(X2, theta2)
# Output:
array([[19.5992..], [-2.8003..], [-25.1999..], [-47.5996..]])
```
## Remarks:
- You can create more training data by generating an $x$ array with random values and computing the corresponding $y$ vector as a linear expression of $x$. You can then fit a model on this artificial data and find out if it comes out with the same $\theta$ coefficients that first you used. 
- It is possible that $\theta_0$ and $\theta_1$ become "[nan]". In that case, it means you probably used a learning rate that is too large.
