%******************************************************************************%
%                                                                              %
%                                 Interlude                                    %
%                         for Machine Learning module                          %
%                                                                              %
%******************************************************************************%

% =============================== %
\section*{Interlude - Improve with the Gradient}
% ******************************* %

\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.25]{assets/Improve.png}
    %\caption{The Learning Cycle: Improve}
\end{figure}

% =============================== %
\section*{Multivariate Gradient}
% ******************************* %
From our multivariate linear hypothesis we can derive our multivariate gradient.
It looks a lot like the one we saw during the previous module, but instead of having just two components, the gradient now has as many as there are parameters.
This means that now we need to calculate $\nabla(J)_0,\nabla(J)_1,\dots,\nabla(J)_n$.


If we take the univariate equations we used during the previous module and replace the formula for $\nabla(J)_1$ by a more general $\nabla(J)_j$, we get the following:

$$
\begin{matrix}
\nabla(J)_0 &  = &\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)}) & \\
\nabla(J)_j & = &\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})x_{j}^{(i)} & \text{ for j = 1, ..., n}    
\end{matrix}
$$

Where:
\begin{itemize}
    \item $\nabla(J)$ is a vector of size $(n + 1)$, the gradient vector,
    \item $\nabla(J)_j$ is the j$^\text{th}$ component of $\nabla(J)$, the partial derivative of $J$ with respect to $\theta_j$,
    \item $y$ is a vector of dimension $m$, the vector of expected values,
    \item $y^{(i)}$ is a scalar, the i$^\text{th}$ component of vector $y$,
    \item $x^{(i)}$ is the feature vector of the i$^\text{th}$ example,
    \item $x^{(i)}_j$ is a scalar, the j$^\text{th}$ feature value of the i$^\text{th}$ example,
    \item $h_{\theta}(x^{(i)})$ is a scalar, the model's estimation of $y^{(i)}$. (It can also be denoted $\hat{y}^{(i)}$).
\end{itemize}


% =============================== %
\section*{Vectorized Form}
% ******************************* %
As usual, we can use some linear algebra magic to get a more compact (and computationally efficient) formula.
First we can use our convention that each training example has an extra $x_0 = 1$ feature, and replace the gradient formulas above by one single equation that is valid for all $j$ components:

$$
\begin{matrix}
\nabla(J)_j & = &\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})x_{j}^{(i)} & \text{ for j = 0, ..., n}
\end{matrix}
$$

And this generic equation can then be rewritten in a vectorized form:

$$
\nabla(J) = \frac{1}{m} {X'}^T(X'\theta - y)
$$  

Where:  
\begin{itemize}
    \item $\nabla(J)$ is the gradient vector of size $(n + 1)$,
    \item $X'$ is a matrix of dimension $(m \times (n + 1))$, the design matrix onto which a column of $1$'s was added as the first column,
    \item ${X'}^T$ means the matrix has been transposed,
    \item $\theta$ is a vector of size $(n + 1)$, the parameter vector, 
    \item $y$ is a vector of size $m$, the vector of expected values.
\end{itemize}

The vectorized equation can output the entire gradient vector all at once, in one calculation!
So if you understand the linear algebra operations, you can forget about the equations we presented at the top of the page and simply use the vectorized one.
