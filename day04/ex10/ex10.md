 # Exercise 10 - Regularized Logistic Regression

|                         |                         |
| -----------------------:| ----------------------- |
|   Turn-in directory :   |  ex10                   |
|   Files to turn in :    |  my_logistic_regression.py |
|   Authorized modules :  |  Numpy             |
|   Forbidden modules :   |  sklearn           | 


## Objectives:

In the last exercice, you implemented of a regularized version of the linear regression algorithm, called Ridge regression. Now it is time to update your own logistic regression as well!

In the `scikit-learn` library, the logistic regression implementation have a regularization policy by default which can be selected using the parameter `penalty`. 

The goal of this exercice is to update your old `MyLogisticRegression` class to take that into account.   

## Instructions:
In the my_logistic_regression.py file, update your `MyLogisticRegression` class according to the following : 
- **add** a `penalty` parameter wich can take the following values:  
  `{'l2', 'none'}, default = 'l2'`.

```python
class MyLogisticRegression():
	"""
	Description:
		My personnal logistic regression to classify things.
	"""
    def __init__(self, theta, alpha=0.001, n_cycle=1000, penalty='l2'):
        self.alpha = alpha
        self.max_iter = max_iter
        self.theta = theta
        self.penalty=penalty
        # Your code here

	#... other methods ...
```
- **update** the `fit_(self, x, y)` method: 
    - `if penalty == 'l2'`:  
    use a **regularized version** of the gradient descent.
    - `if penalty = 'none'`:  
    use the **unregularized version** of the gradient descent from day03.


Now that you have done this, you will test your model with the same datasets from day03 and compare the results with and without regularization.  

Have a look at what you got is you do it using polynomial features.
