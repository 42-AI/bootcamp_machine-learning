# Exercise 02 - Information Gain

|                         |                    |
| -----------------------:| ------------------ |
|   Turn-in directory :   |  ex02              |
|   Files to turn in :    |  information_gain.py           |
|   Forbidden modules :   |  sklearn            |
|   Forbidden functions : |  Anything that computes directly the functions. |
|   Remarks :             |  Read the doc      |
|  Helpful links          |  [Shannon Entropy and Information Gain](https://www.youtube.com/watch?v=9r7FIXEAGvs&feature=youtu.be) |

## Objectives:

You must implement the following formula as a function: 
$$
IG(q) = S_0 - \sum_{i = 1}^{q} \frac{n_i}{N} S_i 
$$

Where:  
- $X$ is a vector of dimension N * 1
- S_0 is the actual value of impurity (or entropy)
- $q$ is the number of different parts of $X$ found after the split 
- $n_i$ is the number of components of part $i$
- $S_i$ is the impurity (or entropy) value of part $i$


## Instructions:

In the information_gain.py file, create the function as per the instructions given below:

```python
def information_gain(array_source, array_children_list, criterion='gini'):
    """
    Computes the information gain between the first and second array using the criterion ('gini' or 'entropy').
    Args:
        array_source: a numpy.ndarray.
        array_children_list: a numpy.ndarray list.
        criterion: str which should be in ['gini', 'entropy'].
    Returns
        IG: information gain as a float
	None if input is not a non-empty numpy.ndarray.
	None if invalid input.
    Raises:
        This function should not raise any Exception.
    """
```

## Examples:

```bash
Information gain between ['0' '0' '1' '0' 'bob' '1'] and [array(['0', 'bob', '1'], dtype='<U21'), array([0, 0, 1])] is 0.05555555555555561 with criterion 'gini' and 0.20751874963942196 with criterion 'entropy'
```
