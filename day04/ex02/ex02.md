# Exercise 02 - Information Gain

|                         |                    |
| -----------------------:| ------------------ |
|   Turn-in directory :   |  ex02              |
|   Files to turn in :    |  information_gain.py           |
|   Forbidden modules :   |  sklearn            |
|   Forbidden functions : |  Anything that computes directly the functions. |
|   Remarks :             |  Read the doc      |


**Objective:**
You must implement the following formula as a function: 
$$
IG(q) = S_0 - \sum_{i = 1}^{q} \frac{n_i}{N} S_i 
$$

Where:  
- $X$ is a vector of dimension N * 1
- S_0 is the actual value of impurity (or entropy)
- $q$ is the number of different parts of $X$ found after the split 
- $n_i$ is the number of components of part $i$
- $S_i$ is the impurity (or entropy) value of part $i$


**Instruction:**
In the information_gain.py file, create the function as per the instructions given below:

```python
def information_gain(array_source, array_children_list, criterion='gini'):
    """
    Computes the information gain between the first and second array using the criterion ('gini' or 'entropy')

    :param numpy.ndarray array_source:
    :param list array_children_list: list of numpy.ndarray
    :param str criterion: Should be in ['gini', 'entropy']
    :return float: Shannon entropy as a float or None if input is not a non-empty numpy.ndarray or None if invalid input
    """
```

**Output examples:**
```bash
Information gain between [] and [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] is None with criterion 'gini' and None with criterion 'entropy'
Information gain between ['a' 'a' 'b' 'b'] and {1, 2} is None with criterion 'gini' and None with criterion 'entropy'
Information gain between [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.] and [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] is 0.18 with criterion 'gini' and 0.4689955935892812 with criterion 'entropy'
Information gain between ['0' '0' '1' '0' 'bob' '1'] and [array(['0', 'bob', '1'], dtype='<U21'), array([0, 0, 1])] is 0.05555555555555561 with criterion 'gini' and 0.20751874963942196 with criterion 'entropy'
Information gain between ['0' '0' '1' '0' 'bob' '1'] and [0 0 1 0 2 1] is 0.0 with criterion 'gini' and 0.0 with criterion 'entropy'
```