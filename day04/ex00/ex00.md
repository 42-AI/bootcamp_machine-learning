# Exercise 0 - Understanding mathematical underlying concepts

|                         |                    |
| -----------------------:| ------------------ |
|   Turn-in directory :   |  ex00              |
|   Files to turn in :    |  answers.txt       |
|   Forbidden modules :   |  NA                |
|   Forbidden functions : |  NA                |
|   Remarks :             |  Read the doc      |


**Objective:**

The aim of this first exercise is to understand better 3 key mathematical concepts for decision trees:
- Gini impurity of a dataset.
- Shannon entropy of a dataset.
- Information gain between two datasets.



**Instruction:**

In the answers.txt file, answer the following questions in 3 sentences maximum. The idea is to understand the underlying concepts. These are simple questions (no traps!)
1) Define what Gini impurity is about and what it measures. (No mathematical formula, just the general concept).
2) Define what Shannon entropy is and what it measures. (No mathematical formula, just the general concept).
3) Define what Information gain is and what it measures. (No mathematical formula, just the general concept).
4) Explain how these 3 concepts are used for decision trees. 
5) If the dataset has 2 classes, explain what are the boundaries (minimum and maximum) of Gini impurity and Shannon entropy.
6) What does it mean if the Gini impurity is 0? If Shannon entropy is 0 ?


**Bonus question**
7) Why should you use Gini impurity as default?
8) What are the pros and cons of just using the criteria Information Gain positive or negative to pick the feature of a decision tree? Can we mitigate this risk ?
