# Exercise 00 - Shannon's Entropy

|                         |                    |
| -----------------------:| ------------------ |
|   Turn-in directory :   |  ex00              |
|   Files to turn in :    |  entropy.py        |
|   Forbidden modules :   |  sklearn           |
|   Forbidden functions : |  Anything that computes directly the functions. |
|   Remarks :             |  Read the doc      |
|   Helpful links :       |  [Shannon Entropy and Information Gain](https://www.youtube.com/watch?v=9r7FIXEAGvs&feature=youtu.be) |


## Objectives:

You must implement the following formula as a function: 
$$
S(X) = - \sum_{i = 1}^{C} p_i \log_2(p_i)
$$

Where:  
- $X$ is a vector of dimension N * 1
- $C$ is the number of different class found in X's components 
- $p_i$ is the probability of class $i$ in X: $p_i = \frac{n}{N}$ where $n$ is the number of components of $X$ belonging to class $i$
- $log_2$ is the logarithm function in base 2 


## Instructions:

In the entropy.py file, create the function as per the instructions given below:

```python
def entropy(X):
    """ Computes the Shannon entropy of the array X.
    Args:
        X: a non-empty numpy.ndarray.
    Returns:
        S: Shannon entropy as a float.
	None if X is an empty numpy.ndarray.
    Raises:
        This function should not raise any Exception.
    """
```

## Examples:

```bash
Shannon entropy for [] is None
Shannon entropy for {1, 2} is None
Shannon entropy for bob is None
Shannon entropy for [0 0 0 0 0 0] is 0.0
Shannon entropy for [6] is 0.0
Shannon entropy for ['a' 'a' 'b' 'b'] is 1.0
Shannon entropy for ['0' '0' '1' '0' 'bob' '1'] is 1.4591479170272448
Shannon entropy for [0 0 1 0 2 1] is 1.4591479170272448
Shannon entropy for ['0' 'bob' '1'] is 1.584962500721156
Shannon entropy for [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] is 0.0
Shannon entropy for [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.] is 0.4689955935892812
Shannon entropy for [0 0 1] is 0.9182958340544896
```
