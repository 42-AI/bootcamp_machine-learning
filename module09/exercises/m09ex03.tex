\chapter{Exercise 03}
\extitle{Regularized Logistic Loss Function}
%\input{exercises/en.ex03_interlude.tex}
%\newpage
\turnindir{ex03}
\exnumber{03}
\exfiles{logistic\_loss\_reg.py}
\exforbidden{sklearn}
\makeheaderfilesforbidden

% ================================= %
\section*{Objective}
% --------------------------------- %
You must implement the following formula as a function:

$$
J( \theta) = -\frac{1} {m} \lbrack y \cdot \log(\hat{y}) + (\vec{1} - y) \cdot \log(\vec{1} - \hat{y})\rbrack + \frac{\lambda}{2m} (\theta' \cdot \theta')
$$
\\
Where:
\begin{itemize}
  \item $\hat{y}$ is a vector of dimension $m$, the vector of predicted values
  \item $y$ is a vector of dimension $m$, the vector of expected values
  \item $\vec{1}$ is a vector of dimension $m$, a vector full of ones
  \item $\lambda$ is a constant, the regularization hyperparameter
  \item $\theta'$ is a vector of dimension $n$, constructed using the following rules: 
\end{itemize}
$$
\begin{matrix}
\theta'_0 & =  0 \\
\theta'_j & =  \theta_j & \text{ for } j = 1, \dots, n\\    
\end{matrix}
$$
\newpage
% ================================= %
\section*{Instructions}
% --------------------------------- %
In the \texttt{logistic\_loss\_reg.py} file, write the following function as 
per the instructions given below:\\
\\
\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
def reg_log_loss_(y, y_hat, theta, lambda_):
	"""Computes the regularized loss of a logistic regression model from two non-empty numpy.ndarray, 
	without any for loop. The two arrays must have the same shapes.
	Args:
		y: has to be an numpy.ndarray, a vector of shape m * 1.
		y_hat: has to be an numpy.ndarray, a vector of shape m * 1.
		theta: has to be a numpy.ndarray, a vector of shape n * 1.
		lambda_: has to be a float.
	Returns:
		The regularized loss as a float.
		None if y, y_hat, or theta is empty numpy.ndarray.
		None if y and y_hat do not share the same shapes.
	Raises:
		This function should not raise any Exception.
	"""
	... Your code ...
\end{minted}

\hint{Here again, seems to be a good use case for decorators ...}

% ================================= %
\section*{Examples}
% --------------------------------- %
\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
y = np.array([1, 1, 0, 0, 1, 1, 0]).reshape((-1, 1))
y_hat = np.array([.9, .79, .12, .04, .89, .93, .01]).reshape((-1, 1))
theta = np.array([1, 2.5, 1.5, -0.9]).reshape((-1, 1))

# Example :
reg_log_loss_(y, y_hat, theta, .5)
# Output:
0.43377043716475955

# Example :
reg_log_loss_(y, y_hat, theta, .05)
# Output:
0.13452043716475953

# Example :
reg_log_loss_(y, y_hat, theta, .9)
# Output:
0.6997704371647596
\end{minted}