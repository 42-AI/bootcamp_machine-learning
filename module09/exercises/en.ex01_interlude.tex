%******************************************************************************%
%                                                                              %
%                                 Interlude                                    %
%                         for Machine Learning module                          %
%                                                                              %
%******************************************************************************%

% =============================== %
\section*{Interlude}
% =============================== %
\subsection*{Fighting Overfitting... With Regularization}
% ------------------------------- %

\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.25]{assets/Evaluate.png}
    %\caption{The Learning Cycle - Evaluate}
\end{figure}

In the \textbf{module07}, we talked about the problem of \textbf{overfitting} and the necessity of splitting the dataset into a \textbf{training set} and a \textbf{test set} in order to spot it.
However, being able to detect overfitting does not mean being able to avoid it.
To address this important issue, it is time to introduce you to a new technique: \textbf{regularization}.
If you remember well, overfitting happens because the model takes advantage of irrelevant signals in the training data.
The basic idea behind regularization is to \textbf{penalize the model for putting too much weight on certain} (usually heavy polynomial) \textbf{features}.
We do this by adding an extra term in the loss function:

$$
\text{regularized loss function} = \text{loss function} + \frac{\lambda}{2m} \sum_{j = 1}^n \theta_j^2
$$

By doing so, \textbf{we are encouraging the model to keep its} $\theta$ \textbf{values as small as possible}.
Indeed, the values of $\theta$ \textit{themselves} are now taken into account when calculating the loss.


$\lambda$ (called \textit{lambda}) is the parameter through which you can modulate how reglarization should impact the model's construction.
\begin{itemize}
    \item If $\lambda = 0$, there is no regularization (as we did until now).
    \item If $\lambda$ is very large, it will drive all the $\theta$ parameters to $0$.
\end{itemize}

\info{In the regularization term, the sum starts at $j=1$ because we do NOT want to penalize the value of $\theta_0$ (the y-intercept, which doesn't depend on a feature).}

% =============================== %
\subsection*{Be carefull!}
% ------------------------------- %
Machine Learning was essentially developed by computer scientists (not mathematicians).
This can cause problems when we try to represent things mathematically.
For example: using the $\theta_0$ notation to represent the y-intercept makes things easy when we apply the linear algebra trick, \textbf{but} it completly messes up the overall matrix notation!\\
According to that notation, the $X'$ matrix  has the following properties: 
\begin{itemize}
    \item its rows, $x'^{(i)}$, follow the mathematical indexing: starting at $1$.
    \item its columns, $x^{'}_j$, follow the computer science indexing: starting at $0$.
\end{itemize}

$$
X' =
\underbrace{
\begin{bmatrix}
1 & x_1^{(1)} & \dots & x_n^{(1)} \\
\vdots & \vdots & \ddots & \vdots \\ 
1 & x_1^{(m)} & \dots & x_n^{(m)} \\ 
\end{bmatrix}  
}_{\begin{matrix}
    j = 0, \dots, n
\end{matrix}}
=     
\left.
\begin{bmatrix}
x_0^{(1)} & x_1^{(1)} & \dots & x_n^{(1)} \\
\vdots & \vdots & \ddots & \vdots \\ 
x_0^{(m)} & x_1^{(m)} & \dots & x_n^{(m)} \\ 
\end{bmatrix}
\right\} i = 1, \dots, m
$$

It's precisely for this reason that you keep seeing that $X'$ is of dimension $(m \times (n+1))$.


% =============================== %
\subsection*{Terminology}
% ------------------------------- %
The regularization technique we are introducing here is named \texttt{\textbf{L$_2$ regularization}}, because it adds the squared \texttt{\textbf{L$_2$ norm}} of the $\theta$ vector to the loss function.
The L$_2$ norm of a given vector $x$, written

$$
L_2(x) = ||x||_2 = \sqrt{\sum_i x_i^2} \\
L_2(x)^2 = ||x||_2^2 = \sum_i x_i^2
$$

is its \textbf{euclidean norm} (i.e. the sum of the components squared).  

There is an infinite variety of norms that could be used as regularization terms, depending on the desired regularization effect.
Here, we will only use $L_2$, the most common one.


\info{
  The notation $\sum_i$ means: "the sum for all i"\newline
  There is no need to give explicitly the start and the end of the summation index if we want to sum over all the values of $i$.
  However, it is better to do it anyway because it forces us to be sure of what we are doing.
  And in our case, we do not want to sum over $\theta_0$ ...
}

% =========================================== %
\subsection*{Our old friend vectorization ...}
% ------------------------------------------- %
It is not a surprise, we can use vectorization to calculate $\sum_{j = 1}^n \theta_j^2$  more efficiently.
It could be a good exercise for you to try to figure it out by yourself.
We suggest you give it a try and then check the answer on the next page.


% =========================================== %
\subsection*{Answers to the Vectorization Problem}
% ------------------------------------------- %
So, how do you vectorize the following?

$$
\sum_{i = j}^n \theta_j^2
$$ 

It's very similar to a \textbf{dot product} of $\theta$ with itself.
The only problem here is to find a way to not take $\theta_0$ into account.

Let's construct a vector $\theta'$ with the following rules :
$$
\begin{matrix}
\theta'_0 & = 0 &\\
\theta'_j & =  \theta_j & \text{ for } j = 1, \dots, n\\    
\end{matrix}
$$

In other words: 
$$
\\
\theta' = \begin{bmatrix}
  0 \\
  \theta_1 \\
  \vdots \\
  \theta_n
\end{bmatrix}
$$

This way, we can perform the dot product without having $\theta_0$ interfering in our calculations:

$$
\begin{matrix}
\theta' \cdot \theta' & = & 
\begin{bmatrix}
  0 \\
  \theta_1 \\
  \vdots \\
  \theta_n
\end{bmatrix} \cdot \begin{bmatrix}
  0 \\
  \theta_1 \\
  \vdots \\
  \theta_n
\end{bmatrix} \\ 
\\
& = & 0 \cdot 0 + \theta_1 \cdot \theta_1 + \dots + \theta_n \cdot \theta_n \\ 
\\
& = & \sum_{j= 1}^n \theta_j^2
\end{matrix}
$$
