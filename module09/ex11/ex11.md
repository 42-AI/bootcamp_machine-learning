# Exercise 11 - Regularized Logistic Regression

|                         |                         |
| -----------------------:| ----------------------- |
|   Turn-in directory :   |  ex11                   |
|   Files to turn in :    |  my_logistic_regression.py |
|   Authorized modules :  |  Numpy             |
|   Forbidden modules :   |  sklearn           | 


## Objectives:

In the last exercice, you implemented of a regularized version of the linear regression algorithm, called Ridge regression. Now it's time to update your logistic regression classifier as well!

In the `scikit-learn` library, the logistic regression implementation offers a few regularization techniques, which can be selected using the parameter `penalty` (L2 is default). 

The goal of this exercice is to update your old `MyLogisticRegression` class to take that into account.   

## Instructions:
In the my_logistic_regression.py file, update your `MyLogisticRegression` class according to the following : 
- **add** a `penalty` parameter wich can take the following values:  
  `{'l2', 'none'}, default = 'l2'`.

```python
class MyLogisticRegression():
	"""
	Description:
		My personnal logistic regression to classify things.
	"""
    def __init__(self, theta, alpha=0.001, n_cycle=1000, penalty='l2'):
        self.alpha = alpha
        self.max_iter = max_iter
        self.theta = theta
        self.penalty=penalty
        # Your code here

	#... other methods ...
```
- **update** the `fit_(self, x, y)` method: 
    - `if penalty == 'l2'`:  
    use a **regularized version** of the gradient descent.
    - `if penalty = 'none'`:  
    use the **unregularized version** of the gradient descent from module08.

**Hint:** this is also a great use case for decorators...
