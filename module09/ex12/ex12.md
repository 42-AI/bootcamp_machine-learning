# Exercise 12 - Practicing Regularized Logistic Regression

|                         |                                                                      |
| -----------------------:| -------------------------------------------------------------------- |
|   Turn-in directory :   |  ex12/                                                               |
|   Files to turn in :    |  solar_system_census.py, benchmark_train.py, models.[csv/yml/pickle] |
|   Authorized modules :  |  numpy                                                               |
|   Forbidden modules :   |  sklearn                                                             |

## Objective:  
It's training time!  
Let's practice our updated Logistic Regression with polynomial models.

## Introduction:
You have already used the dataset `solar_system_census.csv` and `solar_system_census_planets.csv`.

* The dataset is divided in two files which can be found in the `resources` folder: `solar_system_census.csv` and `solar_system_census_planets.csv`.
* The first file contains biometric information such as the height, weight, and bone density of several Solar System citizens.
* The second file contains the homeland of each citizen, indicated by its Space Zipcode representation (i.e. one number for each planet... :)).  

As you should know, Solar citizens come from four registered areas (zipcodes): 
* The flying cities of Venus (0), 
* United Nations of Earth (1), 
* Mars Republic (2), 
* The Asteroids' Belt colonies (3).

## Instructions:
What you have to do is very similar to what you have done in the exercise 10.

You have to explore different models and select the best one. To do this, you will create 2 programs: `benchmark_train.py` and `solar_system_censu.py`.

### Benchmark and train:
`benchmark_train.py` is expected to:
1. Split the dataset into a **training**, **cross-validation** and **test** sets.
2. Explore several regularized logistic models with polynomial hypotheses (maximum degree of $3$). The models will be trained with different $\lambda$ values,ranging from $0$ to $1$.
3. Evaluate your models with the **f1-score** on the **cross-validation** set.
4. Print the evaluation score which help you to select the best model (evaluation metric vs models + $\lambda$ factor).
5. Save the different models into `models.[csv/yml/pickle]` file.

### solar system census program:
`benchmark_train.py` is expected to:
1. Split the dataset into a **training**, **cross-validation** and **test** sets.
2. Load the differents models from `models.[csv/yml/pickle]` and train from scratch the best model you find during your benchmark.
4. Print and plot the performance of the different models (different hypotheses and different values for $\lambda$ factor) measured with the **f1-score** on the **cross-validation set** and also the score of your best model on the **test** set.
5. Plot the target values and the predicted values of your best model on the same scatter plot. Make some effort to have a readable figure.