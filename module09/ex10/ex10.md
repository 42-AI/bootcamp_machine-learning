# Exercise 10 - Practicing Ridge Regression

|                         |                     |
| -----------------------:| ------------------  |
|   Turn-in directory :   |  ex10               |
|   Files to turn in :    |  polynomial_ridge.py      |
|   Authorized modules :  |  numpy              |
|   Forbidden modules :   |  sklearn            |

## Objectives:  
It's training time!  
Let's practice our brand new Ridge Regression with a polynomial model.

## Instructions:

### Part 1: Data Splitting

Take your `spacecraft_data.csv` dataset and split it in a **training** and a **test** set.

### Part 2: Training
- You will train 10 different models on the training set: **one** Linear Regression model and **nine** Ridge Regression models. All 10 models will use a polynomial hypothesis of **degree 3**. The Ridge Regression models will be trained with different $\lambda$ values, ranging from 0.1 up to 1 (with increments of 0.1).
- Score the performance of each of the 10 models on the **test set** with the **Mean Squared Error** metric. You can use the `mse` function that you implemented in the `ex11` of `module05`.
- To properly visualize your results, make a bar plot showing the MSE score of the models given their $\lambda$ value.   

According to your evaluations, what is the best hypothesis (or model) you can get?

### Part 3: Plots
* For each model you built in Part 2, plot its hypothesis function $h(\theta)$ on top of a scatter plot of the original data points $(x,y)$. 
