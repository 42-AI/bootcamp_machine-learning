\chapter{Exercise 00}
\input{exercises/en.ex00_interlude.tex}
\newpage
\extitle{Linear Gradient - Iterative Version}
\turnindir{ex00}
\exnumber{00}
\exfiles{gradient.py}
\exforbidden{None}
\makeheaderfilesforbidden

% ================================== %
\section*{Objective}
% ---------------------------------- %
Understand and manipulate the notion of gradient and gradient descent in machine learning.\\
\newline
You must write a function that computes the \textbf{\textit{gradient}} of the loss function.
It must compute a partial derivative with respect to each theta parameter separately, and return the vector gradient.\\
\newline
The partial derivatives can be calculated with the following formulas:  
$$
\nabla(J)_0 = \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})
$$

$$
\nabla(J)_1 = \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})x^{(i)}
$$
Where:
\begin{itemize}
  \item $\nabla(J)$ is the gradient vector of size $2 \times 1$, (this strange symbol : $\nabla$ is called nabla)
  \item $x$ is a vector of dimension $m$
  \item $y$ is a vector of dimension $m$
  \item $x^{(i)}$ is the i$^\text{th}$ component of vector $x$
  \item $y^{(i)}$ is the i$^\text{th}$ component of vector $y$
  \item $\nabla(J)_j$ is the j$^\text{th}$ component of $\nabla(J)$
  \item $h_{\theta}(x^{(i)})$ corresponds to the model's prediction of $y^{(i)}$
\end{itemize}

% ================================== %
\section*{Hypothesis Notation}
% ---------------------------------- %
$h_{\theta}(x^{(i)})$ is the same as what we previously noted $\hat{y}^{(i)}$.  
The two notations are equivalent.
They represent the model's prediction (or estimation) of the ${y}^{(i)}$ value.
If you follow Andrew Ng's course material on Coursera, you will see him using the former notation.
\newline
As a reminder:
$h_{\theta}(x^{(i)}) = \theta_0 + \theta_1x^{(i)}$

% ================================== %
\section*{Instructions}
% ---------------------------------- %

In the \texttt{gradient.py} file create the following function as per the instructions given below:
\newline
\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
  def simple_gradient(x, y, theta):
    """Computes a gradient vector from three non-empty numpy.arrays, with a for-loop.
       The three arrays must have compatible shapes.
    Args:
      x: has to be an numpy.array, a vector of shape m * 1.
      y: has to be an numpy.array, a vector of shape m * 1.
      theta: has to be an numpy.array, a 2 * 1 vector.
    Return:
      The gradient as a numpy.array, a vector of shape 2 * 1.
      None if x, y, or theta are empty numpy.array.
      None if x, y and theta do not have compatible shapes.
      None if x, y or theta is not of the expected type.
    Raises:
      This function should not raise any Exception.
    """
    ... Your code ...
\end{minted}

% ================================== %
\section*{Examples}
% ---------------------------------- %

\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
import numpy as np
x = np.array([12.4956442, 21.5007972, 31.5527382, 48.9145838, 57.5088733]).reshape((-1, 1))
y = np.array([37.4013816, 36.1473236, 45.7655287, 46.6793434, 59.5585554]).reshape((-1, 1))

# Example 0:
theta1 = np.array([2, 0.7]).reshape((-1, 1))
simple_gradient(x, y, theta1)
# Output:
array([[-19.0342574], [-586.66875564]])

# Example 1:
theta2 = np.array([1, -0.4]).reshape((-1, 1))
simple_gradient(x, y, theta2)
# Output:
array([[-57.86823748], [-2230.12297889]])
\end{minted}