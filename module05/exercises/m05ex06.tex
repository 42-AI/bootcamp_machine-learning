\chapter{Exercise 06}
\input{exercises/en.ex06_interlude.tex}
\newpage
\extitle{Loss function}
\turnindir{ex06}
\exnumber{06}
\exfiles{loss.py}
\exforbidden{None}
\makeheaderfilesforbidden


% ================================= %
\section*{Objective}
% --------------------------------- %
Understand and experiment with the \textbf{loss function} in machine learning.

You must implement the following formula as a function (and another one very close to it):

$$
J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(\hat{y}^{(i)} - y^{(i)})^2
$$

Where:
\begin{itemize}
  \item $\hat{y}$ is a vector of dimension $m\times 1$, the vector of predicted values
  \item $y$ is a vector of dimension $m\times 1$, the vector of expected values
  \item $\hat{y}^{(i)}$ is the ith component of vector $\hat{y}$
  \item $y^{(i)}$ is the ith component of vector $y$
\end{itemize}

\newpage

% ================================= %
\section*{Instructions}
% --------------------------------- %
The implementation of the loss function has been split in two functions:
\begin{itemize}
  \item \texttt{loss\_elem\_()}, which computes the squared distances for all examples ($\hat{y}^{(i)} - y^{(i)})^2$,
  \item \texttt{loss\_()}, which averages the squared distances of all examples (the $J_(\theta)$ above).
\end{itemize}

In the loss.py file create the following functions as per the instructions given below:
\par
\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
def loss_elem_(y, y_hat):
	"""
	Description:
		Calculates all the elements (y_pred - y)^2 of the loss function.
	Args:
      y: has to be an numpy.array, a two-dimensional array of shape m * 1.
      y_hat: has to be an numpy.array, a two-dimensional array of shape m * 1.
	Returns:
		J_elem: numpy.array, a array of dimension (number of the training examples, 1).
		None if there is a dimension matching problem.
		None if any argument is not of the expected type.
	Raises:
		This function should not raise any Exception.
	"""
	... your code here ...

def loss_(y, y_hat):
	"""
	Description:
		Calculates the value of loss function.
	Args:
      y: has to be an numpy.array, a two-dimensional array of shape m * 1.
      y_hat: has to be an numpy.array, a two-dimensional array of shape m * 1.
	Returns:
		J_value : has to be a float.
		None if there is a dimension matching problem.
		None if any argument is not of the expected type.
	Raises:
		This function should not raise any Exception.
	"""
	... your code here ...
\end{minted}

% ================================= %
\section*{Examples}
% --------------------------------- %
\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
import numpy as np

x1 = np.array([[0.], [1.], [2.], [3.], [4.]])
theta1 = np.array([[2.], [4.]])
y_hat1 = predict_(x1, theta1)
y1 = np.array([[2.], [7.], [12.], [17.], [22.]])

# Example 1:
loss_elem_(y1, y_hat1)

# Output:
array([[0.], [1], [4], [9], [16]])

# Example 2:
loss_(y1, y_hat1)

# Output:
3.0

x2 = np.array([0, 15, -9, 7, 12, 3, -21]).reshape(-1, 1)
theta2 = np.array(np.array([[0.], [1.]]))
y_hat2 = predict_(x2, theta2)
y2 = np.array([2, 14, -13, 5, 12, 4, -19]).reshape(-1, 1)

# Example 3:
loss_(y2, y_hat2)

# Output:
2.142857142857143

# Example 4:
loss_(y2, y2)

# Output:
0.0
\end{minted}

\info{
This loss function is very close to the one called \textbf{"Mean Squared Error"}, which is frequently mentioned in Machine Learning resources.
The difference is in the denominator as you can see in the formula of the $MSE = \frac{1}{m}\sum_{i=1}^{m}(\hat{y}^{(i)} - y^{(i)})^2$.\
\newline
Except for the division by $2m$ instead of $m$, these functions are rigourously identical: $J(\theta) = \frac{MSE}{2}$.\
\newline
MSE is called like that because it represents the mean of the errors (i.e.: the differences between the predicted values and the true values), squared.\
\newline
You might wonder why we choose to divide by two instead of simply using the MSE?  
  \textbf{(It's a good question, by the way.)}
  \begin{itemize}
    \item First, it does not change the overall model evaluation: if all performance measures are divided by two, we can still compare different models and their performance ranking will remain the same.
    \item Second, it will be very convenient when we calculate the gradient tommorow. Be patient, and trust us ;)
  \end{itemize}
}
  