\chapter{Exercise 07}
\extitle{Vectorized loss function}
\turnindir{ex07}
\exnumber{07}
\exfiles{vec\_loss.py}
\exforbidden{None}
\makeheaderfilesforbidden
  
% ================================= %
\section*{Objective}
% --------------------------------- %
Understand and manipulate the notion of loss function in machine learning.
  
You must implement the following formula as a function:  
$$
\begin{matrix}
  J(\theta) &  = & \frac{1}{2m}(\hat{y} - y) \cdot(\hat{y}- y)
\end{matrix}
$$

Where:
\begin{itemize}
  \item $\hat{y}$ is a vector of dimension $m$, the vector of predicted values,
  \item $y$ is a vector of dimension $m$, the vector of expected values.
\end{itemize}

\newpage

% ================================= %
\section*{Instructions}
% --------------------------------- %
In the \texttt{vec\_loss.py} file, create the following function as per the instructions given below:

\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
def loss_(y, y_hat):
    """Computes the half mean squared error of two non-empty numpy.array, without any for loop.
    The two arrays must have the same dimensions.
    Args:
      y: has to be an numpy.array, a one-dimensional array of size m.
      y_hat: has to be an numpy.array, a one-dimensional array of size m.
    Returns:
      The half mean squared error of the two vectors as a float.
      None if y or y_hat are empty numpy.array.
      None if y and y_hat does not share the same dimensions.
    Raises:
      This function should not raise any Exceptions.
    """
    ... Your code ...
\end{minted}


% ================================= %
\section*{Examples}
% --------------------------------- %
\begin{minted}[bgcolor=darcula-back,formatcom=\color{lightgrey},fontsize=\scriptsize]{python}
import numpy as np
X = np.array([0, 15, -9, 7, 12, 3, -21])
Y = np.array([2, 14, -13, 5, 12, 4, -19])

# Example 1:
loss_(X, Y)
# Output:
2.142857142857143

# Example 2:
loss_(X, X)
# Output:
0.0
\end{minted}